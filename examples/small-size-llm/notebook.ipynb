{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a51548b",
   "metadata": {},
   "source": [
    "# Deploy a small-sized LLM\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fsmall-size-llm\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/small-size-llm\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "A small LLM runs on a single node with 1â€“2 GPUs, making it fast, inexpensive, and simple to use. Itâ€™s ideal for prototyping, lightweight applications, latency-critical use cases, cost-sensitive deployments, and environments with limited resources where efficiency matters more than peak accuracy.\n",
    "\n",
    "\n",
    "For larger models, see [Deploy a medium-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html) or [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Set your Hugging Face token in the config file to access gated models like `Llama-3.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e555ca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2026-02-02 16:46:34,980 serve 24568 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-02 16:46:34,981 serve 24568 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-llama-3_1-8b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'_ray_commit': '8cc3d75444aabcd6199650a2097c45229ac8d3fe',\n",
      "                                       'env_vars': {'HF_TOKEN': None},\n",
      "                                       'pip': {'packages': ['ray[llm,server]'],\n",
      "                                               'pip_check': False},\n",
      "                                       'ray_debugger': {'working_dir': '/home/ray/default/small-size-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_36058b1737d3bdf0ff32d97cb090c330d47aa7ad.zip'}}}\n",
      "INFO 2026-02-02 16:46:35,004 serve 24568 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-02 16:46:35,005 serve 24568 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n"
     ]
    }
   ],
   "source": [
    "# serve_llama_3_1_8b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama-3.1-8b\",\n",
    "        # Or unsloth/Meta-Llama-3.1-8B-Instruct for an ungated model\n",
    "        model_source=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    ),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    # Set ingress replicas to 1 (default is 2 per model replica)\n",
    "    experimental_configs=dict(num_ingress_replicas=1),\n",
    "    ### If your model isn't gated, you can skip `HF_TOKEN`\n",
    "    # Share your Hugging Face token with the vllm engine so it can access the gated Llama 3\n",
    "    # Type `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>` in a terminal\n",
    "    runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(max_model_len=8192),\n",
    ")\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a7140",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate to using a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. See [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Metaâ€™s Llama. Store it in `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>`.\n",
    "\n",
    "**Note:** Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Metaâ€™s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_llama_3_1_8b.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 16:51:47,683\tINFO scripts.py:507 -- Running import path: 'serve_llama_3_1_8b:app'.\n",
      "INFO 02-02 16:51:50 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "2026-02-02 16:51:51,190\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.220:6379...\n",
      "2026-02-02 16:51:51,200\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-k5jw2jlhjdpcd4gu66a4k5uj5d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-02 16:51:51,202\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_aa375202733ec95ae7694644b5231d422936c5f6.zip' (0.10MiB) to Ray cluster...\n",
      "2026-02-02 16:51:51,202\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_aa375202733ec95ae7694644b5231d422936c5f6.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-02 16:51:51,210 serve 32651 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-02 16:51:51,211 serve 32651 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-llama-3_1-8b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'_ray_commit': '8cc3d75444aabcd6199650a2097c45229ac8d3fe',\n",
      "                                       'env_vars': {'HF_TOKEN': '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'},\n",
      "                                       'pip': {'packages': ['ray[llm,server]'],\n",
      "                                               'pip_check': False},\n",
      "                                       'ray_debugger': {'working_dir': '/home/ray/default/small-size-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_aa375202733ec95ae7694644b5231d422936c5f6.zip'}}}\n",
      "INFO 2026-02-02 16:51:51,240 serve 32651 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-02 16:51:51,240 serve 32651 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n",
      "\u001b[36m(ProxyActor pid=32855)\u001b[0m INFO 2026-02-02 16:51:53,543 proxy 10.128.5.220 -- Proxy starting on node 8d696e3ba116570fd9a9c4f537079bba3a3b1922744deafae4f33b40 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=32855)\u001b[0m INFO 2026-02-02 16:51:53,583 proxy 10.128.5.220 -- Got updated endpoints: {}.\n",
      "INFO 2026-02-02 16:51:53,587 serve 32651 -- Started Serve in namespace \"serve\".\n",
      "INFO 2026-02-02 16:51:53,613 serve 32651 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,672 controller 32777 -- Deploying new version of Deployment(name='LLMServer:my-llama-3_1-8b', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,673 controller 32777 -- Deploying new version of Deployment(name='OpenAiIngress', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=32855)\u001b[0m INFO 2026-02-02 16:51:53,676 proxy 10.128.5.220 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=32855)\u001b[0m WARNING 2026-02-02 16:51:53,680 proxy 10.128.5.220 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=32855)\u001b[0m INFO 2026-02-02 16:51:53,682 proxy 10.128.5.220 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f6ae1875210>.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,776 controller 32777 -- Adding 1 replica to Deployment(name='LLMServer:my-llama-3_1-8b', app='default').\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,777 controller 32777 -- Assigned rank 0 to new replica hfio14ia during startup\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,778 controller 32777 -- Adding 1 replica to Deployment(name='OpenAiIngress', app='default').\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m INFO 2026-02-02 16:51:53,778 controller 32777 -- Assigned rank 0 to new replica p4mm5yt6 during startup\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:51:59 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:01,102 default_LLMServer:my-llama-3_1-8b hfio14ia -- Downloading the tokenizer for meta-llama/Llama-3.1-8B-Instruct\n",
      "\u001b[36m(ProxyActor pid=3503, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:01,489 proxy 10.128.7.133 -- Proxy starting on node 7456f191caa85dd3ac95050b9f57428f89b3765d68831fc29826265c (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=3503, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:01,556 proxy 10.128.7.133 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=3503, ip=10.128.7.133)\u001b[0m WARNING 2026-02-02 16:52:01,561 proxy 10.128.7.133 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=3503, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:01,570 proxy 10.128.7.133 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7efb2ce49a50>.\n",
      "\u001b[36m(pid=3715, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:08 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_get_vllm_engine_config pid=3715, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:19 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "\u001b[36m(_get_vllm_engine_config pid=3715, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:19 [__init__.py:1815] Using max model len 8192\n",
      "\u001b[36m(_get_vllm_engine_config pid=3715, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:19 [arg_utils.py:1208] Using ray runtime env: {'_ray_commit': '8cc3d75444aabcd6199650a2097c45229ac8d3fe', 'ray_debugger': {'working_dir': '/home/ray/default/small-size-llm'}, 'working_dir': 'gcs://_ray_pkg_aa375202733ec95ae7694644b5231d422936c5f6.zip', 'pip': {'packages': ['ray[llm,server]'], 'pip_check': False}, 'env_vars': {'HF_TOKEN': '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'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n",
      "\u001b[36m(_get_vllm_engine_config pid=3715, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:20,514 default_LLMServer:my-llama-3_1-8b hfio14ia -- Clearing the current platform cache ...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:52:20,520 default_LLMServer:my-llama-3_1-8b hfio14ia -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m WARNING 02-02 16:52:21 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:52:23,855 controller 32777 -- Deployment 'LLMServer:my-llama-3_1-8b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:52:23,856 controller 32777 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:25 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:27 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:27 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=my-llama-3.1-8b, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m 2026-02-02 16:52:27,231\tINFO worker.py:1692 -- Using address ses-k5jw2jlhjdpcd4gu66a4k5uj5d-head:6379 set in the environment variable RAY_ADDRESS\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m 2026-02-02 16:52:27,238\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: ses-k5jw2jlhjdpcd4gu66a4k5uj5d-head:6379...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m 2026-02-02 16:52:27,285\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-k5jw2jlhjdpcd4gu66a4k5uj5d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:28 [ray_utils.py:324] Using the existing placement group\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:28 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m   warnings.warn(\n",
      "\u001b[36m(pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:34 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(pid=8666)\u001b[0m INFO 02-02 16:52:34 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:35 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:35 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_USE_RAY_COMPILED_DAG', 'HF_TOKEN', 'VLLM_WORKER_MULTIPROC_METHOD', 'LD_LIBRARY_PATH', 'VLLM_USE_RAY_SPMD_WORKER']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:52:35 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [W202 16:52:38.636297432 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [W202 16:52:38.643515537 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [W202 16:52:38.643940864 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m WARNING 02-02 16:52:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:38 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:38 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:39 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:52:39 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:52:53,885 controller 32777 -- Deployment 'LLMServer:my-llama-3_1-8b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:52:53,886 controller 32777 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [W202 16:52:38.643515537 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [W202 16:52:38.643940864 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:04 [weight_utils.py:369] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 25.346351 seconds\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.45it/s]\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:08 [default_loader.py:268] Loading weights took 3.23 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:08 [gpu_model_runner.py:2392] Model loading took 14.9889 GiB and 29.237619 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:14 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/3dcdb3babd/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:14 [backends.py:550] Dynamo bytecode transform time: 5.48 s\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:17 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:53:23,945 controller 32777 -- Deployment 'LLMServer:my-llama-3_1-8b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m WARNING 2026-02-02 16:53:23,945 controller 32777 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=32777)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]33)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.34it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:36 [backends.py:215] Compiling a graph for dynamic shape takes 22.17 s\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:37 [monitor.py:34] torch.compile takes 27.65 s in total\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:38 [gpu_worker.py:298] Available KV cache memory: 55.13 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:52:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m WARNING 02-02 16:52:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:52:38 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:52:38 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:52:39 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:52:39 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:04 [weight_utils.py:369] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 25.346351 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:08 [default_loader.py:268] Loading weights took 3.23 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:08 [gpu_model_runner.py:2392] Model loading took 14.9889 GiB and 29.237619 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:14 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/3dcdb3babd/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:14 [backends.py:550] Dynamo bytecode transform time: 5.48 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:17 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:53:38 [kv_cache_utils.py:864] GPU KV cache size: 451,616 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:53:38 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 55.13x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 2/67 [00:00<00:03, 18.42it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 26.26it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m INFO 02-02 16:53:42 [core.py:218] init engine (profile, create kv cache, warmup model) took 33.92 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:36 [backends.py:215] Compiling a graph for dynamic shape takes 22.17 s\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:42 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.49 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=8666, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:42 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 14.99 GiB for weight, 1.19 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.49 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=58513266073` to fit into requested memory, or `--kv-cache-memory=66497608704` to fully utilize gpu memory. Current kv cache memory in use is 59194840473 bytes.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 28226\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:37 [monitor.py:34] torch.compile takes 27.65 s in total\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m INFO 02-02 16:53:38 [gpu_worker.py:298] Available KV cache memory: 55.13 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m WARNING 02-02 16:53:43 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 02-02 16:53:43 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:53:43,861 default_LLMServer:my-llama-3_1-8b hfio14ia -- Started vLLM engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-8b pid=3289, ip=10.128.7.133)\u001b[0m INFO 2026-02-02 16:53:46,074 default_LLMServer:my-llama-3_1-8b hfio14ia da62b6a3-d1fe-4d06-809b-8918cb9fbedc -- CALL llm_config OK 1.8ms\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]ineCore_DP0 pid=3815)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=8666)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 25.54it/s]\u001b[32m [repeated 45x across cluster]\u001b[0mer pid=8666)\u001b[0m \n",
      "INFO 2026-02-02 16:53:46,915 serve 32651 -- Application 'default' is ready at http://0.0.0.0:8000/.\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:03<00:00, 22.16it/s]\u001b[32m [repeated 5x across cluster]\u001b[0mWrapper pid=8666)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-TOKEN>\"\n",
    "!serve run serve_llama_3_1_8b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df944967",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "---\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000`. You can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`.\n",
    "\n",
    "Example curl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5309437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-a2f0022b-7eb6-4a91-ada9-90f4803e6797\",\"object\":\"chat.completion\",\"created\":1770080037,\"model\":\"my-llama-3.1-8b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"2 + 2 = 4.\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":43,\"total_tokens\":52,\"completion_tokens\":9,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \"model\": \"my-llama-3.1-8b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623a30f",
   "metadata": {},
   "source": [
    "Example Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bedc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-02 16:54:01,228\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 24568, \"job_id\": \"06000000\", \"worker_id\": \"06000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"8d696e3ba116570fd9a9c4f537079bba3a3b1922744deafae4f33b40\", \"timestamp_ns\": 1770080041228427165}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and SchrÃ¶dinger's cat?\" \n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\""
     ]
    }
   ],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"FAKE_KEY\"\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama-3.1-8b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095ebf3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your LLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fd3dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 16:54:10,871\tSUCC scripts.py:774 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81fa41",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Deploy to production with Anyscale Services\n",
    "\n",
    "For production deployment, use Anyscale Services to deploy the Ray Serve app to a dedicated cluster without modifying the code. Anyscale ensures scalability, fault tolerance, and load balancing, keeping the service resilient against node failures, high traffic, and rolling updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Anyscale provides out-of-the-box images (`anyscale/ray-llm`) which come pre-loaded with Ray Serve LLM, vLLM, and all required GPU/runtime dependencies. This makes it easy to get started without building a custom image.\n",
    "\n",
    "Create your Anyscale Service configuration in a new `service.yaml` file:\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: deploy-llama-3-8b\n",
    "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "  # Point to your app in your Python module\n",
    "  - import_path: serve_llama_3_1_8b:app\n",
    "```\n",
    "\n",
    "\n",
    "Deploy your service with the following command. Make sure to forward your Hugging Face token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3b53a",
   "metadata": {
    "pygments_lexer": "bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +1.4s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mStarting new service 'deploy-llama-3-8b'.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +4.2s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mCreated compute config: 'compute-v1-9806d5b4d0b5a96919faa02648ca9808:1'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +4.2s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the compute config in the UI: 'https://console.anyscale.com/v2/cld_a6j8iubw9rqbyigfwk9fut4amk/compute-configs/cpt_cfy6baqr735ajukmcnd7gchb9d'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +4.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUploading local dir '.' to cloud storage.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +6.8s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mIncluding workspace-managed pip dependencies.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +7.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService 'deploy-llama-3-8b' deployed (version ID: lw3ct86r).\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +7.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI: 'https://console.anyscale.com/services/service2_3uci5eev33iyijv8e5nzzmdn9x'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +7.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mQuery the service once it's running using the following curl command (add the path you want to query):\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +7.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mcurl -H \"Authorization: Bearer thbyLl-BMyFJm5sFs_SUko-0AYEhdR92xp4wiNvVa8Y\" https://deploy-llama-3-8b-kwkre.cld-a6j8iubw9rqbyigf.s.anyscaleuserdata.com/\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service deploy -f service.yaml --env HF_TOKEN=<<YOUR-HUGGINGFACE-TOKEN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6de36c",
   "metadata": {},
   "source": [
    "**Custom Dockerfile**  \n",
    "You can customize the container by building your own Dockerfile. In your Anyscale Service config, reference the Dockerfile with `containerfile` (instead of `image_uri`):\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "# Replace:\n",
    "# image_uri: anyscale/ray-llm:2.49.0-py311-cu128\n",
    "\n",
    "# with:\n",
    "containerfile: ./Dockerfile\n",
    "```\n",
    "\n",
    "See the [Anyscale base images](https://docs.anyscale.com/reference/base-images) for details on what each image includes.\n",
    "\n",
    "---\n",
    "\n",
    "### Send requests \n",
    "\n",
    "The `anyscale service deploy` command output shows both the endpoint and authentication token:\n",
    "```console\n",
    "(anyscale +3.9s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "You can also retrieve both from the service page in the Anyscale Console. Click the **Query** button at the top. See [Send requests](#send-requests) for example requests, but make sure to use the correct endpoint and authentication token.  \n",
    "\n",
    "---\n",
    "\n",
    "### Access the Serve LLM dashboard\n",
    "\n",
    "See [Enable LLM monitoring](#enable-llm-monitoring) for instructions on enabling LLM-specific logging. To open the Ray Serve LLM Dashboard from an Anyscale Service:\n",
    "1. In the Anyscale console, go to your **Service** or **Workspace**.\n",
    "2. Navigate to the **Metrics** tab.\n",
    "3. Expand **View in Grafana** and click **Serve LLM Dashboard**.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your Anyscale Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "474b2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +2.0s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService service2_3uci5eev33iyijv8e5nzzmdn9x terminate initiated.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +2.0s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI at https://console.anyscale.com/services/service2_3uci5eev33iyijv8e5nzzmdn9x\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service terminate -n deploy-llama-3-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f67c39",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Enable LLM monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "- Token throughput (tokens/sec).\n",
    "- Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT).\n",
    "- KV cache utilization.\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`. Ensure vLLM V1 is active with `VLLM_USE_V1: \"1\"`.\n",
    "\n",
    "**Note:** `VLLM_USE_V1: \"1\"` is the default value with `ray >= 2.48.0` and can be omitted.\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        runtime_env:\n",
    "          env_vars:\n",
    "            VLLM_USE_V1: \"1\"\n",
    "        ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improve concurrency\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/stable/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.\n",
    "\n",
    "Example log:\n",
    "```console\n",
    "INFO 08-06 20:15:53 [executor_base.py:118] Maximum concurrency for 8192 tokens per request: 3.53x\n",
    "```\n",
    "\n",
    "You can improve concurrency depending on your model and hardware in several ways:  \n",
    "\n",
    "**Reduce `max_model_len`**  \n",
    "Lowering `max_model_len` reduces the memory needed for KV cache.\n",
    "\n",
    "**Example:** Running *llama-3.1-8&nbsp;B* on an A10G or L4 GPU:\n",
    "- `max_model_len = 8192` â†’ concurrency â‰ˆ 3.5\n",
    "- `max_model_len = 4096` â†’ concurrency â‰ˆ 7\n",
    "\n",
    "**Use Quantized Models**  \n",
    "Quantizing your model (for example, to FP8) reduces the model's memory footprint, freeing up memory for more KV cache and enabling more concurrent requests.\n",
    "\n",
    "**Use Tensor Parallelism**  \n",
    "Distribute the model across multiple GPUs with `tensor_parallel_size > 1`.\n",
    "\n",
    "**Note:** Latency may rise if GPUs donâ€™t have strong GPU interconnect like NVLink.\n",
    "\n",
    "**Upgrade to GPUs with more memory**  \n",
    "Some GPUs provide significantly more room for KV cache and allow for higher concurrency out of the box.\n",
    "\n",
    "**Scale with more Replicas**  \n",
    "In addition to tuning per-replica concurrency, you can scale *horizontally* by increasing the number of replicas in your config.  \n",
    "Raising the replica count increases the total number of concurrent requests your service can handle, especially under sustained or bursty traffic.\n",
    "```yaml\n",
    "deployment_config:\n",
    "  autoscaling_config:\n",
    "    min_replicas: 1\n",
    "    max_replicas: 4\n",
    "```\n",
    "\n",
    "*For more details on tuning strategies, hardware guidance, and serving configurations, see [Choose a GPU for LLM serving](https://docs.anyscale.com/llm/serving/gpu-guidance) and [Tune parameters for LLMs on Anyscale services](https://docs.anyscale.com/llm/serving/parameter-tuning).*\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Hugging Face authentication errors**  \n",
    "Some models, such as Llama-3.1, are gated and require prior authorization from the organization. See your modelâ€™s documentation for instructions on obtaining access.\n",
    "\n",
    "**Out-of-memory errors**  \n",
    "Out-of-memory (OOM) errors are one of the most common failure modes when deploying LLMs, especially as model sizes and context length increase.  \n",
    "See this [Troubleshooting Guide](https://docs.anyscale.com/overview) for common errors and how to fix them.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a small-sized LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray cluster, and how to send requests. You also learned how to monitor your app and common troubleshooting issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
