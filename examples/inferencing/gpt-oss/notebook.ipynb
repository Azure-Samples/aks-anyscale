{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a51548b",
   "metadata": {},
   "source": [
    "# Deploy gpt-oss\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fgpt-oss\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/gpt-oss\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "*gpt-oss* is a family of open-source models designed for general-purpose language understanding and generation. The 20&nbsp;B parameter variant (`gpt-oss-20b`) offers strong reasoning capabilities with lower latency. This makes it well-suited for local or specialized use cases. The larger 120&nbsp;B parameter variant (`gpt-oss-120b`) is designed for production-scale, high-reasoning workloads.\n",
    "\n",
    "For more information, see the [gpt-oss collection](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4).\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Below are example configurations for both gpt-oss-20b and gpt-oss-120b, depending on your hardware and use case.\n",
    "\n",
    "---\n",
    "\n",
    "### gpt-oss-20b\n",
    "\n",
    "To deploy a small-sized model such as gpt-oss-20b, a single GPU is sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86070ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 12:29:46 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 12:29:47,943\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
      "2026-02-06 12:29:47,954\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-06 12:29:47,956\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip' (0.04MiB) to Ray cluster...\n",
      "2026-02-06 12:29:47,957\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-06 12:29:47,966 serve 601183 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 12:29:47,967 serve 601183 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-gpt-oss',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/gpt-oss'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip'}}}\n",
      "INFO 2026-02-06 12:29:47,997 serve 601183 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 12:29:47,998 serve 601183 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n"
     ]
    }
   ],
   "source": [
    "# serve_gpt_oss.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-gpt-oss\",\n",
    "        model_source=\"openai/gpt-oss-20b\",\n",
    "    ),\n",
    "    experimental_configs=dict(num_ingress_replicas=1),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb0b16",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### gpt-oss-120b\n",
    "\n",
    "To deploy a medium-sized model such as `gpt-oss-120b`, a single node with multiple GPUs is sufficient. Set `tensor_parallel_size` to distribute the modelâ€™s weights across the GPUs in your instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac648e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2026-02-06 12:29:51,535 serve 601183 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 12:29:51,536 serve 601183 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-gpt-oss',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}, {'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/gpt-oss'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip'}}}\n",
      "INFO 2026-02-06 12:29:51,561 serve 601183 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 12:29:51,561 serve 601183 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n"
     ]
    }
   ],
   "source": [
    "# serve_gpt_oss.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-gpt-oss\",\n",
    "        model_source=\"openai/gpt-oss-120b\",\n",
    "    ),\n",
    "    experimental_configs=dict(num_ingress_replicas=1),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768,\n",
    "        tensor_parallel_size=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a7140",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate to using a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. For an example, see [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment).\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "gpt-oss integration is available starting from `ray>=2.49.0` and `vllm==0.10.1`.\n",
    "\n",
    "```bash\n",
    "pip install \"ray[serve,llm]>=2.49.0\"\n",
    "pip install \"vllm==0.10.1\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Follow the instructions in [Configure Ray Serve LLM](#configure-ray-serve-llm) according to the model size you choose, and define your app in a Python module `serve_gpt_oss.py`.\n",
    "\n",
    "In a terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdb0921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 12:30:00,768\tINFO scripts.py:507 -- Running import path: 'serve_gpt_oss:app'.\n",
      "INFO 02-06 12:30:03 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "Set the 'GPT_OSS_SIZE' environment variable to '20b' or '120b' to use the appropriate config for your model.\n",
      "Using GPT-OSS size: 20b\n",
      "2026-02-06 12:30:04,273\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
      "2026-02-06 12:30:04,283\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-06 12:30:04,284\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip' (0.04MiB) to Ray cluster...\n",
      "2026-02-06 12:30:04,284\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-06 12:30:04,293 serve 606405 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 12:30:04,293 serve 606405 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-gpt-oss',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/gpt-oss'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip'}}}\n",
      "INFO 2026-02-06 12:30:04,320 serve 606405 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 12:30:04,320 serve 606405 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n",
      "\u001b[36m(ProxyActor pid=606598)\u001b[0m INFO 2026-02-06 12:30:06,568 proxy 10.128.5.219 -- Proxy starting on node 1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0 (HTTP port: 8000).\n",
      "INFO 2026-02-06 12:30:06,609 serve 606405 -- Started Serve in namespace \"serve\".\n",
      "INFO 2026-02-06 12:30:06,635 serve 606405 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001b[36m(ProxyActor pid=606598)\u001b[0m INFO 2026-02-06 12:30:06,606 proxy 10.128.5.219 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,710 controller 606519 -- Deploying new version of Deployment(name='LLMServer:my-gpt-oss', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,711 controller 606519 -- Deploying new version of Deployment(name='OpenAiIngress', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=606598)\u001b[0m INFO 2026-02-06 12:30:06,714 proxy 10.128.5.219 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=606598)\u001b[0m WARNING 2026-02-06 12:30:06,717 proxy 10.128.5.219 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=606598)\u001b[0m INFO 2026-02-06 12:30:06,720 proxy 10.128.5.219 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f0e5c331b10>.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,814 controller 606519 -- Adding 1 replica to Deployment(name='LLMServer:my-gpt-oss', app='default').\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,814 controller 606519 -- Assigned rank 0 to new replica r6jnldck during startup\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,816 controller 606519 -- Adding 1 replica to Deployment(name='OpenAiIngress', app='default').\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m INFO 2026-02-06 12:30:06,816 controller 606519 -- Assigned rank 0 to new replica y15utuzp during startup\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:14,020 default_LLMServer:my-gpt-oss r6jnldck -- Downloading the tokenizer for openai/gpt-oss-20b\n",
      "\u001b[36m(ProxyActor pid=28911, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:14,551 proxy 10.128.8.94 -- Proxy starting on node 05ec9d1e0c7dd8a2528884609513faad12d64730f3b39a8a793702d6 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=28911, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:14,646 proxy 10.128.8.94 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=28911, ip=10.128.8.94)\u001b[0m WARNING 2026-02-06 12:30:14,651 proxy 10.128.8.94 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=28911, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:14,661 proxy 10.128.8.94 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7edbc8f54590>.\n",
      "\u001b[36m(pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:21 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:30 [__init__.py:742] Resolved architecture: GptOssForCausalLM\n",
      "Parse safetensors files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Parse safetensors files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.11it/s]\n",
      "Parse safetensors files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.02it/s]\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:31 [__init__.py:1815] Using max model len 32768\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:30:32 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:30:32 [__init__.py:1217] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:32 [arg_utils.py:1208] Using ray runtime env: {'ray_debugger': {'working_dir': '/home/ray/default/gpt-oss'}, 'working_dir': 'gcs://_ray_pkg_4960ab25ad6aa94246a3c1fe8cb24cfe055fb481.zip', 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[36m(_get_vllm_engine_config pid=29119, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:32 [config.py:284] Overriding max cuda graph capture size to 1024 for performance.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m You are using a model of type gpt_oss to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m You are using a model of type gpt_oss to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:34,813 default_LLMServer:my-gpt-oss r6jnldck -- Clearing the current platform cache ...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:30:34,815 default_LLMServer:my-gpt-oss r6jnldck -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:30:36 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m WARNING 2026-02-06 12:30:36,872 controller 606519 -- Deployment 'LLMServer:my-gpt-oss' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m WARNING 2026-02-06 12:30:36,873 controller 606519 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:40 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:43 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='openai/gpt-oss-20b', speculative_config=None, tokenizer='openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend='openai_gptoss'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=my-gpt-oss, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[1024,1008,992,976,960,944,928,912,896,880,864,848,832,816,800,784,768,752,736,720,704,688,672,656,640,624,608,592,576,560,544,528,512,496,480,464,448,432,416,400,384,368,352,336,320,304,288,272,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":1024,\"local_cache_dir\":null}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m 2026-02-06 12:30:43,674\tINFO worker.py:1692 -- Using address ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379 set in the environment variable RAY_ADDRESS\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m 2026-02-06 12:30:43,681\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m 2026-02-06 12:30:43,781\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:43 [ray_utils.py:324] Using the existing placement group\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:43 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m   warnings.warn(\n",
      "\u001b[36m(pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(pid=29291)\u001b[0m INFO 02-06 12:30:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:51 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:51 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_WORKER_MULTIPROC_METHOD', 'LD_LIBRARY_PATH', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_RAY_SPMD_WORKER']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:30:51 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [W206 12:30:53.033721353 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [W206 12:30:53.036035274 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [W206 12:30:53.036444340 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:30:53 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [gpu_model_runner.py:2338] Starting to load model openai/gpt-oss-20b...\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [cuda.py:357] Using Triton backend on V1 engine.\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:53 [mxfp4.py:93] Using Marlin backend\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:54 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.22it/s]\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:58 [default_loader.py:268] Loading weights took 4.02 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:30:58 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:30:59 [gpu_model_runner.py:2392] Model loading took 13.7194 GiB and 5.225259 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:03 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/4992426044/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:03 [backends.py:550] Dynamo bytecode transform time: 4.33 s\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.815 s\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:06 [marlin_utils.py:353] You are running Marlin kernel with bf16 on GPUs before SM90. You can consider change to fp16 to achieve better performance if possible.\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:06 [monitor.py:34] torch.compile takes 4.33 s in total\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m WARNING 2026-02-06 12:31:06,903 controller 606519 -- Deployment 'LLMServer:my-gpt-oss' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m WARNING 2026-02-06 12:31:06,904 controller 606519 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=606519)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [W206 12:30:53.036035274 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [W206 12:30:53.036444340 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:03<00:00,  1.17s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m6m(RayWorkerWrapper pid=29291)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:07 [gpu_worker.py:298] Available KV cache memory: 55.74 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m WARNING 02-06 12:30:53 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [gpu_model_runner.py:2338] Starting to load model openai/gpt-oss-20b...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [cuda.py:357] Using Triton backend on V1 engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:53 [mxfp4.py:93] Using Marlin backend\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:54 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:58 [default_loader.py:268] Loading weights took 4.02 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m WARNING 02-06 12:30:58 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:30:59 [gpu_model_runner.py:2392] Model loading took 13.7194 GiB and 5.225259 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:31:07 [kv_cache_utils.py:1028] GPU KV cache size: 1,217,744 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:31:07 [kv_cache_utils.py:1032] Maximum concurrency for 32,768 tokens per request: 69.66x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/83 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 2/83 [00:00<00:07, 11.21it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 76/83 [00:05<00:00, 15.55it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/83 [00:00<?, ?it/s]re_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 74/83 [00:04<00:00, 15.20it/s]\u001b[32m [repeated 73x across cluster]\u001b[0mid=29291)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:05<00:00, 15.13it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m INFO 02-06 12:31:13 [core.py:218] init engine (profile, create kv cache, warmup model) took 14.21 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:03 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/4992426044/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:03 [backends.py:550] Dynamo bytecode transform time: 4.33 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.815 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:06 [marlin_utils.py:353] You are running Marlin kernel with bf16 on GPUs before SM90. You can consider change to fp16 to achieve better performance if possible.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:06 [monitor.py:34] torch.compile takes 4.33 s in total\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m INFO 02-06 12:31:07 [gpu_worker.py:298] Available KV cache memory: 55.74 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:13 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 0.72 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=29291, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:13 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 13.72 GiB for weight, 1.84 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.72 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=58927771033` to fit into requested memory, or `--kv-cache-memory=66912113664` to fully utilize gpu memory. Current kv cache memory in use is 59854712217 bytes.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:15 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 152218\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:15 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 02-06 12:31:15 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:31:15 [serving_responses.py:147] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:31:16,050 default_LLMServer:my-gpt-oss r6jnldck -- Started vLLM engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-gpt-oss pid=28702, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:31:16,174 default_LLMServer:my-gpt-oss r6jnldck 878e1e14-744e-410f-9a19-7859a63ee637 -- CALL llm_config OK 1.8ms\n",
      "INFO 2026-02-06 12:31:16,855 serve 606405 -- Application 'default' is ready at http://0.0.0.0:8000/.\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 80/83 [00:05<00:00, 16.05it/s]\u001b[32m [repeated 5x across cluster]\u001b[0mper pid=29291)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:05<00:00, 15.13it/s]d=29220)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=29291)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "!serve run serve_gpt_oss:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df944967",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as Ray provisions the cluster, the vLLM server starts, and Ray Serve downloads the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000`. You can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`.\n",
    "\n",
    "#### Example curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5309437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-6c3d0b3f-44d3-42ed-8f8f-a232b6c048b7\",\"object\":\"chat.completion\",\"created\":1770409887,\"model\":\"my-gpt-oss\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The word **â€œstrawberryâ€** contains **3** occurrences of the letter **â€œRâ€** (two consecutive ones at the end plus one earlier).\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":\"User asks \\\"How many Rs in strawberry ?\\\" Possibly counting letter \\\"R\\\" characters in the word \\\"strawberry\\\". \\\"strawberry\\\" contains 'r' twice. But maybe they mean Rs as in rupees? Could be like \\\"How many Rs in strawberry?\\\" expecting count of letters 'r' is 2. let's answer that.\\n\\nBut clarify: \\\"s t r a w b e r r y\\\" has 3 r's? Wait let's check: letters: s(1), t(2), r(3), a(4), w(5), b(6), e(7), r(8), r(9), y(10). Actually there are 3 r's: positions 3,8,9? Wait position 9 is r? Let's write: s t r a w b e r r y. Actually after e, r, r, y. So yes r appears at index 3, 8, and 9? Wait we need to map: indices: 1 s, 2 t, 3 r, 4 a, 5 w, 6 b, 7 e, 8 r, 9 r, 10 y -> 3 r's. So answer: 3 Rs.\\n\\nBut double-check: \\\"strawberry\\\" spelled with two r's? Usually \\\"strawberry\\\" has double r? Actually it's \\\"strawberry\\\" has 'rr' at the end before y. So yes there are 3 r's? But I think \\\"strawberry\\\" has two r's separate? Wait let's quick mental: strawberry: s t r a w b e r r y. That is two consecutive r's at near end. Counting r's: r after t, and then r r. So total r's: 3? But the two at end count as two separate r's indeed. So total r's = 3.\\n\\nThus answer: 3 'Rs'.\\n\\nAnswer accordingly.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":75,\"total_tokens\":524,\"completion_tokens\":449,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \"model\": \"my-gpt-oss\", \"messages\": [{\"role\": \"user\", \"content\": \"How many Rs in strawberry ?\"}] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623a30f",
   "metadata": {},
   "source": [
    "#### Example Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bedc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-06 12:31:36,587\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 601183, \"job_id\": \"17000000\", \"worker_id\": \"17000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770409896587068889}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user: \"How many r's in strawberry\". They want probably to count letters 'r' in the word strawberry. Count including case? In \"strawberry\", letters: s t r a w b e r r y. There are 'r's at positions 3, 8, 9: 3 r's. Maybe answer: 3. Straight up. No trick. Might be a riddle? Some say \"strawberry\" has 3 r's. Could code or just answer. Might also consider uppercase R? But it's letter. So answer: 3. Maybe also ask \"How many r's in strawberry\"? So answer directly.The word **strawberry** contains **3â€¯râ€™s**."
     ]
    }
   ],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example query\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How many r's in strawberry\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream\n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095ebf3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Shut down the service\n",
    "\n",
    "To shutdown your LLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd3dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 12:31:52,253\tSUCC scripts.py:774 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81fa41",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Deploy to production with Anyscale services\n",
    "\n",
    "For production deployment, use Anyscale services to deploy the Ray Serve app to a dedicated cluster without modifying the code. Anyscale ensures scalability, fault tolerance, and load balancing, keeping the service resilient against node failures, high traffic, and rolling updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Anyscale provides out-of-the-box images (`anyscale/ray-llm`), which come pre-loaded with Ray Serve LLM, vLLM, and all required GPU and runtime dependencies. See the [Anyscale base images](https://docs.anyscale.com/reference/base-images) for details on what each image includes.\n",
    "\n",
    "Build a minimal Dockerfile:\n",
    "```Dockerfile\n",
    "FROM anyscale/ray:2.49.0-slim-py312-cu128\n",
    "\n",
    "# C compiler for Tritonâ€™s runtime build step (vLLM V1 engine)\n",
    "# https://github.com/vllm-project/vllm/issues/2997\n",
    "RUN sudo apt-get update && \\\n",
    "    sudo apt-get install -y --no-install-recommends build-essential\n",
    "\n",
    "RUN pip install vllm==0.10.1\n",
    "```\n",
    "\n",
    "Create your Anyscale service configuration in a new `service.yaml` file and reference the Dockerfile with `containerfile`:\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: deploy-gpt-oss\n",
    "containerfile: ./Dockerfile # Build Ray Serve LLM with vllm==0.10.1\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "  # Point to your app in your Python module\n",
    "  - import_path: serve_gpt_oss:app\n",
    "```\n",
    "\n",
    "\n",
    "Deploy your service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa0556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +1.4s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mRestarting existing service 'deploy-gpt-oss'.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.3s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mCreated compute config: 'compute-v1-9806d5b4d0b5a96919faa02648ca9808:1'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.3s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the compute config in the UI: 'https://console.anyscale.com/v2/cld_a6j8iubw9rqbyigfwk9fut4amk/compute-configs/cpt_cfy6baqr735ajukmcnd7gchb9d'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +11.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUploading local dir '.' to cloud storage.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +13.7s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mIncluding workspace-managed pip dependencies.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +14.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService 'deploy-gpt-oss' deployed (version ID: dleekrem).\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +14.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI: 'https://console.anyscale.com/services/service2_vpr8lh7agmu6p8srlilj2qf8dt'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +14.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mQuery the service once it's running using the following curl command (add the path you want to query):\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +14.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mcurl -H \"Authorization: Bearer Ou2VmqwsZAV_DOrZvMpJU7CXXeLpMFd17HNRm5XEbdY\" https://deploy-gpt-oss-kwkre.cld-a6j8iubw9rqbyigf.s.anyscaleuserdata.com/\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service deploy -f service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6de36c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Send requests \n",
    "\n",
    "The `anyscale service deploy` command output shows both the endpoint and authentication token:\n",
    "\n",
    "```console\n",
    "(anyscale +3.9s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "\n",
    "You can also retrieve both from the service page in the Anyscale console. Click **Query** at the top. See [Send requests](#send-requests) for example requests, but make sure to use the correct endpoint and authentication token.  \n",
    "\n",
    "---\n",
    "\n",
    "### Access the Serve LLM dashboard\n",
    "\n",
    "For instructions on enabling LLM-specific logging, see [Enable LLM monitoring](#enable-llm-monitoring). To open the Ray Serve LLM Dashboard from an Anyscale service:\n",
    "\n",
    "1. In the Anyscale console, go to the **Service** or **Workspace** tab.\n",
    "1. Navigate to the **Metrics** tab.\n",
    "1. Click **View in Grafana** and click **Serve LLM Dashboard**.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "To shutdown your Anyscale Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474b2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +2.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService service2_vpr8lh7agmu6p8srlilj2qf8dt terminate initiated.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +2.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI at https://console.anyscale.com/services/service2_vpr8lh7agmu6p8srlilj2qf8dt\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service terminate -n deploy-gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f67c39",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Enable LLM monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "- Token throughput (tokens/sec).\n",
    "- Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT).\n",
    "- KV cache utilization.\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`:\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improve concurrency\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/stable/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.\n",
    "\n",
    "Example log for gpt-oss-20b with 1xL4:\n",
    "```console\n",
    "INFO 09-08 17:34:28 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 5.22x\n",
    "```\n",
    "\n",
    "Example log for gpt-oss-120b with 2xL40S:\n",
    "```console\n",
    "INFO 09-09 00:32:32 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 6.18x\n",
    "```\n",
    "\n",
    "To improve concurrency for gpt-oss models, see [Deploy a small-sized LLM: Improve concurrency](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html#improve-concurrency) for small-sized models such as `gpt-oss-20b`, and [Deploy a medium-sized LLM: Improve concurrency](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html#improve-concurrency) for medium-sized models such as `gpt-oss-120b`.\n",
    "\n",
    "**Note:** Some example guides recommend using quantization to boost concurrency. `gpt-oss` weights are already 4-bit by default, so further quantization typically isnâ€™t applicable.  \n",
    "\n",
    "For broader guidance, also see [Choose a GPU for LLM serving](https://docs.anyscale.com/llm/serving/gpu-guidance) and [Optimize performance for Ray Serve LLM](https://docs.anyscale.com/llm/serving/performance-optimization).\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning configuration\n",
    "\n",
    "You donâ€™t need a custom reasoning parser when deploying `gpt-oss` with Ray Serve LLM, you can access the reasoning content in the model's response directly. You can also control the reasoning effort of the model in the request.\n",
    "\n",
    "---\n",
    "\n",
    "### Access reasoning output\n",
    "\n",
    "The reasoning content is available directly in the `reasoning_content` field of the response:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Control reasoning effort\n",
    "\n",
    "`gpt-oss` supports [three reasoning levels](https://huggingface.co/openai/gpt-oss-20b#reasoning-levels): **low**, **medium**, and **high**. The default level is **medium**.\n",
    "\n",
    "You can control reasoning with the `reasoning_effort` request parameter:  \n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What are the three main touristic spots to see in Paris?\"}\n",
    "    ],\n",
    "    reasoning_effort=\"low\" # Or \"medium\", \"high\"\n",
    ")\n",
    "```\n",
    "\n",
    "You can also set a level explicitly in the system prompt:  \n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Reasoning: low. You are an AI travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the three main touristic spots to see in Paris?\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** There's no reliable way to completely disable reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Can't download the vocab file  \n",
    "```console\n",
    "openai_harmony.HarmonyError: error downloading or loading vocab file: failed to download or load vocab\n",
    "```\n",
    "\n",
    "The `openai_harmony` library needs the *tiktoken* encoding files and tries to fetch them from OpenAI's public host. Common causes include:\n",
    "- Corporate firewall or proxy blocks `openaipublic.blob.core.windows.net`. You may need to whitelist this domain.\n",
    "- Intermittent network issues.\n",
    "- Race conditions when multiple processes try to download to the same cache. This can happen when [deploying multiple models at the same time](https://github.com/openai/harmony/pull/41).\n",
    "\n",
    "You can also directly download the *tiktoken* encoding files in advance and set the `TIKTOKEN_ENCODINGS_BASE` environment variable:\n",
    "```bash\n",
    "mkdir -p tiktoken_encodings\n",
    "wget -O tiktoken_encodings/o200k_base.tiktoken \"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\"\n",
    "wget -O tiktoken_encodings/cl100k_base.tiktoken \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\"\n",
    "export TIKTOKEN_ENCODINGS_BASE=${PWD}/tiktoken_encodings\n",
    "```\n",
    "\n",
    "### `gpt-oss` architecture not recognized \n",
    "```console\n",
    "Value error, The checkpoint you are trying to load has model type `gpt_oss` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n",
    "```\n",
    "Older vLLM and Transformers versions don't register `gpt_oss`, raising an error when vLLM hands off to Transformers. Upgrade **vLLM â‰¥ 0.10.1** and let your package resolver such as `pip` handle the other dependencies.\n",
    "```bash\n",
    "pip install -U \"vllm>=0.10.1\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to deploy `gpt-oss` models with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on a Ray cluster, send requests, and monitor your service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
