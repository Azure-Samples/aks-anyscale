{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f6fcbd",
   "metadata": {},
   "source": [
    "# Deploy a medium-sized LLM\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fmedium-size-llm\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/medium-size-llm\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "A medium LLM typically runs on a single node with 4-8 GPUs. It offers a balance between performance and efficiency. These models provide stronger accuracy and reasoning than small models while remaining more affordable and resource-friendly than very large ones. This makes them a solid choice for production workloads that need good quality at lower cost. They're also ideal for scaling applications where large models would be too slow or expensive.\n",
    "\n",
    "This tutorial deploys a medium-sized LLM using Ray Serve LLM. For smaller models, see [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html), and for larger models, see [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "You can deploy a medium-sized LLM on a single node with multiple GPUs. To leverage all available GPUs, set `tensor_parallel_size` to the number of GPUs on the node, which distributes the modelâ€™s weights evenly across them.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Set your Hugging Face token in the config file to access gated models like `Llama-3.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d185d580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 10:22:46 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 10:22:48,455\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
      "2026-02-06 10:22:48,466\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-06 10:22:48,468\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_a8cdb4606e8abebcc63e70f4b349cf5a4cc18247.zip' (0.03MiB) to Ray cluster...\n",
      "2026-02-06 10:22:48,469\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_a8cdb4606e8abebcc63e70f4b349cf5a4cc18247.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-06 10:22:48,479 serve 538368 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 10:22:48,480 serve 538368 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-llama-3_1-70b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'env_vars': {'HF_TOKEN': None},\n",
      "                                       'ray_debugger': {'working_dir': '/home/ray/default/medium-size-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_a8cdb4606e8abebcc63e70f4b349cf5a4cc18247.zip'}}}\n",
      "INFO 2026-02-06 10:22:48,518 serve 538368 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 10:22:48,518 serve 538368 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +23s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "# serve_llama_3_1_70b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama-3.1-70b\",\n",
    "        # Or unsloth/Meta-Llama-3.1-70B-Instruct for an ungated model\n",
    "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    ),\n",
    "    experimental_configs=dict(num_ingress_replicas=1),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    ### If your model is not gated, you can skip `HF_TOKEN`\n",
    "    # Share your Hugging Face token with the vllm engine so it can access the gated Llama 3.\n",
    "    # Type `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>` in a terminal\n",
    "    runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768,\n",
    "        # Split weights among 8 GPUs in the node\n",
    "        tensor_parallel_size=8,\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2231a5",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate to using a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. See [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Metaâ€™s Llama. Store it in `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>`.\n",
    "\n",
    "**Note: **Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Metaâ€™s Llama model approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_llama_3_1_70b.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9da12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 10:24:04,046\tINFO scripts.py:507 -- Running import path: 'serve_llama_3_1_70b:app'.\n",
      "INFO 02-06 10:24:06 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "2026-02-06 10:24:08,001\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
      "2026-02-06 10:24:08,011\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-06 10:24:08,013\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_879c615e534218819702187d5c3db1491730a404.zip' (0.03MiB) to Ray cluster...\n",
      "2026-02-06 10:24:08,013\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_879c615e534218819702187d5c3db1491730a404.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-06 10:24:08,059 serve 539218 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 10:24:08,061 serve 539218 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-llama-3_1-70b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'env_vars': {'HF_TOKEN': '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'},\n",
      "                                       'ray_debugger': {'working_dir': '/home/ray/default/medium-size-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_879c615e534218819702187d5c3db1491730a404.zip'}}}\n",
      "INFO 2026-02-06 10:24:08,093 serve 539218 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 10:24:08,093 serve 539218 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n",
      "\u001b[36m(ProxyActor pid=539442)\u001b[0m INFO 2026-02-06 10:24:10,646 proxy 10.128.5.219 -- Proxy starting on node 1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0 (HTTP port: 8000).\n",
      "INFO 2026-02-06 10:24:10,728 serve 539218 -- Started Serve in namespace \"serve\".\n",
      "INFO 2026-02-06 10:24:10,758 serve 539218 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001b[36m(ProxyActor pid=539442)\u001b[0m INFO 2026-02-06 10:24:10,725 proxy 10.128.5.219 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,856 controller 539353 -- Deploying new version of Deployment(name='LLMServer:my-llama-3_1-70b', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,857 controller 539353 -- Deploying new version of Deployment(name='OpenAiIngress', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=539442)\u001b[0m INFO 2026-02-06 10:24:10,860 proxy 10.128.5.219 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,961 controller 539353 -- Adding 1 replica to Deployment(name='LLMServer:my-llama-3_1-70b', app='default').\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,961 controller 539353 -- Assigned rank 0 to new replica bkzhdls8 during startup\n",
      "\u001b[36m(ProxyActor pid=539442)\u001b[0m WARNING 2026-02-06 10:24:10,865 proxy 10.128.5.219 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=539442)\u001b[0m INFO 2026-02-06 10:24:10,869 proxy 10.128.5.219 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f77bc89b610>.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,962 controller 539353 -- Adding 1 replica to Deployment(name='OpenAiIngress', app='default').\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m INFO 2026-02-06 10:24:10,962 controller 539353 -- Assigned rank 0 to new replica 8n810ls2 during startup\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:19 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:21,016 default_LLMServer:my-llama-3_1-70b bkzhdls8 -- Downloading the tokenizer for meta-llama/Llama-3.1-70B-Instruct\n",
      "\u001b[36m(ProxyActor pid=1211, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:21,179 proxy 10.128.8.94 -- Proxy starting on node 05ec9d1e0c7dd8a2528884609513faad12d64730f3b39a8a793702d6 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=1211, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:21,285 proxy 10.128.8.94 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=1211, ip=10.128.8.94)\u001b[0m WARNING 2026-02-06 10:24:21,291 proxy 10.128.8.94 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=1211, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:21,300 proxy 10.128.8.94 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7ef4063743d0>.\n",
      "\u001b[36m(pid=2127, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:28 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_get_vllm_engine_config pid=2127, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:39 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "\u001b[36m(_get_vllm_engine_config pid=2127, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:39 [__init__.py:1815] Using max model len 32768\n",
      "\u001b[36m(_get_vllm_engine_config pid=2127, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:39 [arg_utils.py:1208] Using ray runtime env: {'ray_debugger': {'working_dir': '/home/ray/default/medium-size-llm'}, 'working_dir': 'gcs://_ray_pkg_879c615e534218819702187d5c3db1491730a404.zip', 'env_vars': {'HF_TOKEN': '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'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n",
      "\u001b[36m(_get_vllm_engine_config pid=2127, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:40 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:40,502 default_LLMServer:my-llama-3_1-70b bkzhdls8 -- Clearing the current platform cache ...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:24:40,508 default_LLMServer:my-llama-3_1-70b bkzhdls8 -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:24:41,019 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:24:41,019 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m WARNING 02-06 10:24:41 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:45 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:47 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=my-llama-3.1-70b, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m 2026-02-06 10:24:47,180\tINFO worker.py:1692 -- Using address ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379 set in the environment variable RAY_ADDRESS\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m 2026-02-06 10:24:47,188\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m 2026-02-06 10:24:47,246\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:48 [ray_utils.py:324] Using the existing placement group\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:48 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m   warnings.warn(\n",
      "\u001b[36m(pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:55 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:24:55 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:56 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:56 [ray_env.py:65] Copying the following environment variables to workers: ['HF_TOKEN', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_WORKER_MULTIPROC_METHOD', 'LD_LIBRARY_PATH']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:24:56 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file\n",
      "\u001b[36m(autoscaler +54s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m [W206 10:25:04.762757437 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m [W206 10:25:04.139206429 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:04 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:04 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(pid=9236)\u001b[0m INFO 02-06 10:24:55 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_c31d63f6'), local_subscribe_addr='ipc:///tmp/0a6e922c-fd9e-4dfd-9312-1649e32b5ec2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m WARNING 02-06 10:25:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:07 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:25:11,049 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:25:11,049 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m [W206 10:25:04.471552049 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m [W206 10:25:04.474867707 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:25:41,076 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:25:41,076 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:26:11,093 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:26:11,094 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:26:41,113 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:26:41,113 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:27:11,148 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:27:11,148 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:27:41,164 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:27:41,165 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:28:11,182 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:28:11,183 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:28:41,270 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:28:41,271 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:05 [weight_utils.py:369] Time spent downloading weights for meta-llama/Llama-3.1-70B-Instruct: 237.618709 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:04 [__init__.py:1433] Found nccl from library libnccl.so.2\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:04 [pynccl.py:70] vLLM is using nccl==2.27.3\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m WARNING 02-06 10:25:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2370] Loading model from scratch...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:06 [cuda.py:362] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9242, ip=10.128.8.94)\u001b[0m INFO 02-06 10:25:07 [weight_utils.py:348] Using model weights format ['*.safetensors']\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m [W206 10:25:04.471552049 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m [W206 10:25:04.474867707 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:00<00:08,  3.54it/s]\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:29:11,279 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:29:11,279 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s])\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:05<00:12,  1.46it/s]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:15 [default_loader.py:268] Loading weights took 10.10 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:16 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 248.854907 seconds\n",
      "Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:10<00:09,  1.22it/s]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Loading safetensors checkpoint shards:  87% Completed | 26/30 [00:16<00:03,  1.32it/s]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 19.97 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=9178, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 20.25 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:27 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 260.199142 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:40 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/69babfadb2/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:40 [backends.py:550] Dynamo bytecode transform time: 12.55 s\n",
      "\u001b[36m(RayWorkerWrapper pid=9245, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 19.73 seconds\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:27 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 260.168168 seconds\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:29:41,284 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:29:41,285 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:19<00:00,  1.55it/s]\u001b[32m [repeated 11x across cluster]\u001b[0m6m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:44 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:30:11,291 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:30:11,291 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(RayWorkerWrapper pid=9236, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:30 [backends.py:215] Compiling a graph for dynamic shape takes 49.67 s\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:41 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/69babfadb2/rank_1_0/backbone for vLLM's torch.compile\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:41 [backends.py:550] Dynamo bytecode transform time: 13.27 s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:29:45 [backends.py:194] Cache the graph for dynamic shape for later use\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:34 [monitor.py:34] torch.compile takes 63.82 s in total\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:36 [gpu_worker.py:298] Available KV cache memory: 52.56 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=9146, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:32 [backends.py:215] Compiling a graph for dynamic shape takes 50.88 s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(autoscaler +15s, ip=10.128.5.219)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:04 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:04 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(pid=9271)\u001b[0m INFO 02-06 10:24:55 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:06 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_c31d63f6'), local_subscribe_addr='ipc:///tmp/0a6e922c-fd9e-4dfd-9312-1649e32b5ec2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:06 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m WARNING 02-06 10:25:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:25:06 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:25:07 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:05 [weight_utils.py:369] Time spent downloading weights for meta-llama/Llama-3.1-70B-Instruct: 237.618709 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:04 [__init__.py:1433] Found nccl from library libnccl.so.2\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:04 [pynccl.py:70] vLLM is using nccl==2.27.3\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:06 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:06 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m WARNING 02-06 10:25:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-70B-Instruct...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:06 [gpu_model_runner.py:2370] Loading model from scratch...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:25:06 [cuda.py:362] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9242)\u001b[0m INFO 02-06 10:25:07 [weight_utils.py:348] Using model weights format ['*.safetensors']\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:15 [default_loader.py:268] Loading weights took 10.10 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:16 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 248.854907 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 19.97 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9178)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 20.25 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:29:27 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 260.199142 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:29:40 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/69babfadb2/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:29:40 [backends.py:550] Dynamo bytecode transform time: 12.55 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:29:26 [default_loader.py:268] Loading weights took 19.73 seconds\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:29:27 [gpu_model_runner.py:2392] Model loading took 16.4607 GiB and 260.168168 seconds\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:29:44 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:41 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/69babfadb2/rank_1_0/backbone for vLLM's torch.compile\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:41 [backends.py:550] Dynamo bytecode transform time: 13.27 s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:29:45 [backends.py:194] Cache the graph for dynamic shape for later use\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[36m(RayWorkerWrapper pid=9146)\u001b[0m INFO 02-06 10:30:32 [backends.py:215] Compiling a graph for dynamic shape takes 50.88 s\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:864] GPU KV cache size: 1,377,728 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:37 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 42.04x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|â–         | 1/67 [00:00<00:35,  1.87it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:02<00:04,  9.63it/s]\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:30:41,381 controller 539353 -- Deployment 'LLMServer:my-llama-3_1-70b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m WARNING 2026-02-06 10:30:41,382 controller 539353 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=539353)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]neCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:05<00:01, 10.39it/s]\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 62/67 [00:06<00:00,  9.93it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:07<00:00,  9.18it/s]\n",
      "\u001b[36m(RayWorkerWrapper pid=9082, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:45 [custom_all_reduce.py:203] Registering 10626 cuda graph addresses\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:30:34 [monitor.py:34] torch.compile takes 63.82 s in total\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=9242, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:37 [gpu_worker.py:298] Available KV cache memory: 52.56 GiB\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9236)\u001b[0m INFO 02-06 10:30:30 [backends.py:215] Compiling a graph for dynamic shape takes 49.67 s\n",
      "\u001b[36m(RayWorkerWrapper pid=9271, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:46 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 1.10 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=9271, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:46 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 16.46 GiB for weight, 1.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 1.1 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=55096300953` to fit into requested memory, or `--kv-cache-memory=63080512512` to fully utilize gpu memory. Current kv cache memory in use is 56432186777 bytes.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9245)\u001b[0m INFO 02-06 10:30:34 [monitor.py:34] torch.compile takes 63.72 s in total\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9242)\u001b[0m INFO 02-06 10:30:37 [gpu_worker.py:298] Available KV cache memory: 52.56 GiB\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m INFO 02-06 10:30:46 [core.py:218] init engine (profile, create kv cache, warmup model) took 78.93 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:47 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 86108\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:47 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:47 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m WARNING 02-06 10:30:47 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:47 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:48 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 02-06 10:30:48 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:30:48,132 default_LLMServer:my-llama-3_1-70b bkzhdls8 -- Started vLLM engine.\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:02<00:04,  9.63it/s]0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 10:30:50,614 default_LLMServer:my-llama-3_1-70b bkzhdls8 1f3df303-0a11-463c-b56f-b7fc3fa43ec1 -- CALL llm_config OK 2.0ms\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:06<00:00,  9.93it/s]\u001b[32m [repeated 11x across cluster]\u001b[0mr pid=9082)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:07<00:00,  9.84it/s]\u001b[32m [repeated 9x across cluster]\u001b[0mer pid=9082)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:07<00:00,  9.18it/s]0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m \n",
      "INFO 2026-02-06 10:30:51,625 serve 539218 -- Application 'default' is ready at http://0.0.0.0:8000/.\n",
      "\u001b[0m\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9082)\u001b[0m INFO 02-06 10:30:45 [custom_all_reduce.py:203] Registering 10626 cuda graph addresses\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9271)\u001b[0m INFO 02-06 10:30:46 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 1.10 GiB\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-llama-3_1-70b pid=497, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=2225)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=9271)\u001b[0m INFO 02-06 10:30:46 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 16.46 GiB for weight, 1.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 1.1 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=55096300953` to fit into requested memory, or `--kv-cache-memory=63080512512` to fully utilize gpu memory. Current kv cache memory in use is 56432186777 bytes.\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"<YOUR-HUGGINGFACE-TOKEN>\"\n",
    "!serve run serve_llama_3_1_70b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d18e22",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "---\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`.\n",
    "\n",
    "Example curl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1dd345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-e23d799a-c4fc-48d4-8daa-e06a9f60534b\",\"object\":\"chat.completion\",\"created\":1770402694,\"model\":\"my-llama-3.1-70b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"2 + 2 = 4\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":43,\"total_tokens\":51,\"completion_tokens\":8,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \"model\": \"my-llama-3.1-70b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5e4fd",
   "metadata": {},
   "source": [
    "Example Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584f01f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-06 10:31:37,085\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 538368, \"job_id\": \"03000000\", \"worker_id\": \"03000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770402697085818590}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta."
     ]
    }
   ],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"FAKE_KEY\"\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama-3.1-70b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fd1fb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your LLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c03cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 10:31:48,439\tSUCC scripts.py:774 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc223463",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Deploy to production with Anyscale services\n",
    "\n",
    "For production deployment, use Anyscale services to deploy the Ray Serve app to a dedicated cluster without modifying the code. Anyscale ensures scalability, fault tolerance, and load balancing, keeping the service resilient against node failures, high traffic, and rolling updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Anyscale provides out-of-the-box images (`anyscale/ray-llm`), which come pre-loaded with Ray Serve LLM, vLLM, and all required GPU/runtime dependencies. This makes it easy to get started without building a custom image.\n",
    "\n",
    "Create your Anyscale service configuration in a new `service.yaml` file:\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: deploy-llama-3-70b\n",
    "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "  # Point to your app in your Python module\n",
    "  - import_path: serve_llama_3_1_70b:app\n",
    "```\n",
    "\n",
    "Deploy your service. Make sure you forward your Hugging Face token to the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1c6108",
   "metadata": {
    "pygments_lexer": "bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +1.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mStarting new service 'deploy-llama-3-70b'.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +6.2s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mCreated compute config: 'compute-v1-9806d5b4d0b5a96919faa02648ca9808:1'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +6.2s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the compute config in the UI: 'https://console.anyscale.com/v2/cld_a6j8iubw9rqbyigfwk9fut4amk/compute-configs/cpt_cfy6baqr735ajukmcnd7gchb9d'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +6.8s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUploading local dir '.' to cloud storage.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +8.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mIncluding workspace-managed pip dependencies.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +10.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService 'deploy-llama-3-70b' deployed (version ID: e2mqsjzj).\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +10.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI: 'https://console.anyscale.com/services/service2_3eyixs4mlrgpgctnclxx6ekraw'\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +10.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mQuery the service once it's running using the following curl command (add the path you want to query):\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +10.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mcurl -H \"Authorization: Bearer nmbB_igd7ZKcgbL6UIjkJxLluJdFN4HOA-h-NFXmjK0\" https://deploy-llama-3-70b-kwkre.cld-a6j8iubw9rqbyigf.s.anyscaleuserdata.com/\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service deploy -f service.yaml --env HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18226fd7",
   "metadata": {},
   "source": [
    "**Custom Dockerfile**  \n",
    "You can customize the container by building your own Dockerfile. In your Anyscale Service config, reference the Dockerfile with `containerfile` (instead of `image_uri`):\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "# Replace:\n",
    "# image_uri: anyscale/ray-llm:2.49.0-py311-cu128\n",
    "\n",
    "# with:\n",
    "containerfile: ./Dockerfile\n",
    "```\n",
    "\n",
    "See the [Anyscale base images](https://docs.anyscale.com/reference/base-images) for details on what each image includes.\n",
    "\n",
    "---\n",
    "\n",
    "### Send requests \n",
    "\n",
    "The `anyscale service deploy` command output shows both the endpoint and authentication token:\n",
    "```console\n",
    "(anyscale +3.9s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "You can also retrieve both from the service page in the Anyscale console. Click the **Query** button at the top. See [Send requests](#send-requests) for example requests, but make sure to use the correct endpoint and authentication token.  \n",
    "\n",
    "---\n",
    "\n",
    "### Access the Serve LLM dashboard\n",
    "\n",
    "See [Enable LLM monitoring](#enable-llm-monitoring) for instructions on enabling LLM-specific logging. To open the Ray Serve LLM dashboard from an Anyscale service:\n",
    "1. In the Anyscale console, go to your **Service** or **Workspace**\n",
    "2. Navigate to the **Metrics** tab\n",
    "3. Click **View in Grafana** and click **Serve LLM Dashboard**\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown \n",
    " \n",
    "Shutdown your Anyscale service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211d5baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[1m\u001b[36m(anyscale +2.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mService service2_3eyixs4mlrgpgctnclxx6ekraw terminate initiated.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +2.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the service in the UI at https://console.anyscale.com/services/service2_3eyixs4mlrgpgctnclxx6ekraw\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale service terminate -n deploy-llama-3-70b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fba49",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Enable LLM monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "* Token throughput (tokens/sec).\n",
    "* Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT).\n",
    "* KV cache utilization.\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`. Ensure vLLM V1 is active with `VLLM_USE_V1: \"1\"`. \n",
    "**Note:** `VLLM_USE_V1: \"1\"` is the default value with `ray >= 2.48.0` and can be omitted.\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        runtime_env:\n",
    "          env_vars:\n",
    "            VLLM_USE_V1: \"1\"\n",
    "        ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improve concurrency\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/latest/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.  \n",
    "\n",
    "Example log for 8xL40S:\n",
    "```console\n",
    "INFO 08-19 20:57:37 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 17.79x\n",
    "```\n",
    "\n",
    "The following are a few ways to improve concurrency depending on your model and hardware:  \n",
    "\n",
    "**Reduce `max_model_len`**  \n",
    "Lowering `max_model_len` reduces the memory needed for KV cache.\n",
    "\n",
    "**Example:** Running Llama-3.1-70&nbsp;B on 8xL40S:\n",
    "* `max_model_len = 32,768` â†’ concurrency â‰ˆ 18\n",
    "* `max_model_len = 16,384` â†’ concurrency â‰ˆ 36\n",
    "\n",
    "**Use Quantized models**  \n",
    "Quantizing your model (for example, to FP8) reduces the model's memory footprint, freeing up memory for more KV cache and enabling more concurrent requests.\n",
    "\n",
    "**Use pipeline parallelism**  \n",
    "If a single node isn't enough to handle your workload, consider distributing the model's layers across multiple nodes with `pipeline_parallel_size > 1`.\n",
    "\n",
    "**Upgrade to GPUs with more memory**  \n",
    "Some GPUs provide significantly more room for KV cache and allow for higher concurrency out of the box.\n",
    "\n",
    "**Scale with more replicas**  \n",
    "In addition to tuning per-replica concurrency, you can scale *horizontally* by increasing the number of replicas in your config.  \n",
    "Raising the replica count increases the total number of concurrent requests your service can handle, especially under sustained or bursty traffic.\n",
    "```yaml\n",
    "deployment_config:\n",
    "  autoscaling_config:\n",
    "    min_replicas: 1\n",
    "    max_replicas: 4\n",
    "```\n",
    "\n",
    "*For more details on tuning strategies, hardware guidance, and serving configurations, see [Choose a GPU for LLM serving](https://docs.anyscale.com/llm/serving/gpu-guidance) and [Tune parameters for LLMs on Anyscale services](https://docs.anyscale.com/llm/serving/parameter-tuning).*\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Hugging Face auth errors**  \n",
    "Some models, such as Llama-3.1, are gated and require prior authorization from the organization. See your modelâ€™s documentation for instructions on obtaining access.\n",
    "\n",
    "**Out-of-memory errors**  \n",
    "Out-of-memory (OOM) errors are one of the most common failure modes when deploying LLMs, especially as model sizes and context length increase.  \n",
    "See this [Troubleshooting Guide](https://docs.anyscale.com/overview) for common errors and how to fix them.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a medium-sized LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray cluster, and send requests. You also learned how to monitor your app and troubleshoot common issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_ray_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
