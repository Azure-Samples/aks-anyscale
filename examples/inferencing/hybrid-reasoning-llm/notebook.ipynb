{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e926219a",
   "metadata": {},
   "source": [
    "# Deploy a hybrid reasoning LLM\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fhybrid-reasoning-llm\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/hybrid-reasoning-llm\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "A hybrid reasoning model provides flexibility by allowing you to enable or disable reasoning as needed. You can use structured, step-by-step thinking for complex queries while skipping it for simpler ones, balancing accuracy with efficiency depending on the task.\n",
    "\n",
    "This tutorial deploys a hybrid reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Distinction with purely reasoning models\n",
    "\n",
    "*Hybrid reasoning models* are reasoning-capable models that allow you to toggle the thinking process on and off. You can enable structured, step-by-step reasoning when needed but skip it for simpler queries to reduce latency. Purely reasoning models always apply their reasoning behavior, while hybrid models give you fine-grained control over when to use reasoning.\n",
    "<!-- vale Google.Acronyms = NO -->\n",
    "| **Mode**         | **Core behavior**                            | **Use case examples**                                               | **Limitation**                                    |\n",
    "| ---------------- | -------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Thinking ON**  | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used.      |\n",
    "| **Thinking OFF** | Direct answer generation                   | Casual queries, short instructions, single-step answers              | May struggle with complex reasoning or interpretability. |\n",
    "<!-- vale Google.Acronyms = YES -->\n",
    "**Note:** Reasoning often benefits from long context windows (32K up to +1M tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "To see an example of deploying a purely reasoning model like *QwQ-32&nbsp;B*, see [Deploy a reasoning LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/reasoning-llm/README.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Enable or disable thinking\n",
    "\n",
    "Some hybrid reasoning models let you toggle their \"thinking\" mode on or off. This section explains when to use thinking mode versus skipping it, and shows how to control the setting in practice.\n",
    "\n",
    "---\n",
    "<!-- vale Vale.Terms = NO -->\n",
    "### When to enable or disable thinking mode\n",
    "<!-- vale Vale.Terms = YES -->\n",
    "**Enable thinking mode for:**\n",
    "- Complex, multi-step tasks that require reasoning, such as math, physics, or logic problems.\n",
    "- Ambiguous queries or situations with incomplete information.\n",
    "- Planning, workflow orchestration, or when the model needs to act as an \"agent\" coordinating other tools or models.\n",
    "- Analyzing intricate data, images, or charts.\n",
    "- In-depth code reviews or evaluating outputs from other AI systems (LLM as Judge approach).\n",
    "\n",
    "**Disable thinking mode for:**\n",
    "- Simple, well-defined, or routine tasks.\n",
    "- Low latency and fast responses as the priority.\n",
    "- Repetitive, straightforward steps within a larger automated workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### How to enable or disable thinking mode\n",
    "\n",
    "Toggle thinking mode varies by model and framework. Consult the documentation for the model to see how it structures and controls thinking.\n",
    "\n",
    "For example, to [control reasoning in Qwen-3](https://huggingface.co/Qwen/Qwen3-32B#switching-between-thinking-and-non-thinking-mode), you can:\n",
    "* Add `\"/think\"` or `\"/no_think\"` in the prompt.\n",
    "* Set `enable_thinking` in the request:\n",
    "  `extra_body={\"chat_template_kwargs\": {\"enable_thinking\": ...}}`.\n",
    "\n",
    "See [Send request with thinking enabled](#send-request-with-thinking-enabled) or [Send request with thinking disabled](#send-request-with-thinking-disabled) for practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Parse reasoning outputs\n",
    "\n",
    "In thinking mode, hybrid models often separate _reasoning_ from the _final answer_ using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.  \n",
    "\n",
    "To ensure that Ray Serve LLM correctly parses the reasoning output, configure a `reasoning_parser` in your Ray Serve LLM deployment. This tells vLLM how to isolate the modelâ€™s thought process from the rest of the output.  \n",
    "**Note:** For example, *Qwen-3* uses the `qwen3` parser. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwen-3-32b\n",
    "          model_source: Qwen/Qwen3-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: qwen3 # <-- for Qwen-3 models\n",
    "```\n",
    "\n",
    "See [Configure Ray Serve LLM](#configure-ray-serve-llm) for a complete example.\n",
    "\n",
    "**Example response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning like this\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Set your Hugging Face token in the config file to access gated models.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Set `tensor_parallel_size` to distribute the model's weights among 8 GPUs in the node.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1daf892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2026-02-06 12:23:18,163 serve 601243 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 12:23:18,164 serve 601243 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-qwen-3-32b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/hybrid-reasoning-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_2d34fa4bd2de3407c9d9c2ba3068a90164cbcc4f.zip'}}}\n",
      "INFO 2026-02-06 12:23:18,189 serve 601243 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 12:23:18,190 serve 601243 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n"
     ]
    }
   ],
   "source": [
    "# serve_qwen_3_32b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwen-3-32b\",\n",
    "        model_source=\"Qwen/Qwen3-32B\",\n",
    "    ),\n",
    "    experimental_configs=dict(num_ingress_replicas=1),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            # Increase number of replicas for higher throughput/concurrency.\n",
    "            min_replicas=1,\n",
    "            max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    ### Uncomment if your model is gated and needs your Hugging Face token to access it.\n",
    "    # runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(\n",
    "        # 4 GPUs is enough but you can increase tensor_parallel_size to fit larger models.\n",
    "        tensor_parallel_size=4, max_model_len=32768, reasoning_parser=\"qwen3\"\n",
    "    ),\n",
    ")\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32272280",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate your settings to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. See [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`.\n",
    "\n",
    "**Note:** Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Metaâ€™s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_qwen_3_32b.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8f1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 12:23:23,140\tINFO scripts.py:507 -- Running import path: 'serve_qwen_3_32b:app'.\n",
      "INFO 02-06 12:23:25 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "2026-02-06 12:23:26,880\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
      "2026-02-06 12:23:26,891\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-06 12:23:26,892\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_2d34fa4bd2de3407c9d9c2ba3068a90164cbcc4f.zip' (0.04MiB) to Ray cluster...\n",
      "2026-02-06 12:23:26,893\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_2d34fa4bd2de3407c9d9c2ba3068a90164cbcc4f.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "INFO 2026-02-06 12:23:26,901 serve 603513 -- ============== Deployment Options ==============\n",
      "INFO 2026-02-06 12:23:26,902 serve 603513 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
      " 'health_check_period_s': 10,\n",
      " 'health_check_timeout_s': 10,\n",
      " 'max_ongoing_requests': 1000000000,\n",
      " 'name': 'LLMServer:my-qwen-3-32b',\n",
      " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1},\n",
      "                             {'GPU': 1}],\n",
      " 'placement_group_strategy': 'STRICT_PACK',\n",
      " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/hybrid-reasoning-llm'},\n",
      "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
      "                                       'working_dir': 'gcs://_ray_pkg_2d34fa4bd2de3407c9d9c2ba3068a90164cbcc4f.zip'}}}\n",
      "INFO 2026-02-06 12:23:26,930 serve 603513 -- ============== Ingress Options ==============\n",
      "INFO 2026-02-06 12:23:26,930 serve 603513 -- {'autoscaling_config': {'initial_replicas': 1,\n",
      "                        'max_replicas': 1,\n",
      "                        'min_replicas': 1,\n",
      "                        'target_ongoing_requests': 1000000000},\n",
      " 'max_ongoing_requests': 1000000000}\n",
      "\u001b[36m(ProxyActor pid=603704)\u001b[0m INFO 2026-02-06 12:23:29,195 proxy 10.128.5.219 -- Proxy starting on node 1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0 (HTTP port: 8000).\n",
      "INFO 2026-02-06 12:23:29,235 serve 603513 -- Started Serve in namespace \"serve\".\n",
      "INFO 2026-02-06 12:23:29,261 serve 603513 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001b[36m(ProxyActor pid=603704)\u001b[0m INFO 2026-02-06 12:23:29,232 proxy 10.128.5.219 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,323 controller 603633 -- Deploying new version of Deployment(name='LLMServer:my-qwen-3-32b', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,324 controller 603633 -- Deploying new version of Deployment(name='OpenAiIngress', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=603704)\u001b[0m INFO 2026-02-06 12:23:29,327 proxy 10.128.5.219 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=603704)\u001b[0m WARNING 2026-02-06 12:23:29,331 proxy 10.128.5.219 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=603704)\u001b[0m INFO 2026-02-06 12:23:29,334 proxy 10.128.5.219 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7fd63c47d750>.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,427 controller 603633 -- Adding 1 replica to Deployment(name='LLMServer:my-qwen-3-32b', app='default').\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,428 controller 603633 -- Assigned rank 0 to new replica np0bp0iv during startup\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,429 controller 603633 -- Adding 1 replica to Deployment(name='OpenAiIngress', app='default').\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m INFO 2026-02-06 12:23:29,429 controller 603633 -- Assigned rank 0 to new replica to061k53 during startup\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:35 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:36,908 default_LLMServer:my-qwen-3-32b np0bp0iv -- Downloading the tokenizer for Qwen/Qwen3-32B\n",
      "\u001b[36m(ProxyActor pid=26850, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:37,157 proxy 10.128.8.94 -- Proxy starting on node 05ec9d1e0c7dd8a2528884609513faad12d64730f3b39a8a793702d6 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=26850, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:37,229 proxy 10.128.8.94 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=26850, ip=10.128.8.94)\u001b[0m WARNING 2026-02-06 12:23:37,235 proxy 10.128.8.94 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
      "\u001b[36m(ProxyActor pid=26850, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:37,250 proxy 10.128.8.94 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7ef391f30510>.\n",
      "\u001b[36m(pid=27390, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:43 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_get_vllm_engine_config pid=27390, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:52 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "\u001b[36m(_get_vllm_engine_config pid=27390, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:52 [__init__.py:1815] Using max model len 32768\n",
      "\u001b[36m(_get_vllm_engine_config pid=27390, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:52 [arg_utils.py:1208] Using ray runtime env: {'ray_debugger': {'working_dir': '/home/ray/default/hybrid-reasoning-llm'}, 'working_dir': 'gcs://_ray_pkg_2d34fa4bd2de3407c9d9c2ba3068a90164cbcc4f.zip', 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n",
      "\u001b[36m(_get_vllm_engine_config pid=27390, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m You are using a model of type qwen3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m You are using a model of type qwen3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:53,324 default_LLMServer:my-qwen-3-32b np0bp0iv -- Clearing the current platform cache ...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:23:53,326 default_LLMServer:my-qwen-3-32b np0bp0iv -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:23:54 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:23:58 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m WARNING 2026-02-06 12:23:59,496 controller 603633 -- Deployment 'LLMServer:my-qwen-3-32b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m WARNING 2026-02-06 12:23:59,497 controller 603633 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m 2026-02-06 12:24:00,044\tINFO worker.py:1692 -- Using address ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379 set in the environment variable RAY_ADDRESS\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m 2026-02-06 12:24:00,051\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:00 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:00 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend='qwen3'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=my-qwen-3-32b, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m 2026-02-06 12:24:00,094\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:00 [ray_utils.py:324] Using the existing placement group\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:00 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
      "\u001b[36m(pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:06 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:06 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:06 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:06 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_USE_RAY_COMPILED_DAG', 'LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_RAY_SPMD_WORKER']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:06 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(pid=27559)\u001b[0m INFO 02-06 12:24:06 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m [W206 12:24:11.257109358 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m [W206 12:24:11.267148460 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2d1cf572'), local_subscribe_addr='ipc:///tmp/ac706062-a27c-46c2-aa99-895c8dd2a37f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:24:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:13 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:04,  3.28it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m [W206 12:24:11.257109358 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m [W206 12:24:11.267148460 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m [W206 12:24:11.249881097 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m [W206 12:24:11.267023938 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m \n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:05<00:00,  2.50it/s]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27561, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:19 [default_loader.py:268] Loading weights took 6.48 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:11 [__init__.py:1433] Found nccl from library libnccl.so.2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:11 [pynccl.py:70] vLLM is using nccl==2.27.3\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:24:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-32B...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2370] Loading model from scratch...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27558, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:13 [weight_utils.py:348] Using model weights format ['*.safetensors']\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=27561, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:20 [gpu_model_runner.py:2392] Model loading took 15.3920 GiB and 7.270829 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m \n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m WARNING 2026-02-06 12:24:29,541 controller 603633 -- Deployment 'LLMServer:my-qwen-3-32b' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m WARNING 2026-02-06 12:24:29,541 controller 603633 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=603633)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:06<00:00,  2.61it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m36m(RayWorkerWrapper pid=27559)\u001b[0m \n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:34 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/eec549171a/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:34 [backends.py:550] Dynamo bytecode transform time: 12.62 s\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:20 [default_loader.py:268] Loading weights took 6.73 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:21 [gpu_model_runner.py:2392] Model loading took 15.3920 GiB and 8.198438 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.679 s\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:35 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/eec549171a/rank_0_0/backbone for vLLM's torch.compile\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:35 [backends.py:550] Dynamo bytecode transform time: 13.48 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:47 [monitor.py:34] torch.compile takes 12.62 s in total\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.812 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:49 [gpu_worker.py:298] Available KV cache memory: 53.43 GiB\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(pid=27558)\u001b[0m INFO 02-06 12:24:06 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2d1cf572'), local_subscribe_addr='ipc:///tmp/ac706062-a27c-46c2-aa99-895c8dd2a37f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m WARNING 02-06 12:24:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:13 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27561)\u001b[0m INFO 02-06 12:24:19 [default_loader.py:268] Loading weights took 6.48 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:11 [__init__.py:1433] Found nccl from library libnccl.so.2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:11 [pynccl.py:70] vLLM is using nccl==2.27.3\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:12 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:12 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m WARNING 02-06 12:24:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-32B...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:12 [gpu_model_runner.py:2370] Loading model from scratch...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27558)\u001b[0m INFO 02-06 12:24:13 [weight_utils.py:348] Using model weights format ['*.safetensors']\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27561)\u001b[0m INFO 02-06 12:24:20 [gpu_model_runner.py:2392] Model loading took 15.3920 GiB and 7.270829 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:34 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/eec549171a/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:34 [backends.py:550] Dynamo bytecode transform time: 12.62 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:20 [default_loader.py:268] Loading weights took 6.73 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:21 [gpu_model_runner.py:2392] Model loading took 15.3920 GiB and 8.198438 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:35 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/eec549171a/rank_0_0/backbone for vLLM's torch.compile\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:35 [backends.py:550] Dynamo bytecode transform time: 13.48 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.812 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:864] GPU KV cache size: 875,360 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 26.71x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:864] GPU KV cache size: 875,360 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 26.71x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:864] GPU KV cache size: 875,360 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 26.71x\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:864] GPU KV cache size: 875,360 tokens\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:49 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 26.71x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|â–         | 1/67 [00:00<00:06,  9.95it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]eCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:05<00:00, 11.17it/s]\u001b[32m [repeated 58x across cluster]\u001b[0m\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:05<00:00, 11.25it/s]\n",
      "\u001b[36m(RayWorkerWrapper pid=27560, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:56 [custom_all_reduce.py:203] Registering 8643 cuda graph addresses\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:47 [monitor.py:34] torch.compile takes 12.62 s in total\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.679 s\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:49 [gpu_worker.py:298] Available KV cache memory: 53.43 GiB\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:05<00:00, 11.29it/s]\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:56 [gpu_model_runner.py:3118] Graph capturing finished in 7 secs, took 0.93 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=27559, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:56 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 15.39 GiB for weight, 1.41 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.93 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=56212573593` to fit into requested memory, or `--kv-cache-memory=64196850688` to fully utilize gpu memory. Current kv cache memory in use is 57368104345 bytes.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:47 [monitor.py:34] torch.compile takes 13.48 s in total\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27561)\u001b[0m INFO 02-06 12:24:49 [gpu_worker.py:298] Available KV cache memory: 53.43 GiB\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m INFO 02-06 12:24:56 [core.py:218] init engine (profile, create kv cache, warmup model) took 35.55 seconds\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 54710\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:24:58 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:24:58,369 default_LLMServer:my-qwen-3-32b np0bp0iv -- Started vLLM engine.\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 02-06 12:24:58 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:24:58,457 default_LLMServer:my-qwen-3-32b np0bp0iv 6550f07a-8015-42a4-a534-005a18028b4a -- CALL llm_config OK 1.8ms\n",
      "INFO 2026-02-06 12:24:59,512 serve 603513 -- Application 'default' is ready at http://0.0.0.0:8000/.\n",
      "\u001b[0m\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27560)\u001b[0m INFO 02-06 12:24:56 [custom_all_reduce.py:203] Registering 8643 cuda graph addresses\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:56 [gpu_model_runner.py:3118] Graph capturing finished in 7 secs, took 0.93 GiB\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-3-32b pid=26453, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m INFO 02-06 12:24:56 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 15.39 GiB for weight, 1.41 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.93 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=56212573593` to fit into requested memory, or `--kv-cache-memory=64196850688` to fully utilize gpu memory. Current kv cache memory in use is 57368104345 bytes.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:05<00:00, 11.22it/s]\u001b[32m [repeated 3x across cluster]\u001b[0mer pid=27559)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:05<00:00, 11.21it/s]\u001b[32m [repeated 3x across cluster]\u001b[0mer pid=27559)\u001b[0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:05<00:00, 11.29it/s] pid=27487)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=27559)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "!serve run serve_qwen_3_32b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24501f5",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`\n",
    "\n",
    "Use the `model_id` defined in your config (here, `my-qwen-3-32b`) to query your model. Below are some examples on how to send a request to a Qwen-3 deployment with thinking enabled or disabled. \n",
    "\n",
    "---\n",
    "\n",
    "### Send request with thinking disabled\n",
    "\n",
    "You can disable thinking in Qwen-3 by either adding a `/no_think` tag in the prompt or by forwarding `enable_thinking: False` to the vLLM inference engine.  \n",
    "\n",
    "Example curl with `/no_think`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77d2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-6397590a-811d-4582-bfde-440b239e42b3\",\"object\":\"chat.completion\",\"created\":1770409555,\"model\":\"my-qwen-3-32b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\\n\\nTo determine which number is greater between **7.8** and **7.11**, follow this comparison:\\n\\n- **7.8** is the same as **7.80**.\\n- **7.11** is already in two decimal places.\\n\\nNow compare:\\n\\n- **7.80** vs. **7.11**\\n\\nSince **80 > 11**, **7.80 > 7.11**\\n\\nâœ… **Answer: 7.8 is greater than 7.11**.\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":\"\\n\\n\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":27,\"total_tokens\":143,\"completion_tokens\":116,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{ \"model\": \"my-qwen-3-32b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /no_think\"}] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127ea5f",
   "metadata": {},
   "source": [
    "Example Python with `enable_thinking: False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51e9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-06 12:26:00,007\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 601243, \"job_id\": \"12000000\", \"worker_id\": \"12000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770409560007933807}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: \n",
      "None\n",
      "\n",
      "\n",
      "Answer: \n",
      " The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "#client_thinking_disabled.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"FAKE_KEY\"\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765b3f8",
   "metadata": {},
   "source": [
    "Notice the `reasoning_content` is empty here. \n",
    "**Note:** Depending on your model's documentation, empty could mean `None`, an empty string or even empty tags `\"<think></think>\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Send request with thinking enabled\n",
    " \n",
    "You can enable thinking in Qwen-3 by either adding a `/think` tag in the prompt or by forwarding `enable_thinking: True` to the vLLM inference engine.  \n",
    "\n",
    "Example curl with `/think`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8702258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-17ae842c-0bc5-43d7-9ac1-ff5e43450095\",\"object\":\"chat.completion\",\"created\":1770409563,\"model\":\"my-qwen-3-32b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\\n\\nWhen comparing the numbers **7.8** and **7.11**, it's crucial to understand how place value works in decimal numbers. Here's a clear breakdown:\\n\\n---\\n\\n### Step-by-Step Comparison\\n\\n1. **Align the Decimal Places**  \\n   To make the comparison easier, we convert both numbers to have the same number of decimal places:\\n   - **7.8** becomes **7.80** (adding a zero at the end does not change the value).\\n   - **7.11** remains **7.11**.\\n\\n2. **Break Down the Place Values**  \\n   Now we compare:\\n   - **7.80** = 7 (ones) + 8 (tenths) + 0 (hundredths)\\n   - **7.11** = 7 (ones) + 1 (tenths) + 1 (hundredths)\\n\\n3. **Compare from Left to Right**  \\n   - The **ones** place is the same in both numbers: **7**.\\n   - The **tenths** place is where the difference occurs: **8** (in 7.80) vs. **1** (in 7.11).\\n     - Since **8 > 1**, **7.80 is greater than 7.11**.\\n\\n---\\n\\n### Alternative Methods\\n\\n- **Fraction Conversion**  \\n  - **7.8** = 7 + 8/10 = 7 + 4/5 = 39/5 = **780/100**\\n  - **7.11** = 7 + 11/100 = **711/100**\\n  - Comparing **780/100** and **711/100**, it's clear **780 > 711**, so **7.8 > 7.11**.\\n\\n- **Money Analogy**  \\n  - **7.80** is like **$7.80**, and **7.11** is like **$7.11**. Clearly, **$7.80 is more** than **$7.11**.\\n\\n- **Subtraction**  \\n  - **7.80 - 7.11 = 0.69**, a positive number. This confirms that **7.80 > 7.11**.\\n\\n---\\n\\n### Key Insight\\n\\nThe **tenths place** is more significant than the **hundredths place**. Even though **11 > 8**, the **8 in the tenths place** (which is equivalent to 80 hundredths) is much greater than **11 hundredths**.\\n\\n---\\n\\n### Final Answer\\n\\n**7.8 is greater than 7.11.**\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":\"\\nOkay, so I need to figure out which number is greater between 7.8 and 7.11. Hmm, let me start by recalling how decimal numbers work. I know that when comparing two decimal numbers, you start by looking at the digits from left to right, comparing each place value one by one. The first place where they differ will tell you which number is larger.\\n\\nFirst, let me write down both numbers so I can see them clearly:\\n\\n7.8 and 7.11\\n\\nWait, but 7.8 is written as 7.8 and 7.11 is written as 7.11. I notice that 7.8 has one digit after the decimal point, while 7.11 has two digits. Maybe I should convert them to have the same number of decimal places to make the comparison easier. How do I do that?\\n\\nWell, 7.8 is the same as 7.80 because adding a zero at the end of a decimal doesn't change its value. Similarly, 7.11 is already 7.11. So now, the numbers are 7.80 and 7.11. That might help me compare them more easily.\\n\\nNow, let's break down the numbers into their place values. For 7.80:\\n\\n- The whole number part is 7.\\n- The tenths place is 8.\\n- The hundredths place is 0.\\n\\nFor 7.11:\\n\\n- The whole number part is 7.\\n- The tenths place is 1.\\n- The hundredths place is 1.\\n\\nComparing the whole number parts first: both are 7, so they are equal up to that point. So, we move to the next place value, which is the tenths place.\\n\\nIn the tenths place, 7.80 has 8 and 7.11 has 1. Since 8 is greater than 1, that should mean that 7.80 is greater than 7.11, right? Wait, but hold on a second. Is that correct?\\n\\nLet me verify. If I think of 7.8 as 7 and 8 tenths, and 7.11 as 7 and 11 hundredths. Wait, tenths and hundredths... Maybe I need to convert them to the same units to compare properly.\\n\\nAlternatively, I can think of 7.8 as 7.80, which is 780 hundredths. And 7.11 is 711 hundredths. So, 780 hundredths vs. 711 hundredths. Since 780 is more than 711, 7.80 is greater than 7.11. So, that would mean 7.8 is greater than 7.11.\\n\\nBut wait, another way to think about this is that 7.11 is like 7 and 11 hundredths, whereas 7.8 is 7 and 8 tenths. But 8 tenths is equal to 80 hundredths. So, 80 hundredths vs. 11 hundredths. So again, 80 hundredths is more than 11 hundredths. Therefore, 7.8 is greater.\\n\\nBut sometimes, people might get confused because 11 is a bigger number than 8. But in the decimal system, the place value matters. The tenths place is bigger than the hundredths place. So even though 11 is numerically larger than 8, the 8 is in the tenths place, which is a higher place value. Therefore, 8 tenths (0.8) is more than 11 hundredths (0.11). \\n\\nWait, let me check with fractions. 0.8 is equal to 8/10, which simplifies to 4/5 or 0.8. 0.11 is 11/100, which is 0.11. If I convert both to fractions over 100, 0.8 is 80/100, and 0.11 is 11/100. Comparing 80/100 and 11/100, clearly 80 is larger. Therefore, 0.8 is larger than 0.11, so 7.8 is larger than 7.11.\\n\\nAnother way to visualize this is by using a number line. If I imagine a number line from 7.0 to 8.0, 7.8 would be closer to 8.0, while 7.11 would be just a little past 7.1. So, 7.8 is much further to the right on the number line, indicating it's a larger number.\\n\\nBut maybe I should also consider if there's a common mistake here. Sometimes people might see 7.11 and think it's 7 and 11 tenths, but that's not the case. The decimal system is based on powers of ten, so each position after the decimal is a power of ten. The first digit after the decimal is tenths (10^-1), the second is hundredths (10^-2), and so on. So, in 7.11, the first 1 is in the tenths place and the second 1 is in the hundredths place. Therefore, 7.11 is 7 + 1/10 + 1/100 = 7 + 0.1 + 0.01 = 7.11. Whereas 7.8 is 7 + 8/10 = 7 + 0.8 = 7.8. So, 7.8 is indeed larger.\\n\\nAlternatively, if I convert both numbers to fractions:\\n\\n7.8 can be written as 7 + 8/10 = 7 + 4/5 = 39/5.\\n\\n7.11 can be written as 7 + 11/100 = 7 + 11/100 = 711/100.\\n\\nNow, comparing 39/5 and 711/100. To compare these fractions, I can convert them to have the same denominator. The least common denominator of 5 and 100 is 100. So, converting 39/5 to a fraction with denominator 100:\\n\\nMultiply numerator and denominator by 20: 39*20 = 780, 5*20 = 100. So, 39/5 = 780/100.\\n\\nNow, comparing 780/100 and 711/100. Since 780 > 711, 780/100 > 711/100. Therefore, 7.8 > 7.11.\\n\\nAnother approach is to subtract the two numbers to see which is larger. If I subtract 7.11 from 7.8, what do I get?\\n\\nFirst, write 7.8 as 7.80. Then subtract 7.11 from 7.80:\\n\\n7.80\\n-7.11\\n------\\n0.69\\n\\nThe result is 0.69, which is a positive number. Since the result is positive, that means 7.80 is greater than 7.11 by 0.69.\\n\\nAlternatively, if I subtract 7.80 from 7.11, I would get a negative number, which also confirms that 7.11 is less than 7.80.\\n\\nI can also think in terms of money. If I have $7.80 versus $7.11, clearly $7.80 is more money. 7 dollars and 80 cents is more than 7 dollars and 11 cents. That analogy helps solidify the concept.\\n\\nBut wait, maybe someone might still be confused because 11 is more than 8. But in this case, the 8 is in the tenths place, which is worth more than the hundredths place. Each tenths place is ten times larger than the hundredths place. So, 8 tenths is 80 hundredths, which is way more than 11 hundredths. So, even though 11 is a bigger number than 8, the place value of the 8 makes it larger.\\n\\nLet me also consider another example to test my understanding. For instance, which is larger: 2.3 or 2.15? Following the same logic, 2.3 is 2.30, which is 230 hundredths. 2.15 is 215 hundredths. So, 230 is more than 215, so 2.3 is larger. Similarly, 0.5 is 0.50, which is 50 hundredths, and 0.49 is 49 hundredths. So, 0.5 is larger. This pattern seems consistent.\\n\\nAnother example: 5.09 vs. 5.1. Here, 5.1 is 5.10, which is 510 hundredths. 5.09 is 509 hundredths. So, 5.1 is larger. So, in this case, even though 09 is less than 10, the tenths place in 5.1 is 1 versus 0 in 5.09. So, the tenths place determines it here.\\n\\nTherefore, going back to the original problem, 7.8 vs. 7.11. The tenths place is 8 versus 1, so 8 is larger. Hence, 7.8 is larger.\\n\\nI think I've covered multiple ways to look at this problem: converting to the same decimal places, converting to fractions, using money analogy, and even subtracting one from the other. All methods consistently show that 7.8 is greater than 7.11. The key takeaway is understanding the place value of decimal digits and not just the face value of the numbers after the decimal point.\\n\\nOne thing to be cautious about is when the numbers after the decimal are in different places. For example, if you have 7.8 and 7.11, the 8 is in the tenths place, and the 11 is in the hundredths place. Since tenths are larger units than hundredths, the number with the larger digit in the tenths place will be bigger, regardless of what's in the hundredths or subsequent places. So even if the hundredths place had a larger number, if the tenths place is smaller, the overall number is still smaller.\\n\\nSo, in conclusion, after considering various methods and verifying each step, I'm confident that 7.8 is greater than 7.11.\\n\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":25,\"total_tokens\":2983,\"completion_tokens\":2958,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{ \"model\": \"my-qwen-3-32b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /think\"}] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bad31b",
   "metadata": {},
   "source": [
    " Example Python with `enable_thinking: True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a52eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-06 12:26:58,966\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 601243, \"job_id\": \"12000000\", \"worker_id\": \"12000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770409618966867734}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: \n",
      "\n",
      "Okay, so the user is asking for the capital of France. Let me think. I remember that France is a country in Europe, and I think the capital is Paris. But wait, let me make sure. Sometimes people might confuse it with Lyon or Marseille, but those are other major cities. Paris is definitely the capital. I can recall that the Eiffel Tower is in Paris, and it's a major city known for art and fashion. Also, the French government is based there. Yeah, I'm pretty confident it's Paris. Let me double-check in my mind. No, I don't think I'm mixing it up with another country. France's capital is indeed Paris. So the answer should be Paris.\n",
      "\n",
      "\n",
      "\n",
      "Answer: \n",
      " \n",
      "\n",
      "The capital of France is **Paris**. It is a major global city known for its cultural landmarks, such as the Eiffel Tower and the Louvre Museum, and serves as the political, economic, and administrative center of the country. \n",
      "\n",
      "**Answer:** Paris.\n"
     ]
    }
   ],
   "source": [
    "#client_thinking_enabled.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"FAKE_KEY\"\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36ba3d",
   "metadata": {},
   "source": [
    "If you configure a valid reasoning parser, the reasoning output should appear in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, typically wrapped in `<think>...</think>` tags. See [Parse reasoning outputs](#parse-reasoning-outputs) for more information.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown \n",
    "\n",
    "Shutdown your LLM service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc5cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 12:27:43,227\tSUCC scripts.py:774 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009515b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Deploy to production with Anyscale services\n",
    "\n",
    "For production, it's recommended to use Anyscale services to deploy your Ray Serve app on a dedicated cluster without any code changes. Anyscale provides scalability, fault tolerance, and load balancing, ensuring resilience against node failures, high traffic, and rolling updates. See [Deploy a medium-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html#deploy-to-production-with-anyscale-services) for an example with a medium-sized model like the *Qwen-32b* from this tutorial.\n",
    "\n",
    "---\n",
    "\n",
    "## Stream reasoning content\n",
    "\n",
    "In thinking mode, hybrid reasoning models may take longer to begin generating the main content. You can stream intermediate reasoning output in the same way as the main content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5f5a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2026-02-06 12:27:01,765\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 601243, \"job_id\": \"12000000\", \"worker_id\": \"12000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770409621765202428}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user wants to plan a trip to Paris from Seattle. Let me break down what they need. First, flight costs. They might be looking for the best times to fly or budget-friendly options. I should check current prices and maybe suggest flexible dates. Then, a 3-day itinerary. They might want to see the major attractions but also have some downtime. I need to balance popular spots with less crowded areas. Also, vegetarian restaurants. I should make sure the suggestions are up-to-date and consider different areas of Paris for each day. Let me start with flights.\n",
      "\n",
      "For flights, I remember that prices can vary a lot. Maybe suggest using Google Flights or Skyscanner to track prices. Also, mention that flying mid-week might be cheaper. The average cost from Seattle to Paris is around $1,000-$1,500, but it depends on the time of year. If they're flexible, they might find cheaper options. Maybe highlight some airlines that offer good deals, like Air France or low-cost carriers if available.\n",
      "\n",
      "Next, the itinerary. Three days is short, so focus on key areas. Day 1: Arrival and the Left Bank. Maybe start with a walk around the Latin Quarter, visit the Pantheon, and have lunch at a vegetarian spot there. Then the Seine River, Notre-Dame, and a sunset at Eiffel Tower. Day 2: Right Bank. Montmartre, SacrÃ©-CÅ“ur, then the Champs-Ã‰lysÃ©es and Arc de Triomphe. Evening at a cabaret like Moulin Rouge. Day 3: Museums and shopping. Louvre, Tuileries Garden, Champs-Ã‰lysÃ©es again for shopping, and a farewell dinner. Need to make sure the timing is realistic, not too rushed.\n",
      "\n",
      "For vegetarian restaurants, I need to list places in each area. On the Left Bank, maybe Le Foyer de la Madeleine or Le Potager du Marais. On the Right Bank, Le Jardin des Plantes or Le CafÃ© de la Madeleine. For the third day, maybe a place near the Louvre. Also, mention markets like MarchÃ© d'Aligre for fresh options. Need to check if these places are still open and have good reviews for vegetarian options.\n",
      "\n",
      "Dietary restrictions: Vegetarian, so no meat or fish. Some places might be vegan, which is a plus. Should also note if any restaurants require reservations, especially in Paris where it's common. Maybe suggest booking in advance.\n",
      "\n",
      "Other considerations: Transportation between places. Metro is efficient, so recommend a metro pass. Also, time of year â€“ if they're going in winter, the Eiffel Tower might have different hours. Maybe add tips like getting a Paris Pass for attractions, or using apps like Citymapper for navigation.\n",
      "\n",
      "Wait, the user might not have mentioned budget constraints, but flight costs are part of the query. Maybe include a rough estimate for each part. Also, check if there are any current events in Paris that might affect the itinerary, but since it's 3 days, maybe stick to the classics.\n",
      "\n",
      "Need to make sure the itinerary isn't too packed. Each day should have a good mix of walking and resting. Maybe suggest a morning, afternoon, and evening plan each day. Also, include approximate times for each activity to avoid overlap.\n",
      "\n",
      "For restaurants, maybe include a variety of cuisines. French vegetarian, maybe some international options like Indian or Middle Eastern that are vegetarian-friendly. Also, mention that many Parisian restaurants have vegetarian options on their regular menus, so it's not just limited to specific places.\n",
      "\n",
      "Double-check the addresses and opening hours of the suggested restaurants. Some might be closed on certain days. Also, consider if the user has any other dietary needs, but they only mentioned vegetarian. Maybe add a note about common allergens or vegan options if available.\n",
      "\n",
      "Overall, structure the answer with sections: Flights, Itinerary, Restaurants, and Additional Tips. Keep it clear and organized so the user can follow easily. Make sure to use bullet points or numbered lists for readability. Also, maybe add a note about language â€“ some restaurants might have English menus, but it's good to know basic French phrases.\n",
      "\n",
      "\n",
      "Here's a comprehensive plan for your 3-day trip to Paris from Seattle, including flight insights, an itinerary, and vegetarian restaurant suggestions:\n",
      "\n",
      "---\n",
      "\n",
      "### **Flight Cost Research**  \n",
      "- **Average Round-Trip Cost (Seattle to Paris):**  \n",
      "  - **$1,000â€“$1,500** for economy flights (prices vary by season; cheaper options may be available mid-week).  \n",
      "  - **Airlines:** Air France, Delta, and low-cost carriers like Norwegian (seasonal) often offer competitive rates.  \n",
      "  - **Tips:** Use [Google Flights](https://www.google.com/flights) or [Skyscanner](https://www.skyscanner.net) to track prices and set alerts for deals.\n",
      "\n",
      "---\n",
      "\n",
      "### **3-Day Paris Itinerary**  \n",
      "**Day 1: Left Bank & Iconic Landmarks**  \n",
      "- **Morning:**  \n",
      "  - Arrive in Paris and grab a vegetarian breakfast at *CafÃ© de l'Escargot* (near the Eiffel Tower).  \n",
      "  - Walk through the **Latin Quarter** and visit the **Pantheon** (free entry; book tickets in advance).  \n",
      "- **Lunch:** *Le Foyer de la Madeleine* (vegetarian French dishes, 3-star rating on Tripadvisor).  \n",
      "- **Afternoon:**  \n",
      "  - Stroll along the **Seine River**, visit **Notre-Dame Cathedral** (exterior only due to renovations), and relax in **Jardin du Luxembourg**.  \n",
      "- **Evening:**  \n",
      "  - Sunset at the **Eiffel Tower** (glow lights at 7 PM). Dinner at *Le Potager du Marais* (vegan/vegetarian-friendly, modern French cuisine).  \n",
      "\n",
      "**Day 2: Right Bank & Cultural Highlights**  \n",
      "- **Morning:**  \n",
      "  - Explore **Montmartre** and **SacrÃ©-CÅ“ur Basilica** (free entry). Walk through the artistic **Place du Tertre**.  \n",
      "  - Lunch at *Le Jardin des Plantes* (near the Natural History Museum; seasonal vegetarian dishes).  \n",
      "- **Afternoon:**  \n",
      "  - Visit the **Louvre Museum** (book tickets online; focus on vegetarian-friendly hours: 8 AMâ€“6 PM).  \n",
      "  - Walk to **Tuileries Garden** and the **Champs-Ã‰lysÃ©es**.  \n",
      "- **Evening:**  \n",
      "  - Dinner at *Le CafÃ© de la Madeleine* (classic French bistro with vegetarian options). Optional: Evening show at the **Moulin Rouge** (book tickets in advance).  \n",
      "\n",
      "**Day 3: Markets & Departure**  \n",
      "- **Morning:**  \n",
      "  - Breakfast at *MarchÃ© d'Aligre* (vegetarian market with fresh pastries and produce).  \n",
      "  - Visit the **Centre Pompidou** (modern art) or **MusÃ©e Rodin** (sculpture garden).  \n",
      "- **Afternoon:**  \n",
      "  - Shopping on **Rue Saint-HonorÃ©** or **Galeries Lafayette**.  \n",
      "  - Final lunch at *Le Comptoir VÃ©gÃ©talien* (vegan/vegetarian, 10-minute walk from Galeries Lafayette).  \n",
      "- **Evening:** Head to the airport with time to spare.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Vegetarian Restaurant Suggestions**  \n",
      "1. **Le Foyer de la Madeleine** (1st Arrondissement)  \n",
      "   - Highlights: *Grilled Portobello Mushrooms*, *Quinoa Salad*.  \n",
      "   - Reservations: Recommended.  \n",
      "2. **Le Potager du Marais** (4th Arrondissement)  \n",
      "   - Highlights: *Vegan Croissants*, *Stuffed Zucchini Blossoms*.  \n",
      "   - Reservations: Required for dinner.  \n",
      "3. **Le Jardin des Plantes** (5th Arrondissement)  \n",
      "   - Highlights: *Vegetarian Fondue*, *Herb-Infused Risotto*.  \n",
      "   - Reservations: Optional.  \n",
      "4. **Le Comptoir VÃ©gÃ©talien** (9th Arrondissement)  \n",
      "   - Highlights: *Vegan Burger*, *Chickpea Curry*.  \n",
      "   - Reservations: Suggested for lunch.  \n",
      "5. **MarchÃ© d'Aligre** (12th Arrondissement)  \n",
      "   - Fresh vegetarian options: Juices, salads, and baked goods.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Additional Tips**  \n",
      "- **Transportation:** Use a **Paris Visite Travel Pass** for metro/bus access (costs ~â‚¬20/day).  \n",
      "- **Language:** Learn basic French phrases (e.g., *â€œSâ€™il vous plaÃ®tâ€* for â€œpleaseâ€). Many restaurants list vegetarian options under *â€œVÃ©gÃ©tarienâ€* on menus.  \n",
      "- **Budget:** Allocate ~â‚¬150â€“â‚¬200/day for meals, entry fees, and transport.  \n",
      "- **Packing:** Comfortable shoes for walking, a reusable water bottle, and a compact umbrella (Paris weather can be unpredictable).  \n",
      "\n",
      "Let me know if youâ€™d like to adjust the itinerary or need help booking! ðŸ—¼âœ¨"
     ]
    }
   ],
   "source": [
    "#client_streaming.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"FAKE_KEY\"\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I need to plan a trip to Paris from Seattle. Can you help me research flight costs, create an itinerary for 3 days, and suggest restaurants based on my dietary restrictions (vegetarian)?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}},\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream \n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6357c06",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a hybrid reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray cluster, send requests, and parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
