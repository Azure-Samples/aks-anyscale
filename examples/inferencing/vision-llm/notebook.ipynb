{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "23243c2e",
            "metadata": {},
            "source": [
                "# Deploy a vision LLM\n",
                "\n",
                "<div align=\"left\">\n",
                "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fvision-llm\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
                "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/vision-llm\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
                "</div>\n",
                "\n",
                "A vision LLM can interpret images as well as text, enabling tasks like answering questions about charts, analyzing photos, or combining visuals with instructions. It extends LLMs beyond language to support multimodal reasoning and richer applications.  \n",
                "\n",
                "This tutorial deploys a vision LLM using Ray Serve LLM.  \n",
                "\n",
                "---\n",
                "\n",
                "## Configure Ray Serve LLM\n",
                "\n",
                "Make sure to set your Hugging Face token in the config file to access gated models.\n",
                "\n",
                "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ebc41d60",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-06 12:38:42 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-06 12:38:43,490\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
                        "2026-02-06 12:38:43,502\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
                        "2026-02-06 12:38:43,503\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip' (0.06MiB) to Ray cluster...\n",
                        "2026-02-06 12:38:43,504\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip'.\n",
                        "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
                        "  warnings.warn(\n",
                        "INFO 2026-02-06 12:38:43,512 serve 609385 -- ============== Deployment Options ==============\n",
                        "INFO 2026-02-06 12:38:43,513 serve 609385 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
                        " 'health_check_period_s': 10,\n",
                        " 'health_check_timeout_s': 10,\n",
                        " 'max_ongoing_requests': 1000000000,\n",
                        " 'name': 'LLMServer:my-qwen-VL',\n",
                        " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
                        " 'placement_group_strategy': 'STRICT_PACK',\n",
                        " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/vision-llm'},\n",
                        "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
                        "                                       'working_dir': 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip'}}}\n",
                        "INFO 2026-02-06 12:38:43,545 serve 609385 -- ============== Ingress Options ==============\n",
                        "INFO 2026-02-06 12:38:43,545 serve 609385 -- {'autoscaling_config': {'initial_replicas': 1,\n",
                        "                        'max_replicas': 1,\n",
                        "                        'min_replicas': 1,\n",
                        "                        'target_ongoing_requests': 1000000000},\n",
                        " 'max_ongoing_requests': 1000000000}\n"
                    ]
                }
            ],
            "source": [
                "# serve_qwen_VL.py\n",
                "from ray.serve.llm import LLMConfig, build_openai_app\n",
                "import os\n",
                "\n",
                "llm_config = LLMConfig(\n",
                "    model_loading_config=dict(\n",
                "        model_id=\"my-qwen-VL\",\n",
                "        model_source=\"qwen/Qwen2.5-VL-7B-Instruct\",\n",
                "    ),\n",
                "    experimental_configs=dict(num_ingress_replicas=1),\n",
                "    deployment_config=dict(\n",
                "        autoscaling_config=dict(\n",
                "            min_replicas=1,\n",
                "            max_replicas=1,\n",
                "        )\n",
                "    ),\n",
                "    ### Uncomment if your model is gated and needs your Hugging Face token to access it.\n",
                "    # runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
                "    engine_kwargs=dict(max_model_len=8192),\n",
                ")\n",
                "\n",
                "app = build_openai_app({\"llm_configs\": [llm_config]})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c76a6362",
            "metadata": {},
            "source": [
                "**Note:** Before moving to a production setup, migrate to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. See [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment) for an example.\n",
                "\n",
                "---\n",
                "\n",
                "## Deploy locally\n",
                "\n",
                "**Prerequisites**\n",
                "\n",
                "* Access to GPU compute.\n",
                "* (Optional) A **Hugging Face token** if using gated models. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
                "\n",
                "**Note:** Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Metaâ€™s Llama models approval can take anywhere from a few hours to several weeks.\n",
                "\n",
                "**Dependencies:**  \n",
                "```bash\n",
                "pip install \"ray[serve,llm]\"\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### Launch\n",
                "\n",
                "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_qwen_VL.py`.  \n",
                "\n",
                "In a terminal, run:   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "7eb8734c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2026-02-06 12:36:14,387\tINFO scripts.py:507 -- Running import path: 'serve_qwen_VL:app'.\n",
                        "INFO 02-06 12:36:16 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
                        "2026-02-06 12:36:18,033\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.5.219:6379...\n",
                        "2026-02-06 12:36:18,043\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
                        "2026-02-06 12:36:18,044\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip' (0.06MiB) to Ray cluster...\n",
                        "2026-02-06 12:36:18,044\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip'.\n",
                        "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
                        "  warnings.warn(\n",
                        "INFO 2026-02-06 12:36:18,053 serve 609405 -- ============== Deployment Options ==============\n",
                        "INFO 2026-02-06 12:36:18,053 serve 609405 -- {'autoscaling_config': {'max_replicas': 1, 'min_replicas': 1},\n",
                        " 'health_check_period_s': 10,\n",
                        " 'health_check_timeout_s': 10,\n",
                        " 'max_ongoing_requests': 1000000000,\n",
                        " 'name': 'LLMServer:my-qwen-VL',\n",
                        " 'placement_group_bundles': [{'CPU': 1, 'GPU': 1}],\n",
                        " 'placement_group_strategy': 'STRICT_PACK',\n",
                        " 'ray_actor_options': {'runtime_env': {'ray_debugger': {'working_dir': '/home/ray/default/vision-llm'},\n",
                        "                                       'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook',\n",
                        "                                       'working_dir': 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip'}}}\n",
                        "INFO 2026-02-06 12:36:18,082 serve 609405 -- ============== Ingress Options ==============\n",
                        "INFO 2026-02-06 12:36:18,082 serve 609405 -- {'autoscaling_config': {'initial_replicas': 1,\n",
                        "                        'max_replicas': 1,\n",
                        "                        'min_replicas': 1,\n",
                        "                        'target_ongoing_requests': 1000000000},\n",
                        " 'max_ongoing_requests': 1000000000}\n",
                        "INFO 2026-02-06 12:36:20,373 serve 609405 -- Started Serve in namespace \"serve\".\n",
                        "INFO 2026-02-06 12:36:20,399 serve 609405 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
                        "\u001b[36m(ProxyActor pid=609597)\u001b[0m INFO 2026-02-06 12:36:20,331 proxy 10.128.5.219 -- Proxy starting on node 1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0 (HTTP port: 8000).\n",
                        "\u001b[36m(ProxyActor pid=609597)\u001b[0m INFO 2026-02-06 12:36:20,370 proxy 10.128.5.219 -- Got updated endpoints: {}.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,466 controller 609527 -- Deploying new version of Deployment(name='LLMServer:my-qwen-VL', app='default') (initial target replicas: 1).\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,467 controller 609527 -- Deploying new version of Deployment(name='OpenAiIngress', app='default') (initial target replicas: 1).\n",
                        "\u001b[36m(ProxyActor pid=609597)\u001b[0m INFO 2026-02-06 12:36:20,469 proxy 10.128.5.219 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
                        "\u001b[36m(ProxyActor pid=609597)\u001b[0m WARNING 2026-02-06 12:36:20,473 proxy 10.128.5.219 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
                        "\u001b[36m(ProxyActor pid=609597)\u001b[0m INFO 2026-02-06 12:36:20,476 proxy 10.128.5.219 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f39b89e72d0>.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,570 controller 609527 -- Adding 1 replica to Deployment(name='LLMServer:my-qwen-VL', app='default').\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,570 controller 609527 -- Assigned rank 0 to new replica 1f7o2sx5 during startup\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,571 controller 609527 -- Adding 1 replica to Deployment(name='OpenAiIngress', app='default').\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m INFO 2026-02-06 12:36:20,572 controller 609527 -- Assigned rank 0 to new replica dkqazclw during startup\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:26 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m No cloud storage mirror configured\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:27,943 default_LLMServer:my-qwen-VL 1f7o2sx5 -- Downloading the tokenizer for qwen/Qwen2.5-VL-7B-Instruct\n",
                        "\u001b[36m(ProxyActor pid=30073, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:28,508 proxy 10.128.8.94 -- Proxy starting on node 05ec9d1e0c7dd8a2528884609513faad12d64730f3b39a8a793702d6 (HTTP port: 8000).\n",
                        "\u001b[36m(ProxyActor pid=30073, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:28,593 proxy 10.128.8.94 -- Got updated endpoints: {Deployment(name='OpenAiIngress', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
                        "\u001b[36m(ProxyActor pid=30073, ip=10.128.8.94)\u001b[0m WARNING 2026-02-06 12:36:28,599 proxy 10.128.8.94 -- ANYSCALE_RAY_SERVE_GRPC_RUN_PROXY_ROUTER_SEPARATE_LOOP has been deprecated and will be removed in the ray v2.50.0. Please use RAY_SERVE_RUN_ROUTER_IN_SEPARATE_LOOP instead.\n",
                        "\u001b[36m(ProxyActor pid=30073, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:28,614 proxy 10.128.8.94 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7f9c483cc510>.\n",
                        "\u001b[36m(pid=30283, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:35 [__init__.py:216] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
                        "\u001b[36m(_get_vllm_engine_config pid=30283, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:45 [__init__.py:742] Resolved architecture: Qwen2_5_VLForConditionalGeneration\n",
                        "\u001b[36m(_get_vllm_engine_config pid=30283, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:45 [__init__.py:1815] Using max model len 8192\n",
                        "\u001b[36m(_get_vllm_engine_config pid=30283, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:45 [arg_utils.py:1208] Using ray runtime env: {'ray_debugger': {'working_dir': '/home/ray/default/vision-llm'}, 'working_dir': 'gcs://_ray_pkg_46981a311274126bb320af99503b26e521e1864a.zip', 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}\n",
                        "\u001b[36m(_get_vllm_engine_config pid=30283, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:46 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m You are using a model of type qwen2_5_vl to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m You are using a model of type qwen2_5_vl to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:46,788 default_LLMServer:my-qwen-VL 1f7o2sx5 -- Clearing the current platform cache ...\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:36:46,791 default_LLMServer:my-qwen-VL 1f7o2sx5 -- Using executor class: <class 'vllm.v1.executor.ray_distributed_executor.RayDistributedExecutor'>\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:36:50 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:36:50,609 controller 609527 -- Deployment 'LLMServer:my-qwen-VL' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:36:50,609 controller 609527 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:36:54 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:36:55 [core.py:654] Waiting for init message from front-end.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:36:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='qwen/Qwen2.5-VL-7B-Instruct', speculative_config=None, tokenizer='qwen/Qwen2.5-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=my-qwen-VL, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m 2026-02-06 12:36:56,006\tINFO worker.py:1692 -- Using address ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379 set in the environment variable RAY_ADDRESS\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m 2026-02-06 12:36:56,013\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: ses-9fyy71sw3bgwajvnjflq7jxd9h-head:6379...\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m 2026-02-06 12:36:56,069\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-9fyy71sw3bgwajvnjflq7jxd9h.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:36:56 [ray_utils.py:324] Using the existing placement group\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:36:56 [ray_distributed_executor.py:171] use_ray_spmd_worker: True\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m   warnings.warn(\n",
                        "\u001b[36m(pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:01 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(pid=30452)\u001b[0m INFO 02-06 12:37:01 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:37:02 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:37:02 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_USE_RAY_COMPILED_DAG', 'LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_RAY_SPMD_WORKER']\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:37:02 [ray_env.py:68] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [W206 12:37:05.875693284 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [W206 12:37:05.883582845 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:05 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:37:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:37:10 [profiling.py:280] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:10 [gpu_model_runner.py:2338] Starting to load model qwen/Qwen2.5-VL-7B-Instruct...\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:10 [gpu_model_runner.py:2370] Loading model from scratch...\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:37:10 [cuda.py:217] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:10 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:11 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:37:20,638 controller 609527 -- Deployment 'LLMServer:my-qwen-VL' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:37:20,639 controller 609527 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [W206 12:37:05.883220736 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [W206 12:37:05.883582845 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:40 [weight_utils.py:369] Time spent downloading weights for qwen/Qwen2.5-VL-7B-Instruct: 29.649602 seconds\n",
                        "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:00,  6.39it/s]\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m \n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:44 [default_loader.py:268] Loading weights took 3.51 seconds\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:45 [gpu_model_runner.py:2392] Model loading took 15.6269 GiB and 33.980825 seconds\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:45 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:37:50,679 controller 609527 -- Deployment 'LLMServer:my-qwen-VL' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:37:50,679 controller 609527 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.51it/s]\u001b[32m [repeated 11x across cluster]\u001b[0mm(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:54 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/2ae962d16e/rank_0_0/backbone for vLLM's torch.compile\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:54 [backends.py:550] Dynamo bytecode transform time: 5.62 s\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:37:57 [backends.py:194] Cache the graph for dynamic shape for later use\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:38:20,714 controller 609527 -- Deployment 'LLMServer:my-qwen-VL' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m WARNING 2026-02-06 12:38:20,714 controller 609527 -- Deployment 'OpenAiIngress' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
                        "\u001b[36m(ServeController pid=609527)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:20 [backends.py:215] Compiling a graph for dynamic shape takes 26.13 s\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:22 [monitor.py:34] torch.compile takes 31.75 s in total\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:23 [gpu_worker.py:298] Available KV cache memory: 53.08 GiB\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:05 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m WARNING 02-06 12:37:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m WARNING 02-06 12:37:10 [profiling.py:280] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:10 [gpu_model_runner.py:2338] Starting to load model qwen/Qwen2.5-VL-7B-Instruct...\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:10 [gpu_model_runner.py:2370] Loading model from scratch...\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m WARNING 02-06 12:37:10 [cuda.py:217] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:10 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:11 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:40 [weight_utils.py:369] Time spent downloading weights for qwen/Qwen2.5-VL-7B-Instruct: 29.649602 seconds\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:44 [default_loader.py:268] Loading weights took 3.51 seconds\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:45 [gpu_model_runner.py:2392] Model loading took 15.6269 GiB and 33.980825 seconds\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:45 [gpu_model_runner.py:3000] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:54 [backends.py:539] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/2ae962d16e/rank_0_0/backbone for vLLM's torch.compile\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:54 [backends.py:550] Dynamo bytecode transform time: 5.62 s\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:37:57 [backends.py:194] Cache the graph for dynamic shape for later use\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:38:24 [kv_cache_utils.py:864] GPU KV cache size: 993,808 tokens\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:38:24 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 121.31x\n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 2/67 [00:00<00:03, 18.79it/s]\n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:02<00:00, 28.59it/s]\n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 23.61it/s]\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m INFO 02-06 12:38:27 [core.py:218] init engine (profile, create kv cache, warmup model) took 42.67 seconds\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:38:20 [backends.py:215] Compiling a graph for dynamic shape takes 26.13 s\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:27 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.47 GiB\n",
                        "\u001b[36m(RayWorkerWrapper pid=30452, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:27 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 15.63 GiB for weight, 2.6 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=56324698009` to fit into requested memory, or `--kv-cache-memory=64309040640` to fully utilize gpu memory. Current kv cache memory in use is 56989495193 bytes.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]re_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 27.76it/s]\u001b[32m [repeated 43x across cluster]\u001b[0mid=30452)\u001b[0m \n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:02<00:00, 28.59it/s]d=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 23.61it/s]d=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m \n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m WARNING 02-06 12:38:34 [profiling.py:280] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:38:22 [monitor.py:34] torch.compile takes 31.75 s in total\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:38:23 [gpu_worker.py:298] Available KV cache memory: 53.08 GiB\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:38:27 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.47 GiB\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m \u001b[1;36m(EngineCore_DP0 pid=30381)\u001b[0;0m \u001b[36m(RayWorkerWrapper pid=30452)\u001b[0m INFO 02-06 12:38:27 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 15.63 GiB for weight, 2.6 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=56324698009` to fit into requested memory, or `--kv-cache-memory=64309040640` to fully utilize gpu memory. Current kv cache memory in use is 56989495193 bytes.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 62113\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [api_server.py:1692] Supported_tasks: ['generate']\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m WARNING 02-06 12:38:35 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [serving_responses.py:130] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 1e-06}\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [serving_chat.py:137] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 1e-06}\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 02-06 12:38:35 [serving_completion.py:76] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'temperature': 1e-06}\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:38:35,734 default_LLMServer:my-qwen-VL 1f7o2sx5 -- Started vLLM engine.\n",
                        "\u001b[36m(ServeReplica:default:LLMServer:my-qwen-VL pid=29865, ip=10.128.8.94)\u001b[0m INFO 2026-02-06 12:38:35,847 default_LLMServer:my-qwen-VL 1f7o2sx5 0e7d4ade-9b09-423f-b315-e28f92331005 -- CALL llm_config OK 2.1ms\n",
                        "INFO 2026-02-06 12:38:36,727 serve 609405 -- Application 'default' is ready at http://0.0.0.0:8000/.\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!serve run serve_qwen_VL:app --non-blocking"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d36f41d1",
            "metadata": {},
            "source": [
                "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
                "\n",
                "---\n",
                "\n",
                "### Sending requests with images\n",
                "\n",
                "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`.\n",
                "\n",
                "Example curl with image URL:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "400e7790",
            "metadata": {},
            "outputs": [],
            "source": [
                "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
                "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
                "  -H \"Content-Type: application/json\" \\\n",
                "  -d '{ \"model\": \"my-qwen-VL\", \"messages\": [ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"What do you see in this image?\"}, {\"type\": \"image_url\", \"image_url\": { \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\" }} ] } ] }'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "291743a5",
            "metadata": {},
            "source": [
                "Example Python with image URL:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6b447094",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "{\"asctime\": \"2026-02-06 12:38:52,973\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 609385, \"job_id\": \"1c000000\", \"worker_id\": \"1c000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770410332973892038}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "This image shows two tabby cats lying on a pink couch, appearing to be asleep or resting. Between them are two remote controls placed on the couch. The cats are curled up in comfortable positions, and their relaxed posture suggests they are at ease in their environment."
                    ]
                }
            ],
            "source": [
                "#client_url_image.py\n",
                "from urllib.parse import urljoin\n",
                "from openai import OpenAI\n",
                "\n",
                "API_KEY = \"FAKE_KEY\"\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"my-qwen-VL\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
                "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}}\n",
                "            ]\n",
                "        }\n",
                "    ],\n",
                "    temperature=0.5,\n",
                "    stream=True\n",
                ")\n",
                "\n",
                "for chunk in response:\n",
                "    content = chunk.choices[0].delta.content\n",
                "    if content:\n",
                "        print(content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "811f1d41",
            "metadata": {},
            "source": [
                "Example Python with local image:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "8296023b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "{\"asctime\": \"2026-02-06 12:39:48,908\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"process\": 609385, \"job_id\": \"1c000000\", \"worker_id\": \"1c000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"1a6ddbbb716b74256e415b58e3dca445abdb4074bbfecbc482406ab0\", \"timestamp_ns\": 1770410388908303106}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "This"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " image shows a small dog, possibly a Jack Russell Terrier, standing on its hind legs and giving a high-five to a person's hand. The dog appears happy and engaged, with its mouth open as if smiling or panting. The background features an outdoor setting with greenery, indicating that the scene takes place in a park or a grassy area on a sunny day."
                    ]
                }
            ],
            "source": [
                "#client_local_image.py\n",
                "from urllib.parse import urljoin\n",
                "import base64\n",
                "from openai import OpenAI\n",
                "\n",
                "API_KEY = \"FAKE_KEY\"\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
                "\n",
                "### From an image locally saved as `example.jpg`\n",
                "# Load and encode image as base64\n",
                "with open(\"example.jpg\", \"rb\") as f:\n",
                "    img_base64 = base64.b64encode(f.read()).decode()\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"my-qwen-VL\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
                "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n",
                "            ]\n",
                "        }\n",
                "    ],\n",
                "    temperature=0.5,\n",
                "    stream=True\n",
                ")\n",
                "\n",
                "for chunk in response:\n",
                "    content = chunk.choices[0].delta.content\n",
                "    if content:\n",
                "        print(content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ccc60c1f",
            "metadata": {},
            "source": [
                "\n",
                "---\n",
                "\n",
                "### Shutdown \n",
                "\n",
                "Shutdown your LLM service:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0ee4b879",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2026-02-06 12:39:57,302\tSUCC scripts.py:774 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!serve shutdown -y"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a94c0307",
            "metadata": {},
            "source": [
                "\n",
                "---\n",
                "\n",
                "## Deploy to production with Anyscale services\n",
                "\n",
                "For production, it's recommended to use Anyscale services to deploy your Ray Serve app on a dedicated cluster without code changes. Anyscale provides scalability, fault tolerance, and load balancing, ensuring resilience against node failures, high traffic, and rolling updates. See [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html#deploy-to-production-with-anyscale-services) for an example with a small-sized model like the *Qwen2.5-VL-7&nbsp;B-Instruct* used in this tutorial.\n",
                "\n",
                "---\n",
                "\n",
                "## Limiting images per prompt\n",
                "\n",
                "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/stable/) as its backend engine. You can configure vLLM by passing parameters through the `engine_kwargs` section of your Serve LLM configuration. For a full list of supported options, see the [vLLM documentation](https://docs.vllm.ai/en/stable/configuration/engine_args.html#multimodalconfig).  \n",
                "\n",
                "In particular, you can limit the number of images per request by setting `limit_mm_per_prompt` in your configuration.  \n",
                "```yaml\n",
                "applications:\n",
                "- ...\n",
                "  args:\n",
                "    llm_configs:\n",
                "        ...\n",
                "        engine_kwargs:\n",
                "          ...\n",
                "          limit_mm_per_prompt: {\"image\": 3}\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "In this tutorial, you deployed a vision LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray cluster, and send requests with images."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
