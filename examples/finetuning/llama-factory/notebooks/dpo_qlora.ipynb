{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) at scale with QLoRA\n",
    "\n",
    "This guide provides a step-by-step workflow for preference fine-tuning the [`Qwen/Qwen2.5-7B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) model on a multi-GPU Anyscale cluster. You use LLaMA-Factory as the training framework and `QLoRA` to reduce memory requirements and enable efficient multi-GPU training.\n",
    "\n",
    "DPO aligns a model with human preferences using pairs of “chosen” and “rejected” responses. Rather than training a separate reward model, DPO directly optimizes the policy to increase the likelihood of preferred outputs and decrease the likelihood of rejected ones.\n",
    "\n",
    "## Step 1: Set up your environment\n",
    "\n",
    "### Dependencies\n",
    "First, ensure your environment has the correct libraries. Start with a pre-built container image and install LLaMA-Factory and DeepSpeed on top of it.\n",
    "\n",
    "Recommended container image:\n",
    "```bash\n",
    "anyscale/ray-llm:2.48.0-py311-cu128\n",
    "```\n",
    "\n",
    "Execute the following commands to install the required packages and optional tools for experiment tracking and faster model downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastmcp 2.14.5 requires pydantic[email]>=2.11.7, but you have pydantic 2.10.6 which is incompatible.\n",
      "mcp 1.26.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `llamafactory` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `tensorboard` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `bitsandbytes` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `autoawq` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `hf_transfer` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the specific version of LLaMA-Factory\n",
    "pip install -q llamafactory==0.9.3\n",
    "\n",
    "# (Optional) For visualizing training metrics and logs\n",
    "pip install -q tensorboard==2.20.0\n",
    "\n",
    "# (Optional) For lightweight 8-bit and 4-bit optimizers and inference\n",
    "pip install -q bitsandbytes==0.47.0\n",
    "\n",
    "# (Optional) For AWQ quantization support\n",
    "pip install -q autoawq==0.2.9\n",
    "\n",
    "# (Optional) For accelerated model downloads from Hugging Face\n",
    "pip install -q hf_transfer==0.1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and compute resources\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Base model** | [`Qwen/Qwen2.5-7B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) |\n",
    "| **Workers** | 4 × L4 / A10G |\n",
    "\n",
    "Compared to SFT, DPO holds two copies of the model (policy and reference), and alignment datasets often use long contexts, so it's the ideal workflow for memory optimization techniques such as **QLoRA**. On 24 GB NVIDIA L4 GPUs, running DPO at FP16 for 7B models generally OOMs without QLoRA.\n",
    "\n",
    "## Step 2: Prepare the dataset\n",
    "\n",
    "### Understand the dataset\n",
    "This tutorial uses [`ultrafeedback.jsonl`](https://huggingface.co/datasets/kaitchup/UltraFeedback-prompt-chosen-rejected), a preference dataset tailored for DPO. Each sample contains one instruction **prompt** and two candidate completions: a **preferred** (`chosen`) response and a **less preferred** (`rejected`) response.\n",
    "\n",
    "This dataset includes:\n",
    "- `prompt`: An instruction or question to answer, often multi-sentence, with constraints.\n",
    "- `chosen`: The response that best follows the instruction.\n",
    "- `rejected`: A weaker alternative for the same prompt.\n",
    "\n",
    "**Dataset example**\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"Paraphrase the given questions to have different wording. Your paraphrased questions should have the same answer as the original question. Try to change the sentence as much as possible using synonyms and/or rearranging the structure of the sentence. The questions are in three domains: presidents, national parks, and dogs. Each question has a keyword indicating its domain. Keywords are \\\"this national park\\\", \\\"this dog breed\\\", and \\\"this president\\\", which will be replaced with the name of an actual president, a national park, or a breed of dog. Hence, in paraphrasing, this keyword should also be used the same way. Do not write questions that compare or involve multiple domains. Do not write open-ended or subjective questions (e.g., questions that can be answered differently by different people.) Make your questions specific and concrete. Your question should have the same type of answer as the original question(e.g., if the question is extractive, the paraphrased question should be extractive as well.)\\n\\nWhat lakes are in this national park?\",\n",
    "\n",
    "  \"rejected\": \"What bodies of water are located in this national park? \\n\\nWhich president is commonly known for his efforts to protect natural resources?\\n\\nWhich president is recognized for their dedication to preserving the environment? \\n\\nWhat type of dog breed is known for its loyalty and affectionate nature?\\n\\nWhat breed of dog is renowned for its faithfulness and loving personality?\",\n",
    "  \n",
    "  \"chosen\": \"Which bodies of water can be found within the borders of this particular national park?\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Register the dataset\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add a **`dataset_info.json`** to **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)** such as `/mnt/cluster_storage`. This configuration file acts as a central registry for all your datasets. It maps a custom name to your dataset file location, format, and column structure. \n",
    "\n",
    "If you plan to run DPO post-training on the `ultrafeedback` dataset, first complete the setup steps below. Ensure that you place the dataset files in a storage location that all workers can access like a shared mount or object storage. Avoid storing large files on the head node. \n",
    "\n",
    "`dataset_info.json`\n",
    "```json\n",
    "{\n",
    "  \"my_ultrafeedback\": {\n",
    "    \"file_name\": \"/mnt/cluster_storage/ultrafeedback.jsonl\",\n",
    "    \"ranking\": true,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed dataset preparation and formatting guide, see [Choose your data format](https://docs.anyscale.com/llm/fine-tuning/data-preparation#preference-methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2026-02-08 09:30:25--  https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/alpaca/ultrafeedback.jsonl\n",
      "Resolving anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)... 52.218.179.34, 52.92.206.122, 52.92.251.90, ...\n",
      "Connecting to anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)|52.218.179.34|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 291881 (285K) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘/mnt/cluster_storage/ultrafeedback.jsonl’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 17%  266K 1s\n",
      "    50K .......... .......... .......... .......... .......... 35%  336K 1s\n",
      "   100K .......... .......... .......... .......... .......... 52%  456K 0s\n",
      "   150K .......... .......... .......... .......... .......... 70% 66.5M 0s\n",
      "   200K .......... .......... .......... .......... .......... 87% 1.27M 0s\n",
      "   250K .......... .......... .......... .....                100%  321K=0.6s\n",
      "\n",
      "2026-02-08 09:30:26 (479 KB/s) - ‘/mnt/cluster_storage/ultrafeedback.jsonl’ saved [291881/291881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Make sure all files are accessible to worker nodes\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "wget https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/alpaca/ultrafeedback.jsonl -O /mnt/cluster_storage/ultrafeedback.jsonl\n",
    "# Create a copy of the dataset registry in /mnt/cluster_storage\n",
    "cp ../dataset-configs/dataset_info.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the preference-tuning config (DPO and QLoRA)\n",
    "\n",
    "Next, create the YAML configuration file that defines your DPO run. It specifies the base model, quantization (QLoRA), dataset, DPO hyperparameters, logging, and Ray cluster resources.\n",
    "\n",
    "**Important notes:**\n",
    "- **QLoRA quantization:** `quantization_bit: 4` with `quantization_method: bnb` applies quantization using bitsandbytes, reducing memory while preserving quality. If you use a model *pre-quantized* with AWQ, **omit** these keys.\n",
    "- **LoRA setup**: If you prefer standard LoRA, **disable quantization** by removing both `quantization_bit` and `quantization_method` from the config.\n",
    "- **Access & paths:** The YAML only needs to be on the **head node**, but any referenced paths (`dataset_dir`, `output_dir`) must reside on storage **reachable by all workers** (for example, `/mnt/cluster_storage/`).\n",
    "- **Gated models:** If your base model has gated access (for example, Llama) on Hugging Face, set `HF_TOKEN` in the runtime environment.\n",
    "\n",
    "### Configure LLaMA-Factory with Ray\n",
    "\n",
    "**Note**: To customize the training configuration, edit `train-configs/dpo_qlora.yaml`. \n",
    "\n",
    "```yaml\n",
    "# dpo_qlora.yaml\n",
    "\n",
    "### model\n",
    "trust_remote_code: true\n",
    "model_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "### method\n",
    "# If you instead want to use just LoRA, or a pre-quantized model like Qwen/Qwen2.5-7B-Instruct-AWQ, then omit the quantization_bit/method keys below\n",
    "quantization_bit: 4 # 4-bit base weights (QLoRA). Use 8 for 8-bit; omit for FP16/BF16\n",
    "quantization_method: bnb  # QLoRA via BitsAndBytes or hqq / eetq\n",
    "\n",
    "stage: dpo\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "pref_beta: 0.1\n",
    "pref_loss: sigmoid  # choices: [sigmoid (dpo), orpo, simpo]\n",
    "\n",
    "# local dataset\n",
    "dataset: my_ultrafeedback\n",
    "dataset_dir: /mnt/cluster_storage\n",
    "\n",
    "template: qwen\n",
    "cutoff_len: 1024\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: qwen2.5_7b_qlora_dpo\n",
    "logging_steps: 5\n",
    "save_steps: 5              # For tensorboard logging purpose too. Can increase if not using tensorboard\n",
    "plot_loss: true\n",
    "report_to: tensorboard  # or none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 2\n",
    "num_train_epochs: 3.0  # Low for demo purpose; adjust as needed\n",
    "learning_rate: 5.0e-6\n",
    "bf16: true\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### ray\n",
    "ray_run_name: qwen2.5_7b_qlora_dpo\n",
    "ray_storage_path: /mnt/cluster_storage/\n",
    "ray_num_workers: 4  # Number of GPUs to use.\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  anyscale/accelerator_shape:4xL4: 0.001  # Use this to specify a specific node shape.\n",
    "  # accelerator_type:L4: 0.001            # Or use this to simply specify a GPU type.\n",
    "  # See https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types.\n",
    "\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      # If using gated models like meta-llama/Llama-3.1-8B-Instruct\n",
    "      # HF_TOKEN: <your_huggingface_token>\n",
    "      # Enable faster downloads if hf_transfer is installed:\n",
    "      HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "```\n",
    "\n",
    "## Step 4: Train and monitor\n",
    "\n",
    "With all configurations in place, you can launch fine-tuning or post-training in one of two ways:\n",
    "\n",
    "### Option A: Run from a workspace (quick start)\n",
    "\n",
    "The `USE_RAY=1` prefix tells LLaMA-Factory to run in distributed mode on the Ray cluster attached to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-08 09:30:36,172] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2026-02-08 09:30:36,173] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "INFO 02-08 09:30:38 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 02-08 09:30:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 09:30:40,431\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.128.5.218:6379...\n",
      "2026-02-08 09:30:40,442\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-c1mvc6t862zj4fbguuknngnrgv.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-08 09:30:40,443\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_50420c404e85b1830c428a285b74dd50fa30970a.zip' (0.18MiB) to Ray cluster...\n",
      "2026-02-08 09:30:40,444\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_50420c404e85b1830c428a285b74dd50fa30970a.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/qwen2.5_7b_qlora_dpo\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2026-02-08_07-42-51_475236_185/artifacts/2026-02-08_09-30-40/qwen2.5_7b_qlora_dpo/driver_artifacts`\n",
      "\u001b[36m(TrainTrainable pid=5784, ip=10.128.7.103)\u001b[0m [2026-02-08 09:31:45,373] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "\u001b[36m(TrainTrainable pid=5784, ip=10.128.7.103)\u001b[0m [2026-02-08 09:31:45,374] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                             True │\n",
      "│ train_loop_config/args/cutoff_len                                                                       1024 │\n",
      "│ train_loop_config/args/dataset                                                              my_ultrafeedback │\n",
      "│ train_loop_config/args/dataset_dir                                                      /mnt/cluster_storage │\n",
      "│ train_loop_config/args/ddp_timeout                                                                 180000000 │\n",
      "│ train_loop_config/args/do_train                                                                         True │\n",
      "│ train_loop_config/args/finetuning_type                                                                  lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                         2 │\n",
      "│ train_loop_config/args/learning_rate                                                                   5e-06 │\n",
      "│ train_loop_config/args/logging_steps                                                                       5 │\n",
      "│ train_loop_config/args/lora_rank                                                                           8 │\n",
      "│ train_loop_config/args/lora_target                                                                       all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                              cosine │\n",
      "│ train_loop_config/args/max_samples                                                                      1000 │\n",
      "│ train_loop_config/args/model_name_or_path                                               ...en2.5-7B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                                  3.0 │\n",
      "│ train_loop_config/args/output_dir                                                       qwen2.5_7b_qlora_dpo │\n",
      "│ train_loop_config/args/overwrite_cache                                                                  True │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                         1 │\n",
      "│ train_loop_config/args/plot_loss                                                                        True │\n",
      "│ train_loop_config/args/pref_beta                                                                         0.1 │\n",
      "│ train_loop_config/args/pref_loss                                                                     sigmoid │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                          16 │\n",
      "│ train_loop_config/args/quantization_bit                                                                    4 │\n",
      "│ train_loop_config/args/quantization_method                                                               bnb │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_HUB_ENABLE_HF_TRANSFER                      1 │\n",
      "│ train_loop_config/args/ray_num_workers                                                                     4 │\n",
      "│ train_loop_config/args/ray_run_name                                                     qwen2.5_7b_qlora_dpo │\n",
      "│ train_loop_config/args/ray_storage_path                                                 .../cluster_storage/ │\n",
      "│ train_loop_config/args/report_to                                                                 tensorboard │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                            1 │\n",
      "│ train_loop_config/args/save_steps                                                                          5 │\n",
      "│ train_loop_config/args/stage                                                                             dpo │\n",
      "│ train_loop_config/args/template                                                                         qwen │\n",
      "│ train_loop_config/args/trust_remote_code                                                                True │\n",
      "│ train_loop_config/args/warmup_ratio                                                                      0.1 │\n",
      "│ train_loop_config/callbacks                                                             ... 0x7f6025da2b90>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=5784, ip=10.128.7.103)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=5784, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=5917) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=5784, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=5920) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=5784, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=5919) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=5784, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=5918) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5918, ip=10.128.7.103)\u001b[0m [2026-02-08 09:31:57,268] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [WARNING|2026-02-08 09:32:01] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:01] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:01] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:02,641 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 09:32:02,928 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:32:03,542 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:32:03,544 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,747 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,747 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,747 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,747 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,747 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,748 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 09:32:03,748 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 09:32:04,009 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:04] llamafactory.data.loader:143 >> Loading dataset ultrafeedback.jsonl...\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [2026-02-08 09:31:57,295] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5920, ip=10.128.7.103)\u001b[0m [rank1]:[W208 09:32:04.633084978 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100 examples [00:00, 21953.96 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  88%|████████▊ | 88/100 [00:00<00:00, 846.27 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 100/100 [00:00<00:00, 516.07 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   7%|▋         | 7/100 [00:00<00:08, 11.57 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  21%|██        | 21/100 [00:00<00:02, 32.45 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  34%|███▍      | 34/100 [00:00<00:01, 46.15 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  46%|████▌     | 46/100 [00:01<00:00, 54.50 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 58/100 [00:01<00:00, 59.95 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 76/100 [00:01<00:00, 73.09 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 88/100 [00:01<00:00, 73.16 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 100/100 [00:01<00:00, 72.75 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 100/100 [00:01<00:00, 53.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m chosen_input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 10398, 25, 16246, 264, 11652, 304, 8585, 11, 3410, 458, 13578, 62230, 81, 1475, 2319, 504, 279, 4024, 429, 51844, 279, 1852, 7290, 624, 2505, 25, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 8, 409, 85838, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 13, 1967, 1723, 59304, 96858, 510, 5097, 25, 151645, 198, 151644, 77091, 198, 16, 13, 4270, 1342, 10632, 279, 2661, 11652, 304, 8585, 624, 623, 3757, 12224, 20731, 82008, 320, 59778, 315, 19833, 24847, 82008, 8, 504, 85838, 389, 5470, 220, 21, 11, 220, 16, 22, 17, 20, 13, 10964, 2841, 1033, 510, 41462, 20108, 312, 759, 12784, 424, 25, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 4260, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 1842, 42053, 59304, 96858, 510, 17, 13, 3321, 4079, 519, 279, 2661, 11652, 304, 264, 2155, 1616, 11, 50010, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 88260, 9281, 794, 3757, 1842, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 409, 85838, 568, 1967, 324, 6560, 29719, 1788, 510, 18, 13, 13293, 62230, 81, 1475, 11652, 429, 51844, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 11300, 409, 794, 3757, 1842, 20731, 82008, 320, 10302, 517, 1187, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 45052, 14508, 15555, 939, 59304, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m chosen_inputs:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Definition: Given a sentence in French, provide an equivalent paraphrased version from the original that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Input: St John a épousé Elizabeth Crowley (fille d'Ambrose Crowley) de Greenwich le 6 mars 1725. Leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Output:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 1. Paraphrase the given sentence in French.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m St John married Elizabeth Crowley (daughter of Ambrose Crowley) from Greenwich on March 6, 1725. Their children were:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Première rephrasage: le 6 mars 1725, St John a épousé Elizabeth Crowley (la fille d'Ambrose Crowley de Greenwich), et leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 2. Rephrase the given sentence in a different way, retaining the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Le 6 mars 1725, mariage entre St John et Elizabeth Crowley (fille d'Ambrose Crowley de Greenwich). Leur descendance est:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 3. Another paraphrased sentence that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Le 6 mars 1725, union de St John et Elizabeth Crowley (étant la fille d'Ambrose Crowley de Greenwich), ils ont eu des enfants.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m chosen_label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 4270, 1342, 10632, 279, 2661, 11652, 304, 8585, 624, 623, 3757, 12224, 20731, 82008, 320, 59778, 315, 19833, 24847, 82008, 8, 504, 85838, 389, 5470, 220, 21, 11, 220, 16, 22, 17, 20, 13, 10964, 2841, 1033, 510, 41462, 20108, 312, 759, 12784, 424, 25, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 4260, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 1842, 42053, 59304, 96858, 510, 17, 13, 3321, 4079, 519, 279, 2661, 11652, 304, 264, 2155, 1616, 11, 50010, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 88260, 9281, 794, 3757, 1842, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 409, 85838, 568, 1967, 324, 6560, 29719, 1788, 510, 18, 13, 13293, 62230, 81, 1475, 11652, 429, 51844, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 11300, 409, 794, 3757, 1842, 20731, 82008, 320, 10302, 517, 1187, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 45052, 14508, 15555, 939, 59304, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m chosen_labels:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 1. Paraphrase the given sentence in French.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m St John married Elizabeth Crowley (daughter of Ambrose Crowley) from Greenwich on March 6, 1725. Their children were:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Première rephrasage: le 6 mars 1725, St John a épousé Elizabeth Crowley (la fille d'Ambrose Crowley de Greenwich), et leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 2. Rephrase the given sentence in a different way, retaining the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Le 6 mars 1725, mariage entre St John et Elizabeth Crowley (fille d'Ambrose Crowley de Greenwich). Leur descendance est:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m 3. Another paraphrased sentence that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Le 6 mars 1725, union de St John et Elizabeth Crowley (étant la fille d'Ambrose Crowley de Greenwich), ils ont eu des enfants.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m rejected_input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 10398, 25, 16246, 264, 11652, 304, 8585, 11, 3410, 458, 13578, 62230, 81, 1475, 2319, 504, 279, 4024, 429, 51844, 279, 1852, 7290, 624, 2505, 25, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 8, 409, 85838, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 13, 1967, 1723, 59304, 96858, 510, 5097, 25, 151645, 198, 151644, 77091, 198, 1925, 220, 21, 339, 5470, 220, 16, 22, 17, 20, 11, 20731, 82008, 320, 54032, 24847, 82008, 594, 9803, 504, 85838, 8, 2684, 12224, 311, 794, 13, 3757, 11, 323, 3786, 807, 1030, 1493, 2841, 25, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m rejected_inputs:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Definition: Given a sentence in French, provide an equivalent paraphrased version from the original that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Input: St John a épousé Elizabeth Crowley (fille d'Ambrose Crowley) de Greenwich le 6 mars 1725. Leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Output:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m On 6th March 1725, Elizabeth Crowley (Ambrose Crowley's daughter from Greenwich) got married to St. John, and together they had these children:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m rejected_label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1925, 220, 21, 339, 5470, 220, 16, 22, 17, 20, 11, 20731, 82008, 320, 54032, 24847, 82008, 594, 9803, 504, 85838, 8, 2684, 12224, 311, 794, 13, 3757, 11, 323, 3786, 807, 1030, 1493, 2841, 25, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m rejected_labels:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m On 6th March 1725, Elizabeth Crowley (Ambrose Crowley's daughter from Greenwich) got married to St. John, and together they had these children:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5919, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:01] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:32:08,495 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:32:08,496 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:08] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:08] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:1151] 2026-02-08 09:32:10,043 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/model.safetensors.index.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [rank0]:[W208 09:32:05.594612754 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:2241] 2026-02-08 09:32:36,859 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 09:32:36,861 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:5131] 2026-02-08 09:32:46,930 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:5139] 2026-02-08 09:32:46,931 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1090] 2026-02-08 09:32:47,039 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 09:32:47,040 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,k_proj,down_proj,o_proj,gate_proj,v_proj,q_proj\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 09:32:47] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:756] 2026-02-08 09:32:47,560 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2409] 2026-02-08 09:32:48,352 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2410] 2026-02-08 09:32:48,352 >>   Num examples = 100\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2411] 2026-02-08 09:32:48,352 >>   Num Epochs = 3\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2412] 2026-02-08 09:32:48,352 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2415] 2026-02-08 09:32:48,352 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2416] 2026-02-08 09:32:48,352 >>   Gradient Accumulation steps = 2\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2417] 2026-02-08 09:32:48,353 >>   Total optimization steps = 39\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2418] 2026-02-08 09:32:48,356 >>   Number of trainable parameters = 20,185,088\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]28.7.103)\u001b[0m \n",
      "  3%|▎         | 1/39 [00:01<01:03,  1.66s/it])\u001b[0m \n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  5%|▌         | 2/39 [00:03<00:58,  1.59s/it])\u001b[0m \n",
      "  8%|▊         | 3/39 [00:04<00:58,  1.62s/it])\u001b[0m \n",
      " 10%|█         | 4/39 [00:06<00:56,  1.62s/it])\u001b[0m \n",
      " 13%|█▎        | 5/39 [00:08<00:56,  1.67s/it][INFO|trainer.py:3993] 2026-02-08 09:32:56,604 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6902, 'grad_norm': 7.472792148590088, 'learning_rate': 5e-06, 'rewards/chosen': -0.0011365606915205717, 'rewards/rejected': -0.008174104616045952, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.007037544157356024, 'logps/chosen': -280.1697692871094, 'logps/rejected': -292.0281677246094, 'logits/chosen': -0.8638967275619507, 'logits/rejected': -0.8488122224807739, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5920, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:32:56,858 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:32:56,858 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:32:56,993 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-5/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:32:56,994 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-5/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:32:56,994 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-5/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2026-02-08 09:33:00. Total running time: 2min 19s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s               71.50077 │\n",
      "│ time_total_s                   71.50077 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                               0.4 │\n",
      "│ grad_norm                       7.47279 │\n",
      "│ learning_rate                   0.00001 │\n",
      "│ logits/chosen                   -0.8639 │\n",
      "│ logits/rejected                -0.84881 │\n",
      "│ logps/chosen                 -280.16977 │\n",
      "│ logps/rejected               -292.02817 │\n",
      "│ loss                             0.6902 │\n",
      "│ rewards/accuracies                 0.35 │\n",
      "│ rewards/chosen                 -0.00114 │\n",
      "│ rewards/margins                 0.00704 │\n",
      "│ rewards/rejected               -0.00817 │\n",
      "│ step                                  5 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/39 [00:13<01:30,  2.74s/it])\u001b[0m \n",
      " 18%|█▊        | 7/39 [00:14<01:14,  2.34s/it])\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 21%|██        | 8/39 [00:16<01:03,  2.05s/it])\u001b[0m \n",
      " 23%|██▎       | 9/39 [00:17<00:59,  2.00s/it])\u001b[0m \n",
      " 26%|██▌       | 10/39 [00:19<00:54,  1.90s/it][INFO|trainer.py:3993] 2026-02-08 09:33:07,918 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.7013, 'grad_norm': 11.50890064239502, 'learning_rate': 4.752422169756048e-06, 'rewards/chosen': -0.004898893181234598, 'rewards/rejected': 0.008253341540694237, 'rewards/accuracies': 0.5750000476837158, 'rewards/margins': -0.013152234256267548, 'logps/chosen': -278.3939208984375, 'logps/rejected': -295.9100341796875, 'logits/chosen': -0.8270912170410156, 'logits/rejected': -0.977357029914856, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5918, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:33:08,167 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:33:08,167 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:33:08,302 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-10/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:33:08,302 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-10/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:33:08,302 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-10/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 2 at 2026-02-08 09:33:11. Total running time: 2min 30s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000001 │\n",
      "│ time_this_iter_s               11.19224 │\n",
      "│ time_total_s                     82.693 │\n",
      "│ training_iteration                    2 │\n",
      "│ epoch                               0.8 │\n",
      "│ grad_norm                       11.5089 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.82709 │\n",
      "│ logits/rejected                -0.97736 │\n",
      "│ logps/chosen                 -278.39392 │\n",
      "│ logps/rejected               -295.91003 │\n",
      "│ loss                             0.7013 │\n",
      "│ rewards/accuracies                0.575 │\n",
      "│ rewards/chosen                  -0.0049 │\n",
      "│ rewards/margins                -0.01315 │\n",
      "│ rewards/rejected                0.00825 │\n",
      "│ step                                 10 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 2 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/39 [00:24<01:19,  2.85s/it]\u001b[0m \n",
      " 31%|███       | 12/39 [00:25<01:05,  2.42s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000001)\n",
      " 33%|███▎      | 13/39 [00:26<00:47,  1.84s/it]\u001b[0m \n",
      " 36%|███▌      | 14/39 [00:28<00:46,  1.85s/it]\u001b[0m \n",
      " 38%|███▊      | 15/39 [00:30<00:43,  1.80s/it][INFO|trainer.py:3993] 2026-02-08 09:33:18,446 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6336, 'grad_norm': 6.3686323165893555, 'learning_rate': 4.058724504646834e-06, 'rewards/chosen': -0.02141275256872177, 'rewards/rejected': -0.0034777740947902203, 'rewards/accuracies': 0.3611111044883728, 'rewards/margins': -0.017934981733560562, 'logps/chosen': -221.60379028320312, 'logps/rejected': -241.5094757080078, 'logits/chosen': -0.7992151379585266, 'logits/rejected': -0.9283792972564697, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5920, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:33:18,698 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:33:18,698 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:33:18,831 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-15/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:33:18,832 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-15/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:33:18,832 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-15/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 3 at 2026-02-08 09:33:21. Total running time: 2min 41s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000002 │\n",
      "│ time_this_iter_s               10.60935 │\n",
      "│ time_total_s                   93.30236 │\n",
      "│ training_iteration                    3 │\n",
      "│ epoch                              1.16 │\n",
      "│ grad_norm                       6.36863 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.79922 │\n",
      "│ logits/rejected                -0.92838 │\n",
      "│ logps/chosen                 -221.60379 │\n",
      "│ logps/rejected               -241.50948 │\n",
      "│ loss                             0.6336 │\n",
      "│ rewards/accuracies              0.36111 │\n",
      "│ rewards/chosen                 -0.02141 │\n",
      "│ rewards/margins                -0.01793 │\n",
      "│ rewards/rejected               -0.00348 │\n",
      "│ step                                 15 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 3 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 16/39 [00:34<01:02,  2.71s/it]\u001b[0m \n",
      " 44%|████▎     | 17/39 [00:36<00:54,  2.46s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 46%|████▌     | 18/39 [00:38<00:48,  2.29s/it]\u001b[0m \n",
      " 49%|████▊     | 19/39 [00:40<00:40,  2.04s/it]\u001b[0m \n",
      " 51%|█████▏    | 20/39 [00:41<00:34,  1.82s/it][INFO|trainer.py:3993] 2026-02-08 09:33:29,793 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6843, 'grad_norm': 6.403764247894287, 'learning_rate': 3.056302334890786e-06, 'rewards/chosen': 0.001086606178432703, 'rewards/rejected': -0.01895829103887081, 'rewards/accuracies': 0.5750000476837158, 'rewards/margins': 0.020044900476932526, 'logps/chosen': -276.7025146484375, 'logps/rejected': -287.20098876953125, 'logits/chosen': -0.8141433000564575, 'logits/rejected': -0.8249074816703796, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:33:30,039 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:33:30,040 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:33:30,173 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-20/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:33:30,173 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-20/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:33:30,173 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-20/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=5918, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 4 at 2026-02-08 09:33:33. Total running time: 2min 52s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000003 │\n",
      "│ time_this_iter_s               11.46522 │\n",
      "│ time_total_s                  104.76758 │\n",
      "│ training_iteration                    4 │\n",
      "│ epoch                              1.56 │\n",
      "│ grad_norm                       6.40376 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.81414 │\n",
      "│ logits/rejected                -0.82491 │\n",
      "│ logps/chosen                 -276.70251 │\n",
      "│ logps/rejected               -287.20099 │\n",
      "│ loss                             0.6843 │\n",
      "│ rewards/accuracies                0.575 │\n",
      "│ rewards/chosen                  0.00109 │\n",
      "│ rewards/margins                 0.02004 │\n",
      "│ rewards/rejected               -0.01896 │\n",
      "│ step                                 20 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 4 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 21/39 [00:46<00:49,  2.72s/it]\u001b[0m \n",
      " 56%|█████▋    | 22/39 [00:47<00:38,  2.28s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000003)\n",
      " 59%|█████▉    | 23/39 [00:48<00:31,  1.98s/it]\u001b[0m \n",
      " 62%|██████▏   | 24/39 [00:50<00:27,  1.83s/it]\u001b[0m \n",
      " 64%|██████▍   | 25/39 [00:51<00:23,  1.67s/it][INFO|trainer.py:3993] 2026-02-08 09:33:39,926 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-25\n",
      "\u001b[36m(RayTrainWorker pid=5920, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6899, 'grad_norm': 8.657591819763184, 'learning_rate': 1.9436976651092143e-06, 'rewards/chosen': -0.013525770977139473, 'rewards/rejected': -0.02253251150250435, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.009006739594042301, 'logps/chosen': -220.19992065429688, 'logps/rejected': -298.4376525878906, 'logits/chosen': -0.8283650875091553, 'logits/rejected': -0.882324755191803, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:33:40,172 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:33:40,172 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:33:40,305 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-25/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:33:40,305 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-25/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:33:40,305 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-25/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 5 at 2026-02-08 09:33:43. Total running time: 3min 2s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000004 │\n",
      "│ time_this_iter_s               10.27101 │\n",
      "│ time_total_s                  115.03859 │\n",
      "│ training_iteration                    5 │\n",
      "│ epoch                              1.96 │\n",
      "│ grad_norm                       8.65759 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.82837 │\n",
      "│ logits/rejected                -0.88232 │\n",
      "│ logps/chosen                 -220.19992 │\n",
      "│ logps/rejected               -298.43765 │\n",
      "│ loss                             0.6899 │\n",
      "│ rewards/accuracies                 0.55 │\n",
      "│ rewards/chosen                 -0.01353 │\n",
      "│ rewards/margins                 0.00901 │\n",
      "│ rewards/rejected               -0.02253 │\n",
      "│ step                                 25 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 5 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 26/39 [00:56<00:33,  2.58s/it]\u001b[0m \n",
      " 69%|██████▉   | 27/39 [00:57<00:27,  2.33s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000004)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 72%|███████▏  | 28/39 [00:59<00:23,  2.10s/it]\u001b[0m \n",
      " 74%|███████▍  | 29/39 [01:00<00:18,  1.85s/it]\u001b[0m \n",
      " 77%|███████▋  | 30/39 [01:02<00:16,  1.86s/it][INFO|trainer.py:3993] 2026-02-08 09:33:51,108 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6025, 'grad_norm': 10.21045970916748, 'learning_rate': 9.412754953531664e-07, 'rewards/chosen': 0.002010171767324209, 'rewards/rejected': -0.05112072825431824, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.05313090234994888, 'logps/chosen': -319.646484375, 'logps/rejected': -264.9739990234375, 'logits/chosen': -0.8445010185241699, 'logits/rejected': -0.9562739729881287, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:33:51,462 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:33:51,462 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5918, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000005)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:33:51,594 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-30/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:33:51,595 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-30/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:33:51,595 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-30/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 6 at 2026-02-08 09:33:54. Total running time: 3min 14s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000005 │\n",
      "│ time_this_iter_s               11.35198 │\n",
      "│ time_total_s                  126.39057 │\n",
      "│ training_iteration                    6 │\n",
      "│ epoch                              2.32 │\n",
      "│ grad_norm                      10.21046 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                   -0.8445 │\n",
      "│ logits/rejected                -0.95627 │\n",
      "│ logps/chosen                 -319.64648 │\n",
      "│ logps/rejected                 -264.974 │\n",
      "│ loss                             0.6025 │\n",
      "│ rewards/accuracies              0.66667 │\n",
      "│ rewards/chosen                  0.00201 │\n",
      "│ rewards/margins                 0.05313 │\n",
      "│ rewards/rejected               -0.05112 │\n",
      "│ step                                 30 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 6 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 31/39 [01:07<00:22,  2.86s/it]\u001b[0m \n",
      " 82%|████████▏ | 32/39 [01:09<00:17,  2.45s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000005)\n",
      " 85%|████████▍ | 33/39 [01:10<00:12,  2.09s/it]\u001b[0m \n",
      " 87%|████████▋ | 34/39 [01:12<00:09,  1.91s/it]\u001b[0m \n",
      " 90%|████████▉ | 35/39 [01:13<00:07,  1.86s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'loss': 0.6922, 'grad_norm': 9.275382995605469, 'learning_rate': 2.4757783024395244e-07, 'rewards/chosen': -0.01869027502834797, 'rewards/rejected': -0.024014988914132118, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.0053247129544615746, 'logps/chosen': -224.70375061035156, 'logps/rejected': -296.4294128417969, 'logits/chosen': -0.7401316165924072, 'logits/rejected': -0.8596256375312805, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 35/39 [01:13<00:07,  1.86s/it][INFO|trainer.py:3993] 2026-02-08 09:34:02,272 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-35\n",
      "\u001b[36m(RayTrainWorker pid=5920, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:34:02,521 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:34:02,521 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:34:02,653 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-35/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:34:02,654 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-35/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:34:02,654 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-35/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 7 at 2026-02-08 09:34:05. Total running time: 3min 25s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000006 │\n",
      "│ time_this_iter_s               11.03235 │\n",
      "│ time_total_s                  137.42292 │\n",
      "│ training_iteration                    7 │\n",
      "│ epoch                              2.72 │\n",
      "│ grad_norm                       9.27538 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.74013 │\n",
      "│ logits/rejected                -0.85963 │\n",
      "│ logps/chosen                 -224.70375 │\n",
      "│ logps/rejected               -296.42941 │\n",
      "│ loss                             0.6922 │\n",
      "│ rewards/accuracies                0.575 │\n",
      "│ rewards/chosen                 -0.01869 │\n",
      "│ rewards/margins                 0.00532 │\n",
      "│ rewards/rejected               -0.02401 │\n",
      "│ step                                 35 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 7 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 36/39 [01:19<00:08,  2.87s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000006)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 95%|█████████▍| 37/39 [01:20<00:04,  2.40s/it]\u001b[0m \n",
      " 97%|█████████▋| 38/39 [01:21<00:02,  2.14s/it]\u001b[0m \n",
      "100%|██████████| 39/39 [01:22<00:00,  1.65s/it][INFO|trainer.py:3993] 2026-02-08 09:34:10,837 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-39\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:34:11,095 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:34:11,096 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:34:11,228 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-39/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:34:11,229 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-39/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:34:11,229 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-39/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000007)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 8 at 2026-02-08 09:34:14. Total running time: 3min 33s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000007 │\n",
      "│ time_this_iter_s                8.52126 │\n",
      "│ time_total_s                  145.94418 │\n",
      "│ training_iteration                    8 │\n",
      "│ epoch                              2.72 │\n",
      "│ grad_norm                       9.27538 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.74013 │\n",
      "│ logits/rejected                -0.85963 │\n",
      "│ logps/chosen                 -224.70375 │\n",
      "│ logps/rejected               -296.42941 │\n",
      "│ loss                             0.6922 │\n",
      "│ rewards/accuracies                0.575 │\n",
      "│ rewards/chosen                 -0.01869 │\n",
      "│ rewards/margins                 0.00532 │\n",
      "│ rewards/rejected               -0.02401 │\n",
      "│ step                                 35 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 8 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_e3928_00000_0_2026-02-08_09-30-40/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2676] 2026-02-08 09:34:14,588 >> \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "100%|██████████| 39/39 [01:26<00:00,  2.21s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:3993] 2026-02-08 09:34:14,592 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'train_runtime': 86.2318, 'train_samples_per_second': 3.479, 'train_steps_per_second': 0.452, 'train_loss': 0.6595334884447929, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:34:14,834 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:34:14,834 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:34:14,965 >> chat template saved in qwen2.5_7b_qlora_dpo/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:34:14,965 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:34:14,965 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   epoch                    =        3.0\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   total_flos               = 12512612GF\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   train_loss               =     0.6595\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   train_runtime            = 0:01:26.23\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   train_samples_per_second =      3.479\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m   train_steps_per_second   =      0.452\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Figure saved at: qwen2.5_7b_qlora_dpo/training_loss.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [INFO|modelcard.py:450] 2026-02-08 09:34:15,309 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m Figure saved at: qwen2.5_7b_qlora_dpo/training_rewards_accuracies.png\n",
      "\u001b[36m(RayTrainWorker pid=5917, ip=10.128.7.103)\u001b[0m [WARNING|2026-02-08 09:34:15] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "\n",
      "Training completed after 8 iterations at 2026-02-08 09:34:16. Total running time: 3min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 09:34:16,825\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/qwen2.5_7b_qlora_dpo' in 0.0941s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "USE_RAY=1 llamafactory-cli train ../train-configs/dpo_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run as an Anyscale job (production)\n",
    "\n",
    "For longer or production runs, submit the training as an **Anyscale job**. Jobs run outside your interactive session for better stability, retries, and durable logs. Package LLaMA-Factory and other libraries in a container image and launch with a short job config. See [Run LLaMA-Factory as an Anyscale job](https://docs.anyscale.com/llm/fine-tuning/llamafactory-jobs) for the step-by-step guide.\n",
    "\n",
    "### Tracking with TensorBoard\n",
    "If you enabled TensorBoard logging (`report_to: tensorboard` in your YAML), you can watch metrics (for example, training loss) update live and compare multiple runs with the same run name side-by-side.\n",
    "\n",
    "- **While the job is running:** LLaMA-Factory prints a ready-to-run command that starts with `tensorboard --logdir`. Open a new terminal and run it. For example:\n",
    "  ```bash\n",
    "  tensorboard --logdir /tmp/ray/session_*/artifacts/*/qwen2.5_7b_qlora_dpo/driver_artifacts\n",
    "  ```\n",
    "\n",
    "- **After the job:** Point TensorBoard at `{ray_storage_path}/{ray_run_name}/`. Each `TorchTrainer_*` subfolder holds event files for a single run. Using the parent folder aggregates all runs for easy comparison.\n",
    "  ```bash\n",
    "  tensorboard --logdir /mnt/cluster_storage/qwen2.5_7b_qlora_dpo\n",
    "  ```\n",
    "\n",
    "In your Anyscale workspace, look for the open **port 6006** labeled **TensorBoard** to view the dashboards.\n",
    "\n",
    "![Anyscale workspace showing open ports with TensorBoard on port 6006](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/open-ports.png)\n",
    "\n",
    "**TensorBoard example**\n",
    "\n",
    "![TensorBoard](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/3.2.2/3.2.2-tensorboard.png)\n",
    "\n",
    "For a more detailed guide on tracking experiments with other tools such as Weights & Biases or MLflow, see [Observability and tracking](https://docs.anyscale.com/llm/fine-tuning/observability-and-tracking).\n",
    "\n",
    "## Step 5: Locate checkpoints\n",
    "\n",
    "Ray Train writes checkpoints under `ray_storage_path/ray_run_name`. In this example run, the path is: `/mnt/cluster_storage/qwen2.5_7b_qlora_dpo`. \n",
    "\n",
    "Inside, you see a **trainer session** directory named like:\n",
    "`TorchTrainer_ff224_00000_0_2025-09-19_15-57-20/`.\n",
    "\n",
    "- Ray Train creates `TorchTrainer_*` **when the trainer starts**; the suffix encodes a short run ID and the **start timestamp**.\n",
    "- Within that directory, Ray Train names checkpoints `checkpoint_000xxx/`, where the number is the saved ordered checkpoints.\n",
    "\n",
    "Control the save cadence with `save_strategy` and `save_steps`. For instructions on how to resume interrupted training with `resume_from_checkpoint` and more, see [Understand the artifacts directory](https://docs.anyscale.com/llm/fine-tuning/checkpointing#artifacts-directory).\n",
    "\n",
    "## Step 6: Export the model\n",
    "\n",
    "If you use LoRA, you can keep the base model and adapters separate for [multi-LoRA deployment](https://docs.anyscale.com/llm/serving/multi-lora) or [merge the adapters](https://docs.anyscale.com/llm/fine-tuning/checkpointing#merge-lora) into the base model for low-latency inference. \n",
    "\n",
    "For full fine-tuning or freeze-tuning, export the fine-tuned model directly.\n",
    "\n",
    "You may optionally apply [post-training quantization](https://docs.anyscale.com/llm/fine-tuning/checkpointing#ptq) on merged or full models before serving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
