{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) at scale with DeepSpeed\n",
    "\n",
    "This guide provides a step-by-step workflow for supervised fine-tuning the [`Qwen/Qwen2.5-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) model on a multi-GPU Anyscale cluster. You use LLaMA-Factory for the training framework and `DeepSpeed` to efficiently manage memory and scale the training process.\n",
    "\n",
    "SFT is a technique to adapt a pre-trained model to specific tasks. By showing the model high-quality examples of instructions and their desired outputs, you teach it to follow new instructions more accurately.\n",
    "\n",
    "## Step 1: Set up your environment\n",
    "\n",
    "### Dependencies\n",
    "First, ensure your environment has the correct libraries. Start with a pre-built container image and install LLaMA-Factory and DeepSpeed on top of it.\n",
    "\n",
    "Recommended container image:\n",
    "```bash\n",
    "anyscale/ray-llm:2.48.0-py311-cu128\n",
    "```\n",
    "\n",
    "Execute the following commands to install the required packages and optional tools for experiment tracking and faster model downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.12.1 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `llamafactory` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `deepspeed` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `wandb` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `hf_transfer` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the specific version of LLaMA-Factory\n",
    "pip install -q llamafactory==0.9.3\n",
    "\n",
    "# Install DeepSpeed for large-scale training\n",
    "pip install -q deepspeed==0.16.9\n",
    "\n",
    "# (Optional) For experiment tracking with Weights & Biases\n",
    "pip install -q wandb==0.21.3\n",
    "\n",
    "# (Optional) For accelerated model downloads from Hugging Face\n",
    "pip install -q hf_transfer==0.1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and compute resources\n",
    "\n",
    "DeepSpeed ZeRO-3 partitions parameters, gradients, and optimizer states across multiple GPUs, enabling supervised fine-tuning (SFT) of 30B+ LLMs on just 4 GPUs.\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Base model** | [`Qwen/Qwen2.5-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) |\n",
    "| **Worker Nodes** | 4 × L40S / 4 x A100-40G |\n",
    "\n",
    "## Step 2: Prepare the dataset\n",
    "\n",
    "### Understand the dataset\n",
    "This tutorial uses [`glaive_toolcall_en_demo`](https://huggingface.co/datasets/zuol/glaive_toolcall_en_demo/tree/main), a dataset designed to teach models how to use tools (also known as function calling).\n",
    "\n",
    "This dataset contains conversational examples where the model needs to interact with external tools. Each entry includes:\n",
    "* `conversations`: A turn-by-turn log between a human and the gpt assistant.\n",
    "* `tools`: A JSON schema describing the functions the model can call.\n",
    "\n",
    "**Note**: The `conversations` may include special turns like function_call (the model deciding to call a tool) and observation (the result returned from the tool). This structure is ideal for teaching the model sophisticated tool-use behavior. To maintain role alignment in ShareGPT format, you must follow a strict turn order: `human` and `observation` (tool output) must appear in odd-numbered positions, while `gpt` and `function_call` must appear in even-numbered positions. The model learns to generate the content in the `gpt` and `function_call` turns.\n",
    "\n",
    "**Dataset example**\n",
    "\n",
    "```json\n",
    "{\n",
    "\"conversations\": [\n",
    "    {\n",
    "    \"from\": \"human\",\n",
    "    \"value\": \"Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"gpt\",\n",
    "    \"value\": \"Of course! I can help you with that. Please tell me what ingredients you have.\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"human\",\n",
    "    \"value\": \"I have chicken, bell peppers, and rice.\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"function_call\",\n",
    "    \"value\": \"{\\\"name\\\": \\\"search_recipes\\\", \\\"arguments\\\": {\\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"]}}\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"observation\",\n",
    "    \"value\": \"{\\\"recipes\\\": [{\\\"name\\\": \\\"Chicken and Bell Pepper Stir Fry\\\", \\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"], \\\"instructions\\\": \\\"Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\\\"}, {\\\"name\\\": \\\"Chicken and Rice Casserole\\\", \\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"], \\\"instructions\\\": \\\"Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\\\"}]}\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"gpt\",\n",
    "    \"value\": \"I found two recipes for you. The first one is \\\"Chicken and Bell Pepper Stir Fry\\\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \\\"Chicken and Rice Casserole\\\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"human\",\n",
    "    \"value\": \"They both sound delicious, but I think I'll try the stir fry. Can you order the ingredients for me?\"\n",
    "    },\n",
    "    {\n",
    "    \"from\": \"gpt\",\n",
    "    \"value\": \"I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.\"\n",
    "    }\n",
    "],\n",
    "\"tools\": \"[{\\\"name\\\": \\\"search_recipes\\\", \\\"description\\\": \\\"Search for recipes based on ingredients\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"ingredients\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}, \\\"description\\\": \\\"The ingredients to search for\\\"}}, \\\"required\\\": [\\\"ingredients\\\"]}}]\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Register the dataset\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add a **`dataset_info.json`** to **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)** such as `/mnt/cluster_storage`. This configuration file acts as a central registry for all your datasets. It maps a custom name to your dataset file location, format, and column structure. \n",
    "\n",
    "If you plan to run SFT fine-tuning on the `glaive_toolcall_en_demo` dataset, first complete the setup steps below. Ensure that you place the dataset files in a storage location that all workers can access (for example, a shared mount or object storage). Avoid storing large files on the head node.\n",
    "\n",
    "`dataset_info.json`\n",
    "```json\n",
    "{\n",
    "  \"my_glaive_toolcall_en_demo\": {\n",
    "      \"file_name\": \"/mnt/cluster_storage/glaive_toolcall_en_demo.json\",\n",
    "      \"formatting\": \"sharegpt\",\n",
    "      \"columns\": {\n",
    "          \"messages\": \"conversations\",\n",
    "          \"tools\": \"tools\"\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed dataset preparation and formatting guide, see [Choose your data format](https://docs.anyscale.com/llm/fine-tuning/data-preparation#sft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2026-02-08 08:44:10--  https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/glaive_toolcall_en_demo.json\n",
      "Resolving anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)... 3.5.85.37, 52.92.130.162, 52.92.177.58, ...\n",
      "Connecting to anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)|3.5.85.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 738925 (722K) [application/json]\n",
      "Saving to: ‘/mnt/cluster_storage/glaive_toolcall_en_demo.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  6%  325K 2s\n",
      "    50K .......... .......... .......... .......... .......... 13%  325K 2s\n",
      "   100K .......... .......... .......... .......... .......... 20%  380K 2s\n",
      "   150K .......... .......... .......... .......... .......... 27% 2.19M 1s\n",
      "   200K .......... .......... .......... .......... .......... 34%  123M 1s\n",
      "   250K .......... .......... .......... .......... .......... 41% 94.3M 1s\n",
      "   300K .......... .......... .......... .......... .......... 48% 1.17M 1s\n",
      "   350K .......... .......... .......... .......... .......... 55%  450K 0s\n",
      "   400K .......... .......... .......... .......... .......... 62%  232M 0s\n",
      "   450K .......... .......... .......... .......... .......... 69%  254M 0s\n",
      "   500K .......... .......... .......... .......... .......... 76%  218M 0s\n",
      "   550K .......... .......... .......... .......... .......... 83%  162M 0s\n",
      "   600K .......... .......... .......... .......... .......... 90%  200M 0s\n",
      "   650K .......... .......... .......... .......... .......... 97% 1.18M 0s\n",
      "   700K .......... .......... .                               100%  372M=0.7s\n",
      "\n",
      "2026-02-08 08:44:11 (1.07 MB/s) - ‘/mnt/cluster_storage/glaive_toolcall_en_demo.json’ saved [738925/738925]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Make sure all files are accessible to worker nodes\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "wget https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/glaive_toolcall_en_demo.json -O /mnt/cluster_storage/glaive_toolcall_en_demo.json\n",
    "# Create a copy of the dataset registry in /mnt/cluster_storage\n",
    "cp ../dataset-configs/dataset_info.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the fine-tuning config (SFT with DeepSpeed)\n",
    "\n",
    "Next, create the main YAML configuration file—the master recipe for your fine-tuning job. It specifies the base model, the fine-tuning method (LoRA), the dataset, training hyperparameters, cluster resources, and more.\n",
    "\n",
    "**Important notes:**\n",
    "- **W&B tracking:** To track experiments with Weights & Biases (W&B), set `report_to: wandb` in the config and provide `WANDB_API_KEY` in the runtime environment. If you don't want to use W&B, set `report_to: none` to avoid errors.\n",
    "- **Access and paths:** The YAML only needs to be on the **head node**, but any referenced paths (`dataset_dir`, `output_dir`) must reside on storage **reachable by all workers** (for example, `/mnt/cluster_storage/`).\n",
    "- **Gated models:** If your base model has gated access (for example, Llama) on Hugging Face, set `HF_TOKEN` in the runtime environment.\n",
    "- **GPU selection and placement:** The config uses a 4xL40S node (`ananyscale/accelerator_shape:4xL40S`) so that all 4 GPUs are on the same machine, which is important for efficient DeepSpeed ZeRO-3 communication. You can switch to other multi-GPU nodes such as `4xA100-40GB` or any other node type with comparable or more VRAM, depending on your cloud availability.\n",
    "\n",
    "### Configure LLaMA-Factory with Ray\n",
    "\n",
    "**Note**: To customize the training configuration, edit `train-configs/sft_lora_deepspeed.yaml`. \n",
    "\n",
    "```yaml\n",
    "# sft_lora_deepspeed.yaml\n",
    "\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen2.5-32B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "\n",
    "### deepspeed\n",
    "deepspeed: /mnt/cluster_storage/ds_z3_config.json # path to the DeepSpeed config\n",
    "\n",
    "### dataset\n",
    "dataset: my_glaive_toolcall_en_demo\n",
    "dataset_dir: /mnt/cluster_storage\n",
    "\n",
    "template: qwen\n",
    "cutoff_len: 1024\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: qwen2.5_32b_lora_sft\n",
    "logging_steps: 5\n",
    "save_steps: 50\n",
    "plot_loss: true\n",
    "report_to: wandb # or none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1 # Adjust this depending on your GPU memory and sequence length\n",
    "gradient_accumulation_steps: 4\n",
    "num_train_epochs: 3.0\n",
    "learning_rate: 1.0e-4\n",
    "bf16: true\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### ray\n",
    "ray_run_name: qwen2.5_32b_lora_sft\n",
    "ray_storage_path: /mnt/cluster_storage/\n",
    "ray_num_workers: 4  # Number of GPUs to use\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  # accelerator_type:L40S: 0.001            # Use this to simply specify a GPU type (may place GPUs on separate nodes).\n",
    "  anyscale/accelerator_shape:4xL40S: 0.001  # Prefer this for DeepSpeed so all 4 GPUs are on the same node.\n",
    "  # See https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types.\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      # If using wandb for experiments tracking\n",
    "      WANDB_API_KEY: <your_wandb_token>\n",
    "      # If using gated models like meta-llama/Llama-3.1-8B-Instruct\n",
    "      # HF_TOKEN: <your_huggingface_token>\n",
    "      # If hf_transfer is installed\n",
    "      HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "```\n",
    "\n",
    "**Note:**\n",
    "This configuration assumes `4xL40S` GPUs are available in your cloud environment. If not, you can substitute with `4xA100-40G` (or another supported accelerator with similar VRAM).\n",
    "\n",
    "### DeepSpeed configuration\n",
    "DeepSpeed is an open-source deep-learning optimization library developed by Microsoft, aimed at enabling large-model training. Higher ZeRO stages (1→3) and enabling CPU offload reduce GPU VRAM usage, but might cause slower training.\n",
    "\n",
    "To enable DeepSpeed, create a separate Deepspeed config in the **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)**. and reference it from your main training yaml config with:\n",
    "\n",
    "```yaml\n",
    "deepspeed: /mnt/cluster_storage/ds_z3_config.json\n",
    "```\n",
    "\n",
    "Below is a sample ZeRO-3 config:\n",
    "\n",
    "`ds_z3_config.json`\n",
    "```json\n",
    "{\n",
    "\"train_batch_size\": \"auto\",\n",
    "\"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "\"gradient_accumulation_steps\": \"auto\",\n",
    "\"gradient_clipping\": \"auto\",\n",
    "\"zero_allow_untested_optimizer\": true,\n",
    "\"fp16\": {\n",
    "    \"enabled\": \"auto\",\n",
    "    \"loss_scale\": 0,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "},\n",
    "\"bf16\": {\n",
    "    \"enabled\": \"auto\"\n",
    "},\n",
    "\"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": false,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "}\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed guide on acceleration and optimization methods including DeepSpeed on Ray, see [Speed and memory optimizations](https://docs.anyscale.com/llm/fine-tuning/speed-and-memory-optimizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a copy of the DeepSpeed configuration file in /mnt/cluster_storage\n",
    "cp ../deepspeed-configs/ds_z3_config.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and monitor\n",
    "\n",
    "**Note**: If you installed Weights & Biases, set `WANDB_API_KEY` in the runtime environment. Otherwise, set `report_to: none` in `sft_lora_deepspeed.yaml` to avoid `api_token not set` errors.\n",
    "\n",
    "With all configurations in place, you can launch fine-tuning or post-training in one of two ways:\n",
    "\n",
    "### Option A: Run from a workspace (quick start)\n",
    "\n",
    "The `USE_RAY=1` prefix tells LLaMA-Factory to run in distributed mode on the Ray cluster attached to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-08 08:51:32,783] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2026-02-08 08:51:32,783] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "INFO 02-08 08:51:34 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 02-08 08:51:34 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 08:51:37,024\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.128.5.218:6379...\n",
      "2026-02-08 08:51:37,035\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-c1mvc6t862zj4fbguuknngnrgv.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-08 08:51:37,037\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_1598f5ff1c5690e55af864dcf6517685e149d19c.zip' (0.07MiB) to Ray cluster...\n",
      "2026-02-08 08:51:37,037\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_1598f5ff1c5690e55af864dcf6517685e149d19c.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/qwen2.5_32b_lora_sft\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2026-02-08_07-42-51_475236_185/artifacts/2026-02-08_08-51-37/qwen2.5_32b_lora_sft/driver_artifacts`\n",
      "\u001b[36m(TrainTrainable pid=2869, ip=10.128.7.103)\u001b[0m [2026-02-08 08:51:45,441] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "\u001b[36m(TrainTrainable pid=2869, ip=10.128.7.103)\u001b[0m [2026-02-08 08:51:45,442] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                             True │\n",
      "│ train_loop_config/args/cutoff_len                                                                       1024 │\n",
      "│ train_loop_config/args/dataset                                                          ..._toolcall_en_demo │\n",
      "│ train_loop_config/args/dataset_dir                                                      /mnt/cluster_storage │\n",
      "│ train_loop_config/args/ddp_timeout                                                                 180000000 │\n",
      "│ train_loop_config/args/deepspeed                                                        ...ds_z3_config.json │\n",
      "│ train_loop_config/args/do_train                                                                         True │\n",
      "│ train_loop_config/args/finetuning_type                                                                  lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                         4 │\n",
      "│ train_loop_config/args/learning_rate                                                                  0.0001 │\n",
      "│ train_loop_config/args/logging_steps                                                                       5 │\n",
      "│ train_loop_config/args/lora_rank                                                                           8 │\n",
      "│ train_loop_config/args/lora_target                                                                       all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                              cosine │\n",
      "│ train_loop_config/args/max_samples                                                                      1000 │\n",
      "│ train_loop_config/args/model_name_or_path                                               ...n2.5-32B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                                  3.0 │\n",
      "│ train_loop_config/args/output_dir                                                       qwen2.5_32b_lora_sft │\n",
      "│ train_loop_config/args/overwrite_cache                                                                  True │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                         1 │\n",
      "│ train_loop_config/args/plot_loss                                                                        True │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                          16 │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_HUB_ENABLE_HF_TRANSFER                      1 │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/WANDB_API_KEY                 <your_wandb_token> │\n",
      "│ train_loop_config/args/ray_num_workers                                                                     4 │\n",
      "│ train_loop_config/args/ray_run_name                                                     qwen2.5_32b_lora_sft │\n",
      "│ train_loop_config/args/ray_storage_path                                                 .../cluster_storage/ │\n",
      "│ train_loop_config/args/report_to                                                                        none │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                            1 │\n",
      "│ train_loop_config/args/save_steps                                                                         50 │\n",
      "│ train_loop_config/args/stage                                                                             sft │\n",
      "│ train_loop_config/args/template                                                                         qwen │\n",
      "│ train_loop_config/args/trust_remote_code                                                                True │\n",
      "│ train_loop_config/args/warmup_ratio                                                                      0.1 │\n",
      "│ train_loop_config/callbacks                                                             ... 0x7f56ec40b2d0>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=2869, ip=10.128.7.103)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=2869, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=3001) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=2869, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=3002) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=2869, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=3003) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=2869, ip=10.128.7.103)\u001b[0m - (node_id=c0a063c3a2d8319e8a19333c768480af090e91c460068963ebf9fb27, ip=10.128.7.103, pid=3000) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:51:56,572] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:00,532] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:00] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:00] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:01,201 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 08:52:01,487 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 08:52:02,102 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 08:52:02,104 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 5120,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 27648,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 70,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 40,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 64,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,306 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 08:52:02,307 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 08:52:02,567 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:02] llamafactory.data.loader:143 >> Loading dataset glaive_toolcall_en_demo.json...\n",
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m [2026-02-08 08:51:56,931] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3003, ip=10.128.7.103)\u001b[0m [rank2]:[W208 08:52:02.339028791 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/300 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  88%|████████▊ | 264/300 [00:00<00:00, 2638.98 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 300/300 [00:00<00:00, 1549.14 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/300 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 19/300 [00:00<00:08, 31.32 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|█▉        | 57/300 [00:00<00:02, 85.98 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 95/300 [00:00<00:01, 125.16 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  51%|█████     | 152/300 [00:01<00:00, 182.17 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 190/300 [00:01<00:00, 196.47 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 228/300 [00:01<00:00, 204.51 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 264/300 [00:01<00:00, 204.59 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 300/300 [00:01<00:00, 159.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 382, 2, 13852, 271, 2610, 1231, 1618, 825, 476, 803, 5746, 311, 7789, 448, 279, 1196, 3239, 382, 2610, 525, 3897, 448, 729, 32628, 2878, 366, 15918, 1472, 15918, 29, 11874, 9492, 510, 27, 15918, 397, 4913, 1313, 788, 330, 1688, 497, 330, 1688, 788, 5212, 606, 788, 330, 1836, 7080, 8923, 497, 330, 4684, 788, 330, 5890, 369, 18627, 3118, 389, 13966, 497, 330, 13786, 788, 5212, 1313, 788, 330, 1700, 497, 330, 13193, 788, 5212, 38120, 788, 5212, 1313, 788, 330, 1653, 497, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 4684, 788, 330, 785, 13966, 311, 2711, 369, 9207, 2137, 330, 6279, 788, 4383, 38120, 1341, 3417, 532, 522, 15918, 1339, 2461, 1817, 729, 1618, 11, 470, 264, 2951, 1633, 448, 729, 829, 323, 5977, 2878, 220, 151657, 151658, 11874, 9492, 510, 151657, 198, 4913, 606, 788, 366, 1688, 11494, 8066, 330, 16370, 788, 366, 2116, 56080, 40432, 31296, 151658, 151645, 198, 151644, 872, 198, 13048, 11, 358, 614, 1045, 13966, 323, 358, 1366, 311, 4296, 2494, 13, 2980, 498, 1492, 752, 1477, 264, 11116, 30, 151645, 198, 151644, 77091, 198, 2124, 3308, 0, 358, 646, 1492, 498, 448, 429, 13, 5209, 3291, 752, 1128, 13966, 498, 614, 13, 151645, 198, 151644, 872, 198, 40, 614, 16158, 11, 28419, 57473, 11, 323, 19653, 13, 151645, 198, 151644, 77091, 198, 151657, 198, 4913, 606, 788, 330, 1836, 7080, 8923, 497, 330, 16370, 788, 5212, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 1341, 11248, 151658, 151645, 198, 151644, 872, 198, 27, 14172, 9655, 397, 4913, 49859, 788, 61753, 606, 788, 330, 83863, 323, 17884, 51782, 64192, 52417, 497, 330, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 7914, 330, 62295, 788, 330, 37666, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 1189, 2137, 5212, 606, 788, 330, 83863, 323, 29516, 25442, 261, 1263, 497, 330, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 7914, 330, 62295, 788, 330, 58382, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 1189, 92, 23439, 522, 14172, 9655, 29, 151645, 198, 151644, 77091, 198, 40, 1730, 1378, 18627, 369, 498, 13, 576, 1156, 825, 374, 330, 83863, 323, 17884, 51782, 64192, 52417, 3263, 576, 11221, 525, 25, 19649, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 13, 576, 2086, 825, 374, 330, 83863, 323, 29516, 25442, 261, 1263, 3263, 576, 11221, 525, 25, 12514, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 13, 15920, 825, 1035, 498, 1075, 311, 1430, 30, 151645, 198, 151644, 872, 198, 6865, 2176, 5112, 17923, 11, 714, 358, 1744, 358, 3278, 1430, 279, 23065, 52546, 13, 2980, 498, 1973, 279, 13966, 369, 752, 30, 151645, 198, 151644, 77091, 198, 40, 2776, 14589, 11, 714, 438, 458, 15235, 11, 358, 1513, 944, 614, 279, 22302, 311, 2736, 9250, 9079, 1741, 438, 21391, 13966, 13, 4354, 11, 358, 646, 1492, 498, 1477, 803, 18627, 476, 3410, 17233, 10414, 421, 498, 1184, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m # Tools\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m You may call one or more functions to assist with the user query.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m You are provided with function signatures within <tools></tools> XML tags:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <tools>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {\"type\": \"function\", \"function\": {\"name\": \"search_recipes\", \"description\": \"Search for recipes based on ingredients\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ingredients\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The ingredients to search for\"}}, \"required\": [\"ingredients\"]}}}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m </tools>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Of course! I can help you with that. Please tell me what ingredients you have.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m I have chicken, bell peppers, and rice.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {\"name\": \"search_recipes\", \"arguments\": {\"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"]}}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <tool_response>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {\"recipes\": [{\"name\": \"Chicken and Bell Pepper Stir Fry\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\"}, {\"name\": \"Chicken and Rice Casserole\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\"}]}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m </tool_response><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m I found two recipes for you. The first one is \"Chicken and Bell Pepper Stir Fry\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \"Chicken and Rice Casserole\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m They both sound delicious, but I think I'll try the stir fry. Can you order the ingredients for me?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2124, 3308, 0, 358, 646, 1492, 498, 448, 429, 13, 5209, 3291, 752, 1128, 13966, 498, 614, 13, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151657, 198, 4913, 606, 788, 330, 1836, 7080, 8923, 497, 330, 16370, 788, 5212, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 1341, 11248, 151658, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 1730, 1378, 18627, 369, 498, 13, 576, 1156, 825, 374, 330, 83863, 323, 17884, 51782, 64192, 52417, 3263, 576, 11221, 525, 25, 19649, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 13, 576, 2086, 825, 374, 330, 83863, 323, 29516, 25442, 261, 1263, 3263, 576, 11221, 525, 25, 12514, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 13, 15920, 825, 1035, 498, 1075, 311, 1430, 30, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 2776, 14589, 11, 714, 438, 458, 15235, 11, 358, 1513, 944, 614, 279, 22302, 311, 2736, 9250, 9079, 1741, 438, 21391, 13966, 13, 4354, 11, 358, 646, 1492, 498, 1477, 803, 18627, 476, 3410, 17233, 10414, 421, 498, 1184, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Of course! I can help you with that. Please tell me what ingredients you have.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {\"name\": \"search_recipes\", \"arguments\": {\"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"]}}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m I found two recipes for you. The first one is \"Chicken and Bell Pepper Stir Fry\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \"Chicken and Rice Casserole\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:00,993] [INFO] [comm.py:669:init_distributed] cdb=None\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:01] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 08:52:06,973 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 08:52:06,973 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 5120,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 27648,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 70,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 40,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 64,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:06] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:1151] 2026-02-08 08:52:07,236 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/model.safetensors.index.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:3881] 2026-02-08 08:52:07,238 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 08:52:07,248 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:07,238] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [rank0]:[W208 08:52:03.115556105 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:07,916] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:11,  1.40it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  29%|██▉       | 5/17 [00:05<00:14,  1.18s/it]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  59%|█████▉    | 10/17 [00:11<00:08,  1.18s/it]\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  82%|████████▏ | 14/17 [00:16<00:03,  1.22s/it]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  94%|█████████▍| 16/17 [00:18<00:01,  1.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:19<00:00,  1.13s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:5131] 2026-02-08 08:52:28,004 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|modeling_utils.py:5139] 2026-02-08 08:52:28,004 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-32B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1090] 2026-02-08 08:52:28,115 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 08:52:28,115 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:28] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:28] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:28] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:28] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:28] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,down_proj,v_proj,up_proj,q_proj,gate_proj,o_proj\n",
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:07,353] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|2026-02-08 08:52:29] llamafactory.model.loader:143 >> trainable params: 67,108,864 || all params: 32,830,985,216 || trainable%: 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:756] 2026-02-08 08:52:29,128 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,587] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,654] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,661] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,661] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,769] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,769] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,769] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,769] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,017] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,018] [INFO] [utils.py:782:see_memory_usage] MA 15.38 GB         Max_MA 19.25 GB         CA 15.94 GB         Max_CA 20 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,018] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.22 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,032] [INFO] [stage3.py:170:__init__] Reduce bucket size 26214400\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,032] [INFO] [stage3.py:171:__init__] Prefetch bucket size 23592960\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,280] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,281] [INFO] [utils.py:782:see_memory_usage] MA 15.38 GB         Max_MA 15.38 GB         CA 15.94 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:30,281] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.24 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Parameter Offload: Total persistent parameters: 25760768 in 1025 params\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,025] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,026] [INFO] [utils.py:782:see_memory_usage] MA 15.29 GB         Max_MA 15.38 GB         CA 15.94 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,026] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.37 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,399] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,400] [INFO] [utils.py:782:see_memory_usage] MA 15.29 GB         Max_MA 15.29 GB         CA 15.94 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:32,400] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.36 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,288] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,290] [INFO] [utils.py:782:see_memory_usage] MA 15.29 GB         Max_MA 15.29 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,290] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.42 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:29,587] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,689] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,689] [INFO] [utils.py:782:see_memory_usage] MA 15.29 GB         Max_MA 15.29 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:33,690] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.42 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,078] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,079] [INFO] [utils.py:782:see_memory_usage] MA 15.35 GB         Max_MA 15.38 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,079] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.43 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,483] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,483] [INFO] [utils.py:782:see_memory_usage] MA 15.35 GB         Max_MA 15.35 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,483] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.42 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,887] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,888] [INFO] [utils.py:782:see_memory_usage] MA 15.35 GB         Max_MA 15.41 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,888] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.42 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:34,889] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2409] 2026-02-08 08:52:35,664 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2410] 2026-02-08 08:52:35,664 >>   Num examples = 300\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2411] 2026-02-08 08:52:35,664 >>   Num Epochs = 3\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2412] 2026-02-08 08:52:35,664 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2415] 2026-02-08 08:52:35,664 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2416] 2026-02-08 08:52:35,664 >>   Gradient Accumulation steps = 4\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2417] 2026-02-08 08:52:35,664 >>   Total optimization steps = 57\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2418] 2026-02-08 08:52:35,672 >>   Number of trainable parameters = 67,108,864\n",
      "  0%|          | 0/57 [00:00<?, ?it/s]28.7.103)\u001b[0m \n",
      "Loading checkpoint shards:  88%|████████▊ | 15/17 [00:17<00:02,  1.21s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  94%|█████████▍| 16/17 [00:19<00:01,  1.20s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:20<00:00,  1.18s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,647] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,647] [INFO] [utils.py:782:see_memory_usage] MA 15.43 GB         Max_MA 15.43 GB         CA 15.69 GB         Max_CA 16 GB \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,648] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.64 GB, percent = 1.6%\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,648] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,648] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,648] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,648] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,659] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"partition_activations\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"contiguous_memory_optimization\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"cpu_checkpointing\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"number_checkpoints\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"synchronize_checkpoint_boundary\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"profile\": false\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"start_step\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"end_step\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"metric_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"arg_mappings\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"metric\": \"throughput\", \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"model_info\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"results_dir\": \"autotuning_results\", \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"exps_dir\": \"autotuning_exps\", \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"overwrite\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"fast\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"start_profile_step\": 3, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"end_profile_step\": 5, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"tuner_type\": \"gridsearch\", \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"tuner_early_stopping\": 5, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"tuner_num_trials\": 50, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"model_info_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"mp_size\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"max_train_batch_size\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"min_train_batch_size\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"num_tuning_micro_batch_sizes\": 3\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2285130910>\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,660] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"recompute_fwd_factor\": 0.0, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"profile_step\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"module_depth\": -1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"top_modules\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"detailed\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"output_file\": null\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 4\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"persistent_storage_path\": null, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"persistent_time_interval\": 100, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"num_of_version_in_retention\": 2, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"enable_nebula_load\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"load_path\": null\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,661] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   train_batch_size ............. 16\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   world_size ................... 4\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 08:52:35,662] [INFO] [config.py:993:print_user_config]   json = {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"train_batch_size\": 16, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"train_micro_batch_size_per_gpu\": 1, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"gradient_accumulation_steps\": 4, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"gradient_clipping\": 1.0, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"zero_allow_untested_optimizer\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"fp16\": {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"enabled\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"loss_scale\": 0, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"loss_scale_window\": 1000, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"initial_scale_power\": 16, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"hysteresis\": 2, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"min_loss_scale\": 1\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"bf16\": {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"enabled\": true\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"zero_optimization\": {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage\": 3, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"overlap_comm\": false, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"contiguous_gradients\": true, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"sub_group_size\": 1.000000e+09, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"reduce_bucket_size\": 2.621440e+07, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage3_prefetch_bucket_size\": 2.359296e+07, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage3_param_persistence_threshold\": 5.120000e+04, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m         \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     }, \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"steps_per_print\": inf\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/57 [00:16<15:43, 16.84s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m25s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/57 [00:26<11:40, 12.73s/it])\u001b[0m \n",
      "  5%|▌         | 3/57 [00:36<10:15, 11.40s/it])\u001b[0m \n",
      "  7%|▋         | 4/57 [00:46<09:31, 10.77s/it])\u001b[0m \n",
      "  9%|▉         | 5/57 [00:56<09:01, 10.41s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.5419, 'grad_norm': 0.29426564591950005, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/57 [01:05<08:40, 10.21s/it])\u001b[0m \n",
      " 12%|█▏        | 7/57 [01:15<08:23, 10.08s/it])\u001b[0m \n",
      " 14%|█▍        | 8/57 [01:25<08:09,  9.98s/it])\u001b[0m \n",
      " 16%|█▌        | 9/57 [01:35<07:56,  9.92s/it])\u001b[0m \n",
      " 18%|█▊        | 10/57 [01:45<07:43,  9.87s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.5142, 'grad_norm': 0.1509424780319976, 'learning_rate': 9.91486549841951e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11/57 [01:54<07:32,  9.84s/it]\u001b[0m \n",
      " 21%|██        | 12/57 [02:04<07:21,  9.81s/it]\u001b[0m \n",
      " 23%|██▎       | 13/57 [02:14<07:11,  9.80s/it]\u001b[0m \n",
      " 25%|██▍       | 14/57 [02:24<07:00,  9.78s/it]\u001b[0m \n",
      " 26%|██▋       | 15/57 [02:33<06:50,  9.78s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.4473, 'grad_norm': 0.12575272194787512, 'learning_rate': 9.405060971428923e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 16/57 [02:43<06:40,  9.77s/it]\u001b[0m \n",
      " 30%|██▉       | 17/57 [02:53<06:30,  9.77s/it]\u001b[0m \n",
      " 32%|███▏      | 18/57 [03:03<06:20,  9.76s/it]\u001b[0m \n",
      " 33%|███▎      | 19/57 [03:10<05:41,  8.98s/it]\u001b[0m \n",
      " 35%|███▌      | 20/57 [03:19<05:40,  9.20s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3856, 'grad_norm': 0.12078352261151609, 'learning_rate': 8.480669729814635e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 21/57 [03:29<05:37,  9.37s/it]\u001b[0m \n",
      " 39%|███▊      | 22/57 [03:39<05:31,  9.48s/it]\u001b[0m \n",
      " 40%|████      | 23/57 [03:49<05:24,  9.55s/it]\u001b[0m \n",
      " 42%|████▏     | 24/57 [03:58<05:16,  9.61s/it]\u001b[0m \n",
      " 44%|████▍     | 25/57 [04:08<05:08,  9.65s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3079, 'grad_norm': 0.1151843729189665, 'learning_rate': 7.228691778882693e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 26/57 [04:18<05:00,  9.68s/it]\u001b[0m \n",
      " 47%|████▋     | 27/57 [04:28<04:50,  9.70s/it]\u001b[0m \n",
      " 49%|████▉     | 28/57 [04:37<04:41,  9.71s/it]\u001b[0m \n",
      " 51%|█████     | 29/57 [04:47<04:31,  9.71s/it]\u001b[0m \n",
      " 53%|█████▎    | 30/57 [04:57<04:22,  9.71s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.2882, 'grad_norm': 0.08515978682873142, 'learning_rate': 5.7669582743934284e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 31/57 [05:07<04:12,  9.72s/it]\u001b[0m \n",
      " 56%|█████▌    | 32/57 [05:16<04:03,  9.73s/it]\u001b[0m \n",
      " 58%|█████▊    | 33/57 [05:26<03:53,  9.73s/it]\u001b[0m \n",
      " 60%|█████▉    | 34/57 [05:36<03:43,  9.72s/it]\u001b[0m \n",
      " 61%|██████▏   | 35/57 [05:45<03:33,  9.72s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3361, 'grad_norm': 0.06301017189365173, 'learning_rate': 4.233041725606572e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 36/57 [05:55<03:24,  9.73s/it]\u001b[0m \n",
      " 65%|██████▍   | 37/57 [06:05<03:14,  9.73s/it]\u001b[0m \n",
      " 67%|██████▋   | 38/57 [06:12<02:51,  9.01s/it]\u001b[0m \n",
      " 68%|██████▊   | 39/57 [06:22<02:45,  9.22s/it]\u001b[0m \n",
      " 70%|███████   | 40/57 [06:32<02:39,  9.36s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3296, 'grad_norm': 0.12993469598011465, 'learning_rate': 2.771308221117309e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 41/57 [06:41<02:31,  9.47s/it]\u001b[0m \n",
      " 74%|███████▎  | 42/57 [06:51<02:23,  9.54s/it]\u001b[0m \n",
      " 75%|███████▌  | 43/57 [07:01<02:14,  9.60s/it]\u001b[0m \n",
      " 77%|███████▋  | 44/57 [07:11<02:05,  9.64s/it]\u001b[0m \n",
      " 79%|███████▉  | 45/57 [07:20<01:55,  9.66s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3066, 'grad_norm': 0.06914665181850518, 'learning_rate': 1.5193302701853673e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 46/57 [07:30<01:46,  9.67s/it]\u001b[0m \n",
      " 82%|████████▏ | 47/57 [07:40<01:36,  9.68s/it]\u001b[0m \n",
      " 84%|████████▍ | 48/57 [07:49<01:27,  9.69s/it]\u001b[0m \n",
      " 86%|████████▌ | 49/57 [07:59<01:17,  9.70s/it]\u001b[0m \n",
      " 88%|████████▊ | 50/57 [08:09<01:07,  9.70s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3301, 'grad_norm': 0.08821184709024278, 'learning_rate': 5.949390285710776e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:3993] 2026-02-08 09:01:01,146 >> Saving model checkpoint to qwen2.5_32b_lora_sft/checkpoint-50\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:01:01,428 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:01:01,429 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 5120,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 27648,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 70,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 40,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 64,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:01:01,755 >> chat template saved in qwen2.5_32b_lora_sft/checkpoint-50/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:01:01,756 >> tokenizer config file saved in qwen2.5_32b_lora_sft/checkpoint-50/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:01:01,756 >> Special tokens file saved in qwen2.5_32b_lora_sft/checkpoint-50/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,035] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step49 is about to be saved!\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,085] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: qwen2.5_32b_lora_sft/checkpoint-50/global_step49/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving qwen2.5_32b_lora_sft/checkpoint-50/global_step49/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved qwen2.5_32b_lora_sft/checkpoint-50/global_step49/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving qwen2.5_32b_lora_sft/checkpoint-50/global_step49/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved qwen2.5_32b_lora_sft/checkpoint-50/global_step49/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,560] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved qwen2.5_32b_lora_sft/checkpoint-50/global_step49/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:01:02,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2026-02-08 09:01:09. Total running time: 9min 32s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              561.34709 │\n",
      "│ time_total_s                  561.34709 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                              2.64 │\n",
      "│ grad_norm                       0.08821 │\n",
      "│ learning_rate                   0.00001 │\n",
      "│ loss                             0.3301 │\n",
      "│ step                                 50 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 51/57 [08:43<01:42, 17.13s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 91%|█████████ | 52/57 [08:53<01:14, 14.93s/it]\u001b[0m \n",
      " 93%|█████████▎| 53/57 [09:03<00:53, 13.40s/it]\u001b[0m \n",
      " 95%|█████████▍| 54/57 [09:13<00:36, 12.33s/it]\u001b[0m \n",
      " 96%|█████████▋| 55/57 [09:23<00:23, 11.58s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'loss': 0.3133, 'grad_norm': 0.09929192734739711, 'learning_rate': 8.513450158049108e-07, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 56/57 [09:32<00:11, 11.06s/it]\u001b[0m \n",
      "100%|██████████| 57/57 [09:40<00:00,  9.96s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:3993] 2026-02-08 09:02:32,148 >> Saving model checkpoint to qwen2.5_32b_lora_sft/checkpoint-57\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:02:32,378 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:02:32,379 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 5120,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 27648,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 70,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 40,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 64,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:02:32,633 >> chat template saved in qwen2.5_32b_lora_sft/checkpoint-57/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:02:32,633 >> tokenizer config file saved in qwen2.5_32b_lora_sft/checkpoint-57/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:02:32,633 >> Special tokens file saved in qwen2.5_32b_lora_sft/checkpoint-57/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,064] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step56 is about to be saved!\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,112] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: qwen2.5_32b_lora_sft/checkpoint-57/global_step56/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving qwen2.5_32b_lora_sft/checkpoint-57/global_step56/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved qwen2.5_32b_lora_sft/checkpoint-57/global_step56/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving qwen2.5_32b_lora_sft/checkpoint-57/global_step56/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved qwen2.5_32b_lora_sft/checkpoint-57/global_step56/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,596] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved qwen2.5_32b_lora_sft/checkpoint-57/global_step56/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [2026-02-08 09:02:33,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step56 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3002, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:2676] 2026-02-08 09:02:38,612 >> \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 2 at 2026-02-08 09:02:38. Total running time: 11min 1s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000001 │\n",
      "│ time_this_iter_s                89.5918 │\n",
      "│ time_total_s                  650.93889 │\n",
      "│ training_iteration                    2 │\n",
      "│ epoch                           2.90667 │\n",
      "│ grad_norm                       0.09929 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.3133 │\n",
      "│ step                                 55 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 2 at: (local)/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [10:03<00:00, 10.59s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'train_runtime': 602.9399, 'train_samples_per_second': 1.493, 'train_steps_per_second': 0.095, 'train_loss': 0.36622098924820884, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|trainer.py:3993] 2026-02-08 09:02:55,205 >> Saving model checkpoint to qwen2.5_32b_lora_sft\n",
      "\u001b[36m(RayTrainWorker pid=3000, ip=10.128.7.103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_6ecdb_00000_0_2026-02-08_08-51-37/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 09:02:55,432 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 09:02:55,432 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"hidden_size\": 5120,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"intermediate_size\": 27648,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"max_window_layers\": 70,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_attention_heads\": 40,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_hidden_layers\": 64,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 09:02:55,681 >> chat template saved in qwen2.5_32b_lora_sft/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 09:02:55,681 >> tokenizer config file saved in qwen2.5_32b_lora_sft/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 09:02:55,681 >> Special tokens file saved in qwen2.5_32b_lora_sft/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   epoch                    =        3.0\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   total_flos               =    68936GF\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   train_loss               =     0.3662\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   train_runtime            = 0:10:02.93\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   train_samples_per_second =      1.493\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m   train_steps_per_second   =      0.095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [INFO|modelcard.py:450] 2026-02-08 09:02:55,941 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m Figure saved at: qwen2.5_32b_lora_sft/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [WARNING|2026-02-08 09:02:55] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "\u001b[36m(RayTrainWorker pid=3001, ip=10.128.7.103)\u001b[0m [WARNING|2026-02-08 09:02:55] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "\n",
      "Training completed after 2 iterations at 2026-02-08 09:02:58. Total running time: 11min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 09:02:58,095\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/qwen2.5_32b_lora_sft' in 0.0139s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "USE_RAY=1 llamafactory-cli train ../train-configs/sft_lora_deepspeed.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run as an Anyscale job (production)\n",
    "\n",
    "For longer or production runs, submit the training as an **Anyscale job**. Jobs run outside your interactive session for better stability, retries, and durable logs. You package LLaMA-Factory and other libraries in a container image and launch with a short job config. See [Run LLaMA-Factory as an Anyscale job](https://docs.anyscale.com/llm/fine-tuning/llamafactory-jobs) for the step-by-step guide.\n",
    "\n",
    "### Monitoring with Weights & Biases (WandB)\n",
    "If you enabled Weights & Biases (with `report_to: wandb` in the training config YAML file), you can monitor your training job in real-time. Look for the training loss to decrease steadily, which indicates the model is learning.\n",
    "\n",
    "**Weights & Biases example**\n",
    "\n",
    "![WandB](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/3.2.1/3.2.1-wandb.png)\n",
    "\n",
    "For a more detailed guide on tracking experiments with other tools such as Weights & Biases or MLflow, see [Observability and tracking](https://docs.anyscale.com/llm/fine-tuning/observability-and-tracking).\n",
    "\n",
    "## Step 5: Locate checkpoints\n",
    "\n",
    "Ray Train writes checkpoints under `ray_storage_path/ray_run_name`. In this example run, the path is: `/mnt/cluster_storage/qwen2.5_32b_lora_sft`. \n",
    "\n",
    "Inside, you see a **trainer session** directory named like:\n",
    "`TorchTrainer_8c6a5_00000_0_2025-09-09_09-53-45/`.\n",
    "\n",
    "- Ray Train creates `TorchTrainer_*` **when the trainer starts**; the suffix encodes a short run ID and the **start timestamp**.\n",
    "- Within that directory, Ray Train names checkpoints `checkpoint_000xxx/`, where the number is the saved ordered checkpoints.\n",
    "\n",
    "Control the save cadence with `save_strategy` and `save_steps`. For instructions on how to resume interrupted training with `resume_from_checkpoint` and more, see [Understand the artifacts directory](https://docs.anyscale.com/llm/fine-tuning/checkpointing#artifacts-directory).\n",
    "\n",
    "## Step 6: Export the model\n",
    "\n",
    "If you use LoRA, you can keep the base model and adapters separate for [multi-LoRA deployment](https://docs.anyscale.com/llm/serving/multi-lora) or [merge the adapters](https://docs.anyscale.com/llm/fine-tuning/checkpointing#merge-lora) into the base model for low-latency inference. \n",
    "\n",
    "For full fine-tuning or freeze-tuning, export the fine-tuned model directly.\n",
    "\n",
    "You may optionally apply [post-training quantization](https://docs.anyscale.com/llm/fine-tuning/checkpointing#ptq) on merged or full models before serving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
