{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kahneman–Tversky Optimization (KTO) at scale with LoRA\n",
    "\n",
    "This guide provides a step-by-step workflow for preference fine-tuning the [`meta-llama/Meta-Llama-3-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model on a multi-GPU Anyscale cluster. You use **LLaMA-Factory** as the training framework and **LoRA** to reduce memory footprint and enable efficient multi-GPU training.\n",
    "\n",
    "KTO aligns a model to human preferences using **single binary labels (accept or reject)** instead of pairwise “chosen versus rejected” comparisons. KTO directly optimizes the policy on these unary signals, simplifying data preparation while still encouraging preferred behavior and discouraging undesired outputs.\n",
    "\n",
    "## Step 1: Set up your environment\n",
    "\n",
    "### Dependencies\n",
    "First, ensure your environment has the correct libraries. Start with a pre-built container image and install LLaMA-Factory and DeepSpeed on top of it.\n",
    "\n",
    "Recommended container image:\n",
    "```bash\n",
    "anyscale/ray-llm:2.48.0-py311-cu128\n",
    "```\n",
    "\n",
    "Execute the following commands to install the required packages and optional tools for experiment tracking and faster model downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `llamafactory` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `hf_transfer` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `liger-kernel` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `mlflow` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_kpm6l9gjz6gdcskt2zb8i3fie6?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the specific version of LLaMA-Factory\n",
    "pip install -q llamafactory==0.9.3\n",
    "\n",
    "# (Optional) For accelerated model downloads from Hugging Face\n",
    "pip install -q hf_transfer==0.1.9\n",
    "\n",
    "# (Optional) Acceleration methods (ensure CUDA/Torch compatibility)\n",
    "pip install -q liger-kernel==0.6.2\n",
    "\n",
    "# (Optional) Experiment tracking library\n",
    "pip install -q mlflow==3.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and compute resources\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Base model** | [`meta-llama/Meta-Llama-3-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |\n",
    "| **Workers** | 4 × L40S / A100 (1 GPU each) |\n",
    "\n",
    "Compared to SFT, KTO typically holds two copies of the model (policy and reference), and alignment datasets often use long contexts, so Anyscale recommends GPUs with larger VRAM. Techniques such as **LoRA** and memory-efficient attention can further reduce memory pressure.\n",
    "\n",
    "## Step 2: Prepare the dataset\n",
    "\n",
    "### Understand the dataset\n",
    "This tutorial uses `kto_en_demo`, a unary-preference dataset for KTO. Each record contains a multi-turn ShareGPT-style dialogue with a **binary label** indicating whether the modeled behavior is preferred.\n",
    "\n",
    "This dataset contains:\n",
    "- `messages`: Turn-by-turn chat between a user and the assistant.\n",
    "- `label`: A boolean (`true` or `false`) indicating whether the example is preferred.\n",
    "\n",
    "**Note:** To maintain role alignment in ShareGPT format, you must follow a strict turn order: `human` and `observation` (tool output) must appear in odd-numbered positions, while `gpt` and `function_call` must appear in even-numbered positions. The model learns to generate the content in the `gpt` and `function_call` turns.\n",
    "\n",
    "**Dataset example**\n",
    "```json\n",
    "{\n",
    "\"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"Compare and contrast the roles of the hippocampus and the prefrontal cortex...\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"The human brain is a highly complex organ, responsible for a myriad of cognitive functions...\" },\n",
    "    { \"role\": \"user\", \"content\": \"Discuss the mechanisms through which the prefrontal cortex ...\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"The prefrontal cortex (PFC)...\" },\n",
    "    { \"role\": \"user\", \"content\": \"Can you elaborate on the role of the amygdala...\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"The amygdala plays a crucial role in the emotional processing of stored memories...\" }\n",
    "],\n",
    "\"label\": true\n",
    "}\n",
    "```\n",
    "\n",
    "### Register the dataset\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add a **`dataset_info.json`** to **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)** such as `/mnt/cluster_storage`. This configuration file acts as a central registry for all your datasets. It maps a custom name to your dataset file location, format, and column structure. \n",
    "\n",
    "If you plan to run KTO post-training on the `kto_en_demo` dataset, first complete the setup steps below. Ensure that you place the dataset files in a storage location that all workers can access (for example, a shared mount or object storage). Avoid storing large files on the head node. \n",
    "\n",
    "`dataset_info.json`\n",
    "\n",
    "- `kto_tag` maps the unary preference label used by KTO.\n",
    "- `tags` helps the loader interpret role/content fields in ShareGPT-style records.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"my_kto_en_demo\": {\n",
    "    \"file_name\": \"/mnt/cluster_storage/kto_en_demo.json\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\",\n",
    "      \"kto_tag\": \"label\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed dataset preparation and formatting guide, see [Choose your data format](https://docs.anyscale.com/llm/fine-tuning/data-preparation#kto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2026-02-08 17:35:46--  https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/kto_en_demo.json\n",
      "Resolving anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)... 52.218.216.97, 3.5.84.219, 3.5.82.122, ...\n",
      "Connecting to anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)|52.218.216.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 913519 (892K) [application/json]\n",
      "Saving to: ‘/mnt/cluster_storage/kto_en_demo.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  5%  264K 3s\n",
      "    50K .......... .......... .......... .......... .......... 11%  327K 3s\n",
      "   100K .......... .......... .......... .......... .......... 16%  429K 2s\n",
      "   150K .......... .......... .......... .......... .......... 22%  604K 2s\n",
      "   200K .......... .......... .......... .......... .......... 28% 99.8M 1s\n",
      "   250K .......... .......... .......... .......... .......... 33%  719K 1s\n",
      "   300K .......... .......... .......... .......... .......... 39% 92.2M 1s\n",
      "   350K .......... .......... .......... .......... .......... 44% 70.3M 1s\n",
      "   400K .......... .......... .......... .......... .......... 50% 1.37M 1s\n",
      "   450K .......... .......... .......... .......... .......... 56%  271M 1s\n",
      "   500K .......... .......... .......... .......... .......... 61%  173M 0s\n",
      "   550K .......... .......... .......... .......... .......... 67%  430K 0s\n",
      "   600K .......... .......... .......... .......... .......... 72%  244M 0s\n",
      "   650K .......... .......... .......... .......... .......... 78%  156M 0s\n",
      "   700K .......... .......... .......... .......... .......... 84%  142M 0s\n",
      "   750K .......... .......... .......... .......... .......... 89%  353M 0s\n",
      "   800K .......... .......... .......... .......... .......... 95% 91.7M 0s\n",
      "   850K .......... .......... .......... .......... ..        100%  549M=0.8s\n",
      "\n",
      "2026-02-08 17:35:48 (1.14 MB/s) - ‘/mnt/cluster_storage/kto_en_demo.json’ saved [913519/913519]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Make sure all files are accessible to worker nodes\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "wget https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/kto_en_demo.json -O /mnt/cluster_storage/kto_en_demo.json\n",
    "# Create a copy of the dataset registry in /mnt/cluster_storage\n",
    "cp ../dataset-configs/dataset_info.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the preference-tuning config (KTO and LoRA)\n",
    "\n",
    "Create a YAML file that defines your **KTO** run. It specifies the base model, dataset, **LoRA** settings, KTO hyperparameters, optional acceleration methods, logging, and Ray cluster resources.\n",
    "\n",
    "**Important notes:**\n",
    "- **Acceleration libraries:** `liger-kernel` can reduce VRAM and improve throughput across multiple transformer ops, but actual speed and memory gains vary with GPU architecture, sequence length, batch size, precision, kernel availability. Benchmark your training workloads to confirm improvements.\n",
    "- **Access and paths:** The YAML only needs to be on the **head node**, but any referenced paths (for example, `dataset_dir`, `ray_storage_path`, `output_dir`) must be on **shared storage** (such as `/mnt/cluster_storage/`) visible to all workers.\n",
    "- **Gated models:** If your base model has gated access on Hugging Face, set `HF_TOKEN` in the runtime environment.\n",
    "- **Memory tips:** If VRAM is tight, consider switching to [QLoRA]((https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/llamafactory-llm-fine-tune/notebooks/dpo_qlora.ipynb)) (4/8-bit) and adding the corresponding quantization keys.\n",
    "\n",
    "### Configure LLaMA-Factory with Ray\n",
    "\n",
    "**Note**: To customize the training configuration, edit `train-configs/kto_lora.yaml`. \n",
    "\n",
    "```yaml\n",
    "# kto_lora.yaml\n",
    "\n",
    "### model\n",
    "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: kto\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "pref_beta: 0.1\n",
    "\n",
    "### acceleration methods\n",
    "enable_liger_kernel: true  # Reduce VRAM and improve throughput across multiple transformer ops\n",
    "\n",
    "### dataset\n",
    "dataset: my_kto_en_demo\n",
    "dataset_dir: /mnt/cluster_storage\n",
    "\n",
    "template: llama3\n",
    "cutoff_len: 1024\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: llama3_8b_lora_kto\n",
    "logging_steps: 5\n",
    "save_steps: 50\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "report_to: mlflow   # or none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 2\n",
    "num_train_epochs: 3.0  # Low for demo purpose; adjust as needed\n",
    "learning_rate: 5.0e-6\n",
    "bf16: true\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### ray\n",
    "ray_run_name: llama3_8b_kto_lora\n",
    "ray_storage_path: /mnt/cluster_storage/\n",
    "ray_num_workers: 4\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  anyscale/accelerator_shape:4xL40S: 0.001  # Pin a specific node shape\n",
    "  # accelerator_type:L40S: 0.001            # or just request a GPU type\n",
    "\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      # If using gated models like meta-llama/Llama-3-8B-Instruct\n",
    "      HF_TOKEN: <your_huggingface_token>\n",
    "      # Enable faster downloads if hf_transfer is installed:\n",
    "      HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "      # If using mlflow for experiments tracking\n",
    "      MLFLOW_TRACKING_URI: \"https://<your_cloud_id>.cloud.databricks.com\"\n",
    "      MLFLOW_TRACKING_TOKEN: \"<mlflow_tracking_token>\"\n",
    "      MLFLOW_EXPERIMENT_NAME: \"/Users/<your_user_id>/experiment_name\"\n",
    "```\n",
    "\n",
    "## Step 4: Train and monitor\n",
    "\n",
    "**Note**: For gated models such as [`meta-llama/Meta-Llama-3-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), ensure that you accept the license agreement for the models on the Hugging Face site and set `HF_TOKEN` in the runtime environment. If you installed MLflow, configure its credentials. Otherwise, set `report_to: none` in `kto_lora.yaml` to avoid `api_token not set` errors.\n",
    "\n",
    "With all configurations in place, you can launch fine-tuning or post-training in one of two ways:\n",
    "\n",
    "### Option A: Run from a workspace (quick start)\n",
    "\n",
    "The `USE_RAY=1` prefix tells LLaMA-Factory to run in distributed mode on the Ray cluster attached to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-08 17:36:00,899] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2026-02-08 17:36:00,899] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "INFO 02-08 17:36:03 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 02-08 17:36:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 17:36:05,301\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.128.4.189:6379...\n",
      "2026-02-08 17:36:05,313\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-c1mvc6t862zj4fbguuknngnrgv.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-08 17:36:05,315\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_873ed4d0505528fa538926e1170e8ec9f1d45599.zip' (0.29MiB) to Ray cluster...\n",
      "2026-02-08 17:36:05,316\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_873ed4d0505528fa538926e1170e8ec9f1d45599.zip'.\n",
      "2026-02-08 17:36:05,530\tWARNING tune_controller.py:2132 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (202 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent pending trials.\n",
      "2026-02-08 17:36:05,531\tWARNING tune_controller.py:2132 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (202 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent pending trials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/llama3_8b_kto_lora\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2026-02-08_15-34-45_799476_185/artifacts/2026-02-08_17-36-05/llama3_8b_kto_lora/driver_artifacts`\n",
      "\u001b[36m(TrainTrainable pid=4037, ip=10.128.6.27)\u001b[0m [2026-02-08 17:36:13,658] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "\u001b[36m(TrainTrainable pid=4037, ip=10.128.6.27)\u001b[0m [2026-02-08 17:36:13,659] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                             True │\n",
      "│ train_loop_config/args/cutoff_len                                                                       1024 │\n",
      "│ train_loop_config/args/dataset                                                                my_kto_en_demo │\n",
      "│ train_loop_config/args/dataset_dir                                                      /mnt/cluster_storage │\n",
      "│ train_loop_config/args/ddp_timeout                                                                 180000000 │\n",
      "│ train_loop_config/args/do_train                                                                         True │\n",
      "│ train_loop_config/args/enable_liger_kernel                                                              True │\n",
      "│ train_loop_config/args/finetuning_type                                                                  lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                         2 │\n",
      "│ train_loop_config/args/learning_rate                                                                   5e-06 │\n",
      "│ train_loop_config/args/logging_steps                                                                       5 │\n",
      "│ train_loop_config/args/lora_rank                                                                           8 │\n",
      "│ train_loop_config/args/lora_target                                                                       all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                              cosine │\n",
      "│ train_loop_config/args/max_samples                                                                      1000 │\n",
      "│ train_loop_config/args/model_name_or_path                                               ...ama-3-8B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                                  3.0 │\n",
      "│ train_loop_config/args/output_dir                                                         llama3_8b_lora_kto │\n",
      "│ train_loop_config/args/overwrite_cache                                                                  True │\n",
      "│ train_loop_config/args/overwrite_output_dir                                                             True │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                         1 │\n",
      "│ train_loop_config/args/plot_loss                                                                        True │\n",
      "│ train_loop_config/args/pref_beta                                                                         0.1 │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                          16 │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_HUB_ENABLE_HF_TRANSFER                      1 │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_TOKEN                    ...xTVvvViNkXwmPgTvK │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/MLFLOW_EXPERIMENT_NAME      ...>/experiment_name │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/MLFLOW_TRACKING_TOKEN       ...w_tracking_token> │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/MLFLOW_TRACKING_URI         ...ud.databricks.com │\n",
      "│ train_loop_config/args/ray_num_workers                                                                     4 │\n",
      "│ train_loop_config/args/ray_run_name                                                       llama3_8b_kto_lora │\n",
      "│ train_loop_config/args/ray_storage_path                                                 .../cluster_storage/ │\n",
      "│ train_loop_config/args/report_to                                                                        none │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                            1 │\n",
      "│ train_loop_config/args/save_steps                                                                         50 │\n",
      "│ train_loop_config/args/stage                                                                             kto │\n",
      "│ train_loop_config/args/template                                                                       llama3 │\n",
      "│ train_loop_config/args/trust_remote_code                                                                True │\n",
      "│ train_loop_config/args/warmup_ratio                                                                      0.1 │\n",
      "│ train_loop_config/callbacks                                                             ... 0x7f4546d645d0>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=4037, ip=10.128.6.27)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=4037, ip=10.128.6.27)\u001b[0m - (node_id=b4b2552a8e4226a8a0d5e49624b9a465a3e4259cfbe479ced7e89233, ip=10.128.6.27, pid=4171) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=4037, ip=10.128.6.27)\u001b[0m - (node_id=b4b2552a8e4226a8a0d5e49624b9a465a3e4259cfbe479ced7e89233, ip=10.128.6.27, pid=4172) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=4037, ip=10.128.6.27)\u001b[0m - (node_id=b4b2552a8e4226a8a0d5e49624b9a465a3e4259cfbe479ced7e89233, ip=10.128.6.27, pid=4173) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=4037, ip=10.128.6.27)\u001b[0m - (node_id=b4b2552a8e4226a8a0d5e49624b9a465a3e4259cfbe479ced7e89233, ip=10.128.6.27, pid=4170) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4172, ip=10.128.6.27)\u001b[0m [2026-02-08 17:36:25,494] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:29] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:29] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,498 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,498 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,499 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,499 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,499 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:31,499 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 17:36:31,846 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:36:32,556 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:36:32,559 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2026-02-08 17:36:32,754 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m [rank3]:[W208 17:36:33.278118157 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2026-02-08 17:36:33,128 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:33] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:33] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [WARNING|2026-02-08 17:36:33] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:33] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/kto_en_demo.json...\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [2026-02-08 17:36:25,516] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300 examples [00:00, 16526.02 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/300 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  76%|███████▌  | 228/300 [00:00<00:00, 2273.80 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 300/300 [00:00<00:00, 1488.04 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/300 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 19/300 [00:00<00:14, 19.77 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|█▎        | 38/300 [00:01<00:06, 39.26 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|█▉        | 57/300 [00:01<00:04, 57.42 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  25%|██▌       | 76/300 [00:01<00:03, 73.61 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 95/300 [00:01<00:02, 87.29 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  38%|███▊      | 114/300 [00:01<00:01, 98.62 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  44%|████▍     | 133/300 [00:01<00:01, 107.34 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  51%|█████     | 152/300 [00:01<00:01, 113.45 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 171/300 [00:02<00:01, 119.07 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 190/300 [00:02<00:00, 121.92 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  70%|██████▉   | 209/300 [00:02<00:00, 124.81 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [rank0]:[W208 17:36:34.465790786 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 246/300 [00:02<00:00, 152.01 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 264/300 [00:02<00:00, 136.99 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 282/300 [00:02<00:00, 140.56 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 300/300 [00:03<00:00, 118.81 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 300/300 [00:03<00:00, 93.64 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [128000, 128006, 882, 128007, 271, 791, 12411, 17657, 9849, 374, 2133, 1306, 21930, 1698, 11, 9660, 315, 6500, 2082, 430, 4685, 5694, 389, 279, 19002, 315, 8191, 3932, 311, 3839, 477, 24927, 872, 2930, 7640, 627, 48, 25, 16299, 374, 279, 1888, 12399, 315, 420, 4652, 5380, 38053, 701, 4320, 505, 512, 4444, 570, 4435, 198, 5462, 570, 13482, 198, 3100, 570, 8184, 198, 5549, 570, 10170, 17146, 4842, 198, 40, 1781, 279, 4320, 374, 128009, 128006, 78191, 128007, 271, 46, 2319, 297, 2319, 23128, 23128, 0, 353, 70, 343, 3491, 9, 6914, 757, 1781, 1131, 507, 2319, 297, 2319, 23128, 23128, 0, 353, 70, 343, 3491, 9, 578, 1888, 12399, 315, 420, 4652, 374, 1131, 353, 3696, 372, 1119, 9, 1131, 423, 0, 10170, 17146, 4842, 0, 816, 352, 0, 353, 6263, 29037, 9, 578, 12411, 17657, 9849, 374, 7556, 922, 21930, 1698, 11, 902, 374, 1093, 264, 2697, 6500, 10077, 430, 649, 3821, 1148, 499, 656, 389, 701, 6500, 2085, 499, 14392, 13, 1102, 596, 1093, 264, 6367, 8479, 11, 719, 539, 264, 6555, 832, 0, 353, 70, 13671, 9, 578, 79360, 6944, 311, 3009, 279, 21930, 1698, 505, 3815, 1202, 3245, 11, 779, 814, 2351, 2133, 1306, 433, 0, 353, 1557, 261, 9, 28357, 2319, 2689, 0, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m The Federal Trade Commission is going after spyware, bits of computer code that install themselves on the computers of Internet users to track or disrupt their online activities.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Q: Which is the best summary of this article?\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Pick your answer from:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m (A). World\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m (B). Sports\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m (C). Business\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m (D). Science/Tech\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m I think the answer is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Ooh ooh ah ah! *giggle* Let me think... Ooh ooh ah ah! *giggle* The best summary of this article is... *drumroll*... D! Science/Tech! Yay! *confetti* The Federal Trade Commission is talking about spyware, which is like a little computer bug that can watch what you do on your computer without you knowing. It's like a secret agent, but not a nice one! *gasp* The FTC wants to stop the spyware from doing its thing, so they're going after it! *cheer* Woohoo!<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46, 2319, 297, 2319, 23128, 23128, 0, 353, 70, 343, 3491, 9, 6914, 757, 1781, 1131, 507, 2319, 297, 2319, 23128, 23128, 0, 353, 70, 343, 3491, 9, 578, 1888, 12399, 315, 420, 4652, 374, 1131, 353, 3696, 372, 1119, 9, 1131, 423, 0, 10170, 17146, 4842, 0, 816, 352, 0, 353, 6263, 29037, 9, 578, 12411, 17657, 9849, 374, 7556, 922, 21930, 1698, 11, 902, 374, 1093, 264, 2697, 6500, 10077, 430, 649, 3821, 1148, 499, 656, 389, 701, 6500, 2085, 499, 14392, 13, 1102, 596, 1093, 264, 6367, 8479, 11, 719, 539, 264, 6555, 832, 0, 353, 70, 13671, 9, 578, 79360, 6944, 311, 3009, 279, 21930, 1698, 505, 3815, 1202, 3245, 11, 779, 814, 2351, 2133, 1306, 433, 0, 353, 1557, 261, 9, 28357, 2319, 2689, 0, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Ooh ooh ah ah! *giggle* Let me think... Ooh ooh ah ah! *giggle* The best summary of this article is... *drumroll*... D! Science/Tech! Yay! *confetti* The Federal Trade Commission is talking about spyware, which is like a little computer bug that can watch what you do on your computer without you knowing. It's like a secret agent, but not a nice one! *gasp* The FTC wants to stop the spyware from doing its thing, so they're going after it! *cheer* Woohoo!<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:30] llamafactory.hparams.parser:406 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:36:39,153 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:36:39,154 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:39] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:39] llamafactory.model.model_utils.liger_kernel:143 >> Current training stage does not support chunked cross entropy.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:36:39] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|modeling_utils.py:1151] 2026-02-08 17:36:40,010 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model.safetensors.index.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|modeling_utils.py:2241] 2026-02-08 17:37:01,704 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 17:37:01,708 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m /tmp/ray/session_2026-02-08_15-34-45_799476_185/runtime_resources/pip/08dff354b281bd7401a59f12bd99e4624197f819/virtualenv/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m I have left this message as the final dev message to help you transition.\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m Important Notice:\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m Alternative:\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m For further inquiries, feel free to reach out:\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - X: https://x.com/casper_hansen_\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m - LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m   warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|modeling_utils.py:5131] 2026-02-08 17:37:07,432 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|modeling_utils.py:5139] 2026-02-08 17:37:07,433 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:1090] 2026-02-08 17:37:07,534 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:1135] 2026-02-08 17:37:07,534 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     128001,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     128009\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_length\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"temperature\": 0.6,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"top_p\": 0.9\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:07] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:07] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:07] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:07] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:07] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,v_proj,up_proj,o_proj,q_proj,gate_proj,k_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4172, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4172, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4172, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4172, ip=10.128.6.27)\u001b[0m \n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.76s/it]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4173, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4173, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4173, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4173, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|2026-02-08 17:37:08] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:756] 2026-02-08 17:37:08,992 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2409] 2026-02-08 17:37:09,550 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2410] 2026-02-08 17:37:09,550 >>   Num examples = 300\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2411] 2026-02-08 17:37:09,550 >>   Num Epochs = 3\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2412] 2026-02-08 17:37:09,550 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2415] 2026-02-08 17:37:09,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2416] 2026-02-08 17:37:09,550 >>   Gradient Accumulation steps = 2\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2417] 2026-02-08 17:37:09,550 >>   Total optimization steps = 114\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2418] 2026-02-08 17:37:09,556 >>   Number of trainable parameters = 20,971,520\n",
      "  0%|          | 0/114 [00:00<?, ?it/s]8.6.27)\u001b[0m \n",
      "  1%|          | 1/114 [00:03<07:21,  3.91s/it][0m \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m /tmp/ray/session_2026-02-08_15-34-45_799476_185/runtime_resources/pip/08dff354b281bd7401a59f12bd99e4624197f819/virtualenv/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m I have left this message as the final dev message to help you transition.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Important Notice:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - AutoAWQ is officially deprecated and will no longer be maintained.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Alternative:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m For further inquiries, feel free to reach out:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - X: https://x.com/casper_hansen_\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m - LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  2%|▏         | 2/114 [00:05<04:45,  2.55s/it][0m \n",
      "  3%|▎         | 3/114 [00:06<03:48,  2.06s/it][0m \n",
      "  4%|▎         | 4/114 [00:08<03:19,  1.82s/it][0m \n",
      "  4%|▍         | 5/114 [00:09<03:06,  1.71s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4957, 'grad_norm': 3.1498160362243652, 'learning_rate': 1.6666666666666667e-06, 'rewards/chosen': 0.019817462989262173, 'logps/chosen': -413.7041015625, 'logits/chosen': -23078601.14285714, 'rewards/rejected': -0.10288289189338684, 'logps/rejected': -970.846435546875, 'logits/rejected': -29555157.333333332, 'rewards/margins': 0.12270035488264902, 'kl': 0.3308525085449219, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/114 [00:11<02:46,  1.54s/it][0m \n",
      "  6%|▌         | 7/114 [00:12<02:41,  1.51s/it][0m \n",
      "  7%|▋         | 8/114 [00:13<02:36,  1.47s/it][0m \n",
      "  8%|▊         | 9/114 [00:15<02:29,  1.42s/it][0m \n",
      "  9%|▉         | 10/114 [00:16<02:22,  1.37s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.5045, 'grad_norm': 3.2115280628204346, 'learning_rate': 3.7500000000000005e-06, 'rewards/chosen': 0.008976592123508454, 'logps/chosen': -376.96708984375, 'logits/chosen': -37118019.2, 'rewards/rejected': 0.02862915098667145, 'logps/rejected': -346.9309814453125, 'logits/rejected': -27582691.2, 'rewards/margins': -0.019652558863162993, 'kl': 1.0791082382202148, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 11/114 [00:17<02:20,  1.36s/it]0m \n",
      " 11%|█         | 12/114 [00:19<02:13,  1.31s/it]0m \n",
      " 11%|█▏        | 13/114 [00:20<02:15,  1.34s/it]0m \n",
      " 12%|█▏        | 14/114 [00:21<02:06,  1.26s/it]0m \n",
      " 13%|█▎        | 15/114 [00:22<02:02,  1.24s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4998, 'grad_norm': 2.4262821674346924, 'learning_rate': 4.995258321842611e-06, 'rewards/chosen': -0.007094318668047587, 'logps/chosen': -185.7591552734375, 'logits/chosen': -7981500.0, 'rewards/rejected': 0.012033191110406603, 'logps/rejected': -137.62447684151786, 'logits/rejected': -7264017.714285715, 'rewards/margins': -0.01912750977845419, 'kl': 1.341604232788086, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 16/114 [00:23<01:58,  1.21s/it]0m \n",
      " 15%|█▍        | 17/114 [00:25<01:54,  1.18s/it]0m \n",
      " 16%|█▌        | 18/114 [00:26<01:54,  1.19s/it]0m \n",
      " 17%|█▋        | 19/114 [00:27<01:57,  1.24s/it]0m \n",
      " 18%|█▊        | 20/114 [00:28<01:59,  1.28s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.5031, 'grad_norm': 2.7462239265441895, 'learning_rate': 4.942120794399002e-06, 'rewards/chosen': -0.025224685668945312, 'logps/chosen': -362.2128092447917, 'logits/chosen': -32110208.0, 'rewards/rejected': 0.07098617404699326, 'logps/rejected': -414.457275390625, 'logits/rejected': -10392526.0, 'rewards/margins': -0.09621085971593857, 'kl': 0.2899312973022461, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 21/114 [00:30<01:59,  1.29s/it]0m \n",
      " 19%|█▉        | 22/114 [00:31<02:00,  1.31s/it]0m \n",
      " 20%|██        | 23/114 [00:32<01:56,  1.28s/it]0m \n",
      " 21%|██        | 24/114 [00:34<01:56,  1.29s/it]0m \n",
      " 22%|██▏       | 25/114 [00:35<01:56,  1.31s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4917, 'grad_norm': 3.474839448928833, 'learning_rate': 4.83118057351089e-06, 'rewards/chosen': 0.011423779651522636, 'logps/chosen': -415.261474609375, 'logits/chosen': -15809656.0, 'rewards/rejected': 0.0009204866364598274, 'logps/rejected': -133.61837768554688, 'logits/rejected': -4753514.0, 'rewards/margins': 0.010503293015062809, 'kl': 0.49626922607421875, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 26/114 [00:36<01:52,  1.28s/it]0m \n",
      " 24%|██▎       | 27/114 [00:37<01:50,  1.26s/it]0m \n",
      " 25%|██▍       | 28/114 [00:39<01:50,  1.28s/it]0m \n",
      " 25%|██▌       | 29/114 [00:40<01:48,  1.28s/it]0m \n",
      " 26%|██▋       | 30/114 [00:41<01:47,  1.28s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4966, 'grad_norm': 3.14445424079895, 'learning_rate': 4.665063509461098e-06, 'rewards/chosen': -0.00511518990000089, 'logps/chosen': -320.1336669921875, 'logits/chosen': -43194530.666666664, 'rewards/rejected': -0.008688926696777344, 'logps/rejected': -399.90087890625, 'logits/rejected': -20492260.0, 'rewards/margins': 0.003573736796776454, 'kl': 0.44983482360839844, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 31/114 [00:43<01:44,  1.26s/it]0m \n",
      " 28%|██▊       | 32/114 [00:44<01:42,  1.25s/it]0m \n",
      " 29%|██▉       | 33/114 [00:45<01:43,  1.27s/it]0m \n",
      " 30%|██▉       | 34/114 [00:46<01:43,  1.30s/it]0m \n",
      " 31%|███       | 35/114 [00:48<01:43,  1.31s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.5011, 'grad_norm': 3.3660974502563477, 'learning_rate': 4.447701436314176e-06, 'rewards/chosen': -0.0157562255859375, 'logps/chosen': -319.4732177734375, 'logits/chosen': -2980669.8, 'rewards/rejected': -0.02387329190969467, 'logps/rejected': -322.132763671875, 'logits/rejected': 1842462.4, 'rewards/margins': 0.008117066323757173, 'kl': 0.14889907836914062, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 36/114 [00:49<01:41,  1.30s/it]0m \n",
      " 32%|███▏      | 37/114 [00:50<01:39,  1.30s/it]0m \n",
      " 33%|███▎      | 38/114 [00:51<01:24,  1.11s/it]0m \n",
      " 34%|███▍      | 39/114 [00:52<01:27,  1.16s/it]0m \n",
      " 35%|███▌      | 40/114 [00:54<01:27,  1.18s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4468, 'grad_norm': 2.8790392875671387, 'learning_rate': 4.184239109116393e-06, 'rewards/chosen': -0.044304912288983665, 'logps/chosen': -240.10538736979166, 'logits/chosen': -22431341.333333332, 'rewards/rejected': 0.013453165690104166, 'logps/rejected': -928.1993815104166, 'logits/rejected': -30207528.0, 'rewards/margins': -0.05775807797908783, 'kl': 0.40143871307373047, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 41/114 [00:55<01:28,  1.21s/it]0m \n",
      " 37%|███▋      | 42/114 [00:56<01:27,  1.21s/it]0m \n",
      " 38%|███▊      | 43/114 [00:57<01:28,  1.25s/it]0m \n",
      " 39%|███▊      | 44/114 [00:59<01:28,  1.27s/it]0m \n",
      " 39%|███▉      | 45/114 [01:00<01:27,  1.27s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4913, 'grad_norm': 3.927604913711548, 'learning_rate': 3.880912432401265e-06, 'rewards/chosen': -0.018583680192629497, 'logps/chosen': -118.12808227539062, 'logits/chosen': -6146621.333333333, 'rewards/rejected': -0.049512343747275214, 'logps/rejected': -311.44423130580356, 'logits/rejected': -4062452.285714286, 'rewards/margins': 0.030928663554645717, 'kl': 0.0, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 46/114 [01:01<01:28,  1.30s/it]0m \n",
      " 41%|████      | 47/114 [01:02<01:23,  1.25s/it]0m \n",
      " 42%|████▏     | 48/114 [01:04<01:24,  1.27s/it]0m \n",
      " 43%|████▎     | 49/114 [01:05<01:23,  1.29s/it]0m \n",
      " 44%|████▍     | 50/114 [01:06<01:22,  1.28s/it][INFO|trainer.py:3993] 2026-02-08 17:38:16,507 >> Saving model checkpoint to llama3_8b_lora_kto/checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4908, 'grad_norm': 2.7847020626068115, 'learning_rate': 3.544900862216959e-06, 'rewards/chosen': -0.015361366527421134, 'logps/chosen': -232.72154017857142, 'logits/chosen': -5122232.0, 'rewards/rejected': -0.0729100505510966, 'logps/rejected': -227.35882568359375, 'logits/rejected': -17326696.0, 'rewards/margins': 0.05754868402367547, 'kl': 1.5837535858154297, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:38:16,731 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:38:16,732 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 17:38:16,887 >> chat template saved in llama3_8b_lora_kto/checkpoint-50/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 17:38:16,890 >> tokenizer config file saved in llama3_8b_lora_kto/checkpoint-50/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 17:38:16,890 >> Special tokens file saved in llama3_8b_lora_kto/checkpoint-50/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2026-02-08 17:38:19. Total running time: 2min 14s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              123.13147 │\n",
      "│ time_total_s                  123.13147 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                              1.32 │\n",
      "│ grad_norm                        2.7847 │\n",
      "│ kl                              1.58375 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                 -5122232. │\n",
      "│ logits/rejected              -17326696. │\n",
      "│ logps/chosen                 -232.72154 │\n",
      "│ logps/rejected               -227.35883 │\n",
      "│ loss                             0.4908 │\n",
      "│ rewards/chosen                 -0.01536 │\n",
      "│ rewards/margins                 0.05755 │\n",
      "│ rewards/rejected               -0.07291 │\n",
      "│ step                                 50 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 51/114 [01:11<02:25,  2.31s/it]0m \n",
      " 46%|████▌     | 52/114 [01:12<02:04,  2.01s/it]0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 46%|████▋     | 53/114 [01:14<01:49,  1.80s/it]0m \n",
      " 47%|████▋     | 54/114 [01:15<01:39,  1.66s/it]0m \n",
      " 48%|████▊     | 55/114 [01:16<01:31,  1.55s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4962, 'grad_norm': 3.4667067527770996, 'learning_rate': 3.184157475180208e-06, 'rewards/chosen': -0.009737288313252586, 'logps/chosen': -425.35477120535717, 'logits/chosen': -25772352.0, 'rewards/rejected': -0.07607295115788777, 'logps/rejected': -427.5118001302083, 'logits/rejected': 8856781.333333334, 'rewards/margins': 0.06633566284463518, 'kl': 1.3006458282470703, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 56/114 [01:18<01:25,  1.48s/it]0m \n",
      " 50%|█████     | 57/114 [01:19<01:22,  1.44s/it]0m \n",
      " 51%|█████     | 58/114 [01:20<01:19,  1.42s/it]0m \n",
      " 52%|█████▏    | 59/114 [01:21<01:11,  1.31s/it]0m \n",
      " 53%|█████▎    | 60/114 [01:23<01:10,  1.30s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4845, 'grad_norm': 4.169820785522461, 'learning_rate': 2.8072207266617856e-06, 'rewards/chosen': 0.02910013198852539, 'logps/chosen': -160.93770751953124, 'logits/chosen': -5199646.0, 'rewards/rejected': -0.06544250845909119, 'logps/rejected': -299.449169921875, 'logits/rejected': -15725676.8, 'rewards/margins': 0.09454264044761658, 'kl': 0.0, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 61/114 [01:24<01:08,  1.30s/it]0m \n",
      " 54%|█████▍    | 62/114 [01:25<01:08,  1.31s/it]0m \n",
      " 55%|█████▌    | 63/114 [01:27<01:06,  1.31s/it]0m \n",
      " 56%|█████▌    | 64/114 [01:28<01:03,  1.28s/it]0m \n",
      " 57%|█████▋    | 65/114 [01:29<01:02,  1.28s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4793, 'grad_norm': 3.130155086517334, 'learning_rate': 2.4230123536095746e-06, 'rewards/chosen': 0.01244094967842102, 'logps/chosen': -264.1098876953125, 'logits/chosen': -38034163.2, 'rewards/rejected': -0.06333073377609252, 'logps/rejected': -439.5146484375, 'logits/rejected': -38250316.8, 'rewards/margins': 0.07577168345451354, 'kl': 0.34583091735839844, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 66/114 [01:30<00:59,  1.24s/it]0m \n",
      " 59%|█████▉    | 67/114 [01:32<00:59,  1.26s/it]0m \n",
      " 60%|█████▉    | 68/114 [01:33<00:57,  1.25s/it]0m \n",
      " 61%|██████    | 69/114 [01:34<00:58,  1.29s/it]0m \n",
      " 61%|██████▏   | 70/114 [01:36<00:56,  1.29s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4938, 'grad_norm': 3.2651515007019043, 'learning_rate': 2.040626205458574e-06, 'rewards/chosen': 0.03988765552639961, 'logps/chosen': -339.7088928222656, 'logits/chosen': -47686432.0, 'rewards/rejected': 0.0029575349763035774, 'logps/rejected': -67.38799285888672, 'logits/rejected': 4996398.5, 'rewards/margins': 0.036930120550096035, 'kl': 0.0, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 71/114 [01:37<00:54,  1.27s/it]0m \n",
      " 63%|██████▎   | 72/114 [01:38<00:53,  1.27s/it]0m \n",
      " 64%|██████▍   | 73/114 [01:39<00:52,  1.27s/it]0m \n",
      " 65%|██████▍   | 74/114 [01:41<00:50,  1.27s/it]0m \n",
      " 66%|██████▌   | 75/114 [01:42<00:47,  1.22s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4749, 'grad_norm': 3.6366844177246094, 'learning_rate': 1.6691130013008514e-06, 'rewards/chosen': -0.008752670884132386, 'logps/chosen': -152.84658203125, 'logits/chosen': -27358192.0, 'rewards/rejected': -0.103485107421875, 'logps/rejected': -497.47470703125, 'logits/rejected': -23272065.6, 'rewards/margins': 0.09473243653774262, 'kl': 0.018812179565429688, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 76/114 [01:42<00:40,  1.07s/it]0m \n",
      " 68%|██████▊   | 77/114 [01:44<00:41,  1.12s/it]0m \n",
      " 68%|██████▊   | 78/114 [01:45<00:41,  1.14s/it]0m \n",
      " 69%|██████▉   | 79/114 [01:46<00:41,  1.17s/it]0m \n",
      " 70%|███████   | 80/114 [01:47<00:40,  1.18s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4234, 'grad_norm': 4.389901161193848, 'learning_rate': 1.3172661079099752e-06, 'rewards/chosen': 0.11917743682861329, 'logps/chosen': -551.941552734375, 'logits/chosen': -40424889.6, 'rewards/rejected': -0.14022178947925568, 'logps/rejected': -545.833251953125, 'logits/rejected': -30141144.0, 'rewards/margins': 0.25939922630786894, 'kl': 0.07353878021240234, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 81/114 [01:48<00:39,  1.19s/it]0m \n",
      " 72%|███████▏  | 82/114 [01:50<00:37,  1.18s/it]0m \n",
      " 73%|███████▎  | 83/114 [01:51<00:37,  1.22s/it]0m \n",
      " 74%|███████▎  | 84/114 [01:52<00:36,  1.23s/it]0m \n",
      " 75%|███████▍  | 85/114 [01:54<00:36,  1.26s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4875, 'grad_norm': 3.5152552127838135, 'learning_rate': 9.934134090518593e-07, 'rewards/chosen': 0.023151906828085583, 'logps/chosen': -342.3819580078125, 'logits/chosen': -34245232.0, 'rewards/rejected': -0.0966060683131218, 'logps/rejected': -302.7929992675781, 'logits/rejected': 3230285.0, 'rewards/margins': 0.11975797514120738, 'kl': 0.0, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 85/114 [01:54<00:36,  1.26s/it]0m \n",
      " 75%|███████▌  | 86/114 [01:55<00:35,  1.28s/it]0m \n",
      " 76%|███████▋  | 87/114 [01:56<00:34,  1.26s/it]0m \n",
      " 77%|███████▋  | 88/114 [01:57<00:32,  1.25s/it]0m \n",
      " 78%|███████▊  | 89/114 [01:58<00:30,  1.22s/it]0m \n",
      " 79%|███████▉  | 90/114 [02:00<00:28,  1.19s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4811, 'grad_norm': 2.9937925338745117, 'learning_rate': 7.052201923388955e-07, 'rewards/chosen': 0.03335037330786387, 'logps/chosen': -193.55106608072916, 'logits/chosen': -6227604.666666667, 'rewards/rejected': -0.11973920890263148, 'logps/rejected': -304.60145786830356, 'logits/rejected': -6605980.571428572, 'rewards/margins': 0.15308958221049535, 'kl': 0.0, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 91/114 [02:01<00:28,  1.24s/it]0m \n",
      " 81%|████████  | 92/114 [02:02<00:27,  1.27s/it]0m \n",
      " 82%|████████▏ | 93/114 [02:04<00:26,  1.28s/it]0m \n",
      " 82%|████████▏ | 94/114 [02:05<00:25,  1.26s/it]0m \n",
      " 83%|████████▎ | 95/114 [02:06<00:22,  1.20s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4764, 'grad_norm': 4.578560829162598, 'learning_rate': 4.5950771910944603e-07, 'rewards/chosen': 0.09679057200749715, 'logps/chosen': -452.3634440104167, 'logits/chosen': -20889709.333333332, 'rewards/rejected': -0.1498851776123047, 'logps/rejected': -526.0985107421875, 'logits/rejected': -38741048.0, 'rewards/margins': 0.24667574961980182, 'kl': 0.25988006591796875, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 96/114 [02:07<00:22,  1.25s/it]0m \n",
      " 85%|████████▌ | 97/114 [02:08<00:21,  1.24s/it]0m \n",
      " 86%|████████▌ | 98/114 [02:10<00:20,  1.27s/it]0m \n",
      " 87%|████████▋ | 99/114 [02:11<00:18,  1.25s/it]0m \n",
      " 88%|████████▊ | 100/114 [02:12<00:17,  1.22s/it][INFO|trainer.py:3993] 2026-02-08 17:39:22,183 >> Saving model checkpoint to llama3_8b_lora_kto/checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4731, 'grad_norm': 2.9918088912963867, 'learning_rate': 2.620917716123444e-07, 'rewards/chosen': 0.033857100776263645, 'logps/chosen': -195.83489118303572, 'logits/chosen': -26336368.0, 'rewards/rejected': -0.1773396929105123, 'logps/rejected': -254.9541219075521, 'logits/rejected': -4914320.666666667, 'rewards/margins': 0.21119679368677594, 'kl': 0.0, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:39:22,400 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:39:22,401 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 17:39:22,552 >> chat template saved in llama3_8b_lora_kto/checkpoint-100/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 17:39:22,554 >> tokenizer config file saved in llama3_8b_lora_kto/checkpoint-100/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 17:39:22,554 >> Special tokens file saved in llama3_8b_lora_kto/checkpoint-100/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 2 at 2026-02-08 17:39:26. Total running time: 3min 20s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000001 │\n",
      "│ time_this_iter_s               66.23456 │\n",
      "│ time_total_s                  189.36603 │\n",
      "│ training_iteration                    2 │\n",
      "│ epoch                              2.64 │\n",
      "│ grad_norm                       2.99181 │\n",
      "│ kl                                   0. │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                -26336368. │\n",
      "│ logits/rejected          -4914320.66667 │\n",
      "│ logps/chosen                 -195.83489 │\n",
      "│ logps/rejected               -254.95412 │\n",
      "│ loss                             0.4731 │\n",
      "│ rewards/chosen                  0.03386 │\n",
      "│ rewards/margins                  0.2112 │\n",
      "│ rewards/rejected               -0.17734 │\n",
      "│ step                                100 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 2 at: (local)/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 101/114 [02:17<00:31,  2.40s/it]m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 89%|████████▉ | 102/114 [02:18<00:24,  2.04s/it]m \n",
      " 90%|█████████ | 103/114 [02:20<00:20,  1.83s/it]m \n",
      " 91%|█████████ | 104/114 [02:21<00:16,  1.67s/it]m \n",
      " 92%|█████████▏| 105/114 [02:22<00:14,  1.56s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4848, 'grad_norm': 3.185372829437256, 'learning_rate': 1.1764499893210879e-07, 'rewards/chosen': 0.11968272179365158, 'logps/chosen': -335.6123046875, 'logits/chosen': -26846008.0, 'rewards/rejected': -0.2101486325263977, 'logps/rejected': -351.06842041015625, 'logits/rejected': -6585261.0, 'rewards/margins': 0.3298313543200493, 'kl': 0.0, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 106/114 [02:24<00:12,  1.50s/it]m \n",
      " 94%|█████████▍| 107/114 [02:25<00:10,  1.45s/it]m \n",
      " 95%|█████████▍| 108/114 [02:26<00:08,  1.40s/it]m \n",
      " 96%|█████████▌| 109/114 [02:28<00:06,  1.37s/it]m \n",
      " 96%|█████████▋| 110/114 [02:29<00:05,  1.36s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'loss': 0.4875, 'grad_norm': 3.454695224761963, 'learning_rate': 2.958631979685156e-08, 'rewards/chosen': -0.10164896647135417, 'logps/chosen': -352.5401611328125, 'logits/chosen': -23750568.0, 'rewards/rejected': -0.1265277521950858, 'logps/rejected': -479.50341796875, 'logits/rejected': -38374107.428571425, 'rewards/margins': 0.02487878572373163, 'kl': 1.4368000030517578, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 111/114 [02:30<00:04,  1.34s/it]m \n",
      " 98%|█████████▊| 112/114 [02:32<00:02,  1.34s/it]m \n",
      " 99%|█████████▉| 113/114 [02:33<00:01,  1.33s/it]m \n",
      "100%|██████████| 114/114 [02:34<00:00,  1.15s/it][INFO|trainer.py:3993] 2026-02-08 17:39:43,754 >> Saving model checkpoint to llama3_8b_lora_kto/checkpoint-114\n",
      "\u001b[36m(RayTrainWorker pid=4170, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:39:43,968 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:39:43,969 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 17:39:44,115 >> chat template saved in llama3_8b_lora_kto/checkpoint-114/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 17:39:44,118 >> tokenizer config file saved in llama3_8b_lora_kto/checkpoint-114/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 17:39:44,118 >> Special tokens file saved in llama3_8b_lora_kto/checkpoint-114/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 3 at 2026-02-08 17:39:48. Total running time: 3min 42s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000002 │\n",
      "│ time_this_iter_s               21.82934 │\n",
      "│ time_total_s                  211.19537 │\n",
      "│ training_iteration                    3 │\n",
      "│ epoch                           2.90667 │\n",
      "│ grad_norm                        3.4547 │\n",
      "│ kl                               1.4368 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                -23750568. │\n",
      "│ logits/rejected         -38374107.42857 │\n",
      "│ logps/chosen                 -352.54016 │\n",
      "│ logps/rejected               -479.50342 │\n",
      "│ loss                             0.4875 │\n",
      "│ rewards/chosen                 -0.10165 │\n",
      "│ rewards/margins                 0.02488 │\n",
      "│ rewards/rejected               -0.12653 │\n",
      "│ step                                110 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 3 at: (local)/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:2676] 2026-02-08 17:39:48,072 >> \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "100%|██████████| 114/114 [02:38<00:00,  1.39s/it]m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|trainer.py:3993] 2026-02-08 17:39:48,076 >> Saving model checkpoint to llama3_8b_lora_kto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'train_runtime': 158.5156, 'train_samples_per_second': 5.678, 'train_steps_per_second': 0.719, 'train_loss': 0.48332400907549944, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:698] 2026-02-08 17:39:48,292 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|configuration_utils.py:770] 2026-02-08 17:39:48,293 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2026-02-08 17:39:48,437 >> chat template saved in llama3_8b_lora_kto/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2026-02-08 17:39:48,440 >> tokenizer config file saved in llama3_8b_lora_kto/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2026-02-08 17:39:48,440 >> Special tokens file saved in llama3_8b_lora_kto/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   epoch                    =        3.0\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   total_flos               = 19610724GF\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   train_loss               =     0.4833\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   train_runtime            = 0:02:38.51\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   train_samples_per_second =      5.678\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m   train_steps_per_second   =      0.719\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Figure saved at: llama3_8b_lora_kto/training_loss.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [INFO|modelcard.py:450] 2026-02-08 17:39:48,782 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Figure saved at: llama3_8b_lora_kto/training_rewards_chosen.png\n",
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m [WARNING|2026-02-08 17:39:48] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "\n",
      "Training completed after 3 iterations at 2026-02-08 17:39:50. Total running time: 3min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 17:39:50,308\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/llama3_8b_kto_lora' in 0.0159s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4171, ip=10.128.6.27)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/llama3_8b_kto_lora/TorchTrainer_b359e_00000_0_2026-02-08_17-36-05/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "USE_RAY=1 llamafactory-cli train ../train-configs/kto_lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B — Run as an Anyscale job (production)\n",
    "\n",
    "For longer or production runs, submit the training as an **Anyscale job**. Jobs run outside your interactive session for better stability, retries, and durable logs. You package LLaMA-Factory and other libraries in a container image and launch with a short job config. See [Run LLaMA-Factory as an Anyscale job](https://docs.anyscale.com/llm/fine-tuning/llamafactory-jobs) for the step-by-step guide.\n",
    "\n",
    "### Tracking with MLflow\n",
    "\n",
    "If you enabled MLflow logging (`report_to: mlflow` in your YAML), LLaMA-Factory logs metrics (loss, learning rate, etc.), parameters, and artifacts to your configured MLflow tracking server.\n",
    "\n",
    "**Example YAML snippet:**\n",
    "\n",
    "```yaml\n",
    "report_to: mlflow\n",
    "\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      MLFLOW_TRACKING_URI: \"https://<your_cloud_id>.cloud.databricks.com\"\n",
    "      MLFLOW_TRACKING_TOKEN: \"<mlflow_tracking_token>\"\n",
    "      MLFLOW_EXPERIMENT_NAME: \"/Users/<your_user_id>/experiment_name\"\n",
    "```\n",
    "\n",
    "**MLFlow example**\n",
    "\n",
    "![MLflow](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/3.2.3/3.2.3-mlflow.png)\n",
    "\n",
    "For a more detailed guide on tracking experiments with other tools such as Weights and Biases or MLflow, see [Observability and tracking](https://docs.anyscale.com/llm/fine-tuning/observability-and-tracking).\n",
    "\n",
    "## Step 5: Locate checkpoints\n",
    "\n",
    "Ray Train writes checkpoints under `ray_storage_path/ray_run_name`. In this example run, the path is: `/mnt/cluster_storage/llama3_8b_kto_lora`. \n",
    "\n",
    "Inside, you see a **trainer session** directory named like:\n",
    "`TorchTrainer_75e12_00000_0_2025-09-22_17-58-47`.\n",
    "\n",
    "- Ray Train creates `TorchTrainer_*` **when the trainer starts**; the suffix encodes a short run ID and the **start timestamp**.\n",
    "- Within that directory, Ray Train names checkpoints `checkpoint_000xxx/`, where the number is the saved ordered checkpoints.\n",
    "\n",
    "Control the save cadence with `save_strategy` and `save_steps`. For instructions on how to resume interrupted training with `resume_from_checkpoint` and more, see [Understand the artifacts directory](https://docs.anyscale.com/llm/fine-tuning/checkpointing#artifacts-directory).\n",
    "\n",
    "## Step 6: Export the model\n",
    "\n",
    "If you use LoRA, you can keep the base model and adapters separate for [multi-LoRA deployment](https://docs.anyscale.com/llm/serving/multi-lora) or [merge the adapters](https://docs.anyscale.com/llm/fine-tuning/checkpointing#merge-lora) into the base model for low-latency inference. \n",
    "\n",
    "For full fine-tuning or freeze-tuning, export the fine-tuned model directly.\n",
    "\n",
    "You may optionally apply [post-training quantization](https://docs.anyscale.com/llm/fine-tuning/checkpointing#ptq) on merged or full models before serving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
