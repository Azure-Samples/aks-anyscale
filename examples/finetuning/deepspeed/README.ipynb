{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune an LLM with Ray Train and DeepSpeed\n",
    "\n",
    "**Time to complete:** 20 min\n",
    "\n",
    "This notebook combines **Ray Train** with **DeepSpeed** to efficiently scale PyTorch training across GPUs and nodes while minimizing GPU memory usage.\n",
    "\n",
    "This hands-on example includes the following:\n",
    "-  Fine-tuning an LLM\n",
    "- Checkpoint saving and resuming with Ray Train\n",
    "- Configuring ZeRO for memory and performance (stages, mixed precision, CPU offload)\n",
    "- Launching a distributed training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
    "\n",
    "  <strong>Anyscale specific configuration</strong>\n",
    "\n",
    "  <p><strong>Note:</strong> This template is optimized for the Anyscale platform. On Anyscale, most configuration is automated. When running on open-source Ray, manually complete the following steps:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li><strong>Configure your Ray cluster</strong>: Multi-node setup and resource allocation.</li>\n",
    "    <li><strong>Manage dependencies</strong>: Install prerequisites on each node.</li>\n",
    "    <li><strong>Set up storage</strong>: Provide shared or distributed checkpoint storage.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  div#anyscale-note > p,\n",
    "  div#anyscale-note > ul,\n",
    "  div#anyscale-note > ul li {\n",
    "    color: black;\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    background-color: rgb(255, 243, 205);\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    border: 1px solid #ccc; \n",
    "    border-radius: 8px;\n",
    "    padding: 15px;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install dependencies (if needed)\n",
    "\n",
    "Run the cell below only if your environment still needs these packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89055e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ray/anaconda3/lib/python3.12/site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in /home/ray/anaconda3/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (2023.12.1)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.3.3)\n",
      "Requirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "\u001b[92mSuccessfully registered `torch, torchvision` packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_hjrmz1lbhql5w8qazr3py5m6rd?workspace-tab=dependencies\u001b[0m\n",
      "Requirement already satisfied: transformers in /home/ray/anaconda3/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: datasets==3.6.0 in /home/ray/anaconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: trl==0.23.1 in /home/ray/anaconda3/lib/python3.12/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2023.12.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (1.4.1)\n",
      "Requirement already satisfied: packaging in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==3.6.0) (6.0.1)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from trl==0.23.1) (1.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /home/ray/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /home/ray/anaconda3/lib/python3.12/site-packages (from accelerate>=1.4.0->trl==0.23.1) (5.9.6)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from accelerate>=1.4.0->trl==0.23.1) (2.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ray/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.11.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /home/ray/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from typer-slim->transformers) (8.1.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.0.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.18.3)\n",
      "Requirement already satisfied: anyio in /home/ray/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets==3.6.0) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ray/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ray/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets==3.6.0) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (1.3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets==3.6.0) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.23.1) (2.1.3)\n",
      "\u001b[92mSuccessfully registered `transformers, datasets` and 1 other packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_hjrmz1lbhql5w8qazr3py5m6rd?workspace-tab=dependencies\u001b[0m\n",
      "Requirement already satisfied: deepspeed in /home/ray/anaconda3/lib/python3.12/site-packages (0.18.5)\n",
      "Requirement already satisfied: ray[train] in /home/ray/anaconda3/lib/python3.12/site-packages (2.50.0)\n",
      "Requirement already satisfied: einops in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (0.8.2)\n",
      "Requirement already satisfied: hjson in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (1.0.7)\n",
      "Requirement already satisfied: ninja in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (1.13.0)\n",
      "Requirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (23.0)\n",
      "Requirement already satisfied: psutil in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (2.11.7)\n",
      "Requirement already satisfied: torch in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (2.10.0)\n",
      "Requirement already satisfied: tqdm in /home/ray/anaconda3/lib/python3.12/site-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: click!=8.3.0,>=7.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (8.1.7)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (3.17.0)\n",
      "Requirement already satisfied: jsonschema in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (4.23.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (4.25.8)\n",
      "Requirement already satisfied: pyyaml in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (2.32.3)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (2.3.3)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (19.0.1)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.12/site-packages (from ray[train]) (2023.12.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from jsonschema->ray[train]) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ray/anaconda3/lib/python3.12/site-packages (from jsonschema->ray[train]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ray/anaconda3/lib/python3.12/site-packages (from jsonschema->ray[train]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from jsonschema->ray[train]) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->ray[train]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->ray[train]) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->ray[train]) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests->ray[train]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests->ray[train]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests->ray[train]) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests->ray[train]) (2025.1.31)\n",
      "Requirement already satisfied: setuptools in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch->deepspeed) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch->deepspeed) (1.3.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->ray[train]) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from jinja2->torch->deepspeed) (2.1.3)\n",
      "\u001b[92mSuccessfully registered `deepspeed, ray` packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_a6j8iubw9rqbyigfwk9fut4amk/prj_a8aurpnjjkhushuarbyy4kwkre/workspaces/expwrk_hjrmz1lbhql5w8qazr3py5m6rd?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install torch torchvision\n",
    "pip install transformers datasets==3.6.0 trl==0.23.1\n",
    "pip install deepspeed ray[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e91058",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration constants\n",
    "\n",
    "This notebook uses simple constants instead of `argparse` to simplify execution. Adjust these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11864ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training constants (edit these) ----\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"ag_news\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "SEQ_LENGTH = 512\n",
    "LEARNING_RATE = 1e-6\n",
    "ZERO_STAGE = 3\n",
    "TUTORIAL_STEPS = 30\n",
    "\n",
    "# Ray scaling settings\n",
    "NUM_WORKERS = 2\n",
    "USE_GPU = True\n",
    "\n",
    "# Storage\n",
    "STORAGE_PATH = \"/mnt/cluster_storage/\"\n",
    "EXPERIMENT_PREFIX = \"deepspeed_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27671ed0",
   "metadata": {},
   "source": [
    "## 1. Define the training function\n",
    "\n",
    "First, define the training loop function for each worker to execute. Note that Ray Train allocates a unique GPU to each worker.\n",
    "Ray Train runs this training function on every worker to orchestrate the overall training process. The training function outlines the high-level structure common to most deep learning workflows, showing how setup, data ingestion, optimization, and reporting stages come together on each worker.\n",
    "\n",
    "The training function does the following:\n",
    "\n",
    "1. Initializes the model and optimizer with DeepSpeed (`setup_model_and_optimizer`).\n",
    "1. Restores training from a checkpoint if one is available (`load_checkpoint`).\n",
    "1. Sets up the dataloader (`setup_dataloader`).\n",
    "1. Accesses the device that Ray Train assigns to this worker.\n",
    "1. Iterates through the specified number of epochs.\n",
    "1. For multi-GPU training, ensures each worker sees a unique data shard each epoch.\n",
    "1. For each batch:\n",
    "   - Moves inputs to the device.\n",
    "   - Runs the forward pass to compute loss.\n",
    "   - Logs the loss.\n",
    "1. Performs the backward pass and optimizer step with DeepSpeed.\n",
    "1. Aggregates average loss and reports metrics, saving a checkpoint at the end of each epoch. (`report_metrics_and_save_checkpoint`)\n",
    "\n",
    "Later steps define the above helper functions (`setup_model_and_optimizer`, `load_checkpoint`, `setup_dataloader`, `report_metrics_and_save_checkpoint`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eaf2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"  # Ensure Ray Train v2 APIs\n",
    "import ray\n",
    "\n",
    "def train_loop(config: Dict[str, Any]) -> None:\n",
    "    # (1) Initialize model and optimizer with DeepSpeed\n",
    "    ds_engine = setup_model_and_optimizer(config[\"model_name\"], config[\"learning_rate\"], config[\"ds_config\"])\n",
    "\n",
    "    # (2) Load checkpoint if it exists\n",
    "    ckpt = ray.train.get_checkpoint()\n",
    "    start_epoch = 0\n",
    "    if ckpt:\n",
    "        start_epoch = load_checkpoint(ds_engine, ckpt)\n",
    "\n",
    "    # (3) Set up dataloader\n",
    "    train_loader = setup_dataloader(config[\"model_name\"], config[\"dataset_name\"], config[\"seq_length\"], config[\"batch_size\"])\n",
    "    steps_per_epoch = len(train_loader)\n",
    "\n",
    "    # (4) Access the device for this worker\n",
    "    device = ray.train.torch.get_device()\n",
    "\n",
    "    # Set model to training mode\n",
    "    ds_engine.train()\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        # (6) Ensure unique shard per worker when using multiple GPUs\n",
    "        if ray.train.get_context().get_world_size() > 1 and hasattr(train_loader, \"sampler\"):\n",
    "            sampler = getattr(train_loader, \"sampler\", None)\n",
    "            if sampler and hasattr(sampler, \"set_epoch\"):\n",
    "                sampler.set_epoch(epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # (7) Iterate over batches\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = ds_engine(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            print(f\"Epoch: {epoch} Step: {step + 1}/{steps_per_epoch} Loss: {loss.item()}\")\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            ds_engine.backward(loss)\n",
    "            ds_engine.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Stop early in the tutorial so runs finish quickly\n",
    "            if step + 1 >= config[\"tutorial_steps\"]:\n",
    "                print(f\"Stopping early at {config['tutorial_steps']} steps for the tutorial\")\n",
    "                break\n",
    "\n",
    "        # (8) Report metrics and save checkpoint\n",
    "        report_metrics_and_save_checkpoint(ds_engine, {\"loss\": running_loss / num_batches, \"epoch\": epoch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e9ce1",
   "metadata": {},
   "source": [
    "Ray Train runs the `train_loop` on each worker, which naturally supports **data parallelism**. In this setup, each worker processes a unique shard of data, computes gradients locally, and participates in synchronization to keep model parameters consistent. On top of this, DeepSpeed partitions model and optimizer states across GPUs to reduce memory usage and communication overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145143e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Set Up the dataloader\n",
    "\n",
    "The code below demonstrates how to prepare text data so that each worker can efficiently feed batches during training.\n",
    "\n",
    "1. Downloads a tokenizer from the Hugging Face Hub (`AutoTokenizer`).  \n",
    "2. Loads the `ag_news` dataset using Hugging Face's `load_dataset`.  \n",
    "3. Applies tokenization with padding and truncation by calling `map`.  \n",
    "4. Converts the dataset into a PyTorch `DataLoader`, which handles batching and shuffling.  \n",
    "5. Finally, call `ray.train.torch.prepare_data_loader` to make the dataloader distributed-ready.\n",
    "\n",
    "When you use **data parallelism**, each GPU worker trains on a unique shard of the dataset while holding its own copy of the model; gradients are synchronized after each step.\n",
    "Ray Train's `prepare_data_loader` wraps PyTorch’s `DataLoader` and ensures workers see disjoint data, balances splits, and correctly handle epoch boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28986c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train\n",
    "import ray.train.torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "def setup_dataloader(model_name: str, dataset_name: str, seq_length: int, batch_size: int) -> DataLoader:\n",
    "    # (1) Get tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set pad token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            # Fallback for models without eos_token\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    # (2) Load dataset\n",
    "    # This example uses only 1% of the dataset for quick testing. Adjust as needed.\n",
    "    dataset = load_dataset(dataset_name, split=\"train[:1%]\", download_config=DownloadConfig(disable_tqdm=True))\n",
    "\n",
    "    # (3) Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', max_length=seq_length, truncation=True)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=1, keep_in_memory=True)\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # (4) Create DataLoader\n",
    "    data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (5) Use prepare_data_loader for distributed training\n",
    "    return ray.train.torch.prepare_data_loader(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bb93a",
   "metadata": {},
   "source": [
    "The following code demonstrates how to use the tokenizer to encode a sample string. \n",
    "- `AutoTokenizer.from_pretrained` downloads and configures the tokenizer for your model.\n",
    "- You can encode any text string and inspect the resulting token IDs and attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e00b634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [19591, 16835, 290, 10766, 22785, 787, 9387, 3047, 2562, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of get_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_text = \"Ray Train and DeepSpeed make distributed training easy!\"\n",
    "encoded = tokenizer(sample_text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca2710",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Initialize model and optimizer\n",
    "\n",
    "After preparing and distributing the dataset, the next step is to set up the model and optimizer for training. This function does the following:\n",
    "\n",
    "1. Loads a pretrained model from the Hugging Face Hub (`AutoModelForCausalLM`).  \n",
    "2. Defines the optimizer (`AdamW`).  \n",
    "3. Initializes DeepSpeed with ZeRO options and returns a `DeepSpeedEngine`.\n",
    "\n",
    "DeepSpeed’s `initialize` always partitions **optimizer states** (ZeRO Stage 1) across the GPU memory of all workers participating in training. Depending on the chosen stage, it can also partition **gradients** (Stage 2) and **model parameters/weights** (Stage 3). This staged approach balances memory savings and communication overhead, and the tutorial covers these stages in more detail [in later steps](#deepspeed-zero-stages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f532982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-08 13:31:16,013] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import deepspeed\n",
    "\n",
    "def setup_model_and_optimizer(model_name: str, learning_rate: float, ds_config: Dict[str, Any]) -> deepspeed.runtime.engine.DeepSpeedEngine:\n",
    "    # (1) Load pretrained model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # (2) Define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # (3) Initialize with DeepSpeed (distributed and memory optimizations)\n",
    "    ds_engine, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=ds_config)\n",
    "    return ds_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7758bfa",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Checkpoint saving and loading\n",
    "\n",
    "Checkpointing is crucial for fault tolerance and for resuming training after interruptions. This section saves and restores model and optimizer states in a distributed Ray Train with DeepSpeed setup. It demonstrates how each worker saves its own checkpoint shard, how Ray bundles them into a unified checkpoint, and how this enables seamless recovery or further fine-tuning from the saved state.\n",
    "\n",
    "### Saving checkpoints\n",
    "\n",
    "First define how Ray Train should save checkpoints during training. The code below shows how to create temporary directories, store model states, and report checkpoint information and metrics back to Ray Train for tracking and coordination. Note that DeepSpeed saves model and optimizer states in a **partitioned format**, where each worker stores only its shard.\n",
    "\n",
    "1. Create a temporary directory for storing checkpoints.\n",
    "1. Save the partitioned model and optimizer states with DeepSpeed's `save_checkpoint`.\n",
    "1. Report metrics and checkpoint location to Ray Train with `ray.train.report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6edbefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import ray.train\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "def report_metrics_and_save_checkpoint(\n",
    "    ds_engine: deepspeed.runtime.engine.DeepSpeedEngine,\n",
    "    metrics: Dict[str, Any]\n",
    ") -> None:\n",
    "    \"\"\"Save worker checkpoints and report metrics to Ray Train.\n",
    "    Each rank writes its shard to a temp directory so Ray Train bundles all of them.\n",
    "    \"\"\"\n",
    "    ctx = ray.train.get_context()\n",
    "    epoch_value = metrics[\"epoch\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        checkpoint_dir = os.path.join(tmp_dir, \"checkpoint\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        ds_engine.save_checkpoint(checkpoint_dir)\n",
    "\n",
    "        epoch_file = os.path.join(checkpoint_dir, \"epoch.txt\")\n",
    "        with open(epoch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(epoch_value))\n",
    "\n",
    "        checkpoint = Checkpoint.from_directory(tmp_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)\n",
    "\n",
    "        if ctx.get_world_rank() == 0:\n",
    "            experiment_name = ctx.get_experiment_name()\n",
    "            print(\n",
    "                f\"Checkpoint saved successfully for experiment {experiment_name} at {checkpoint_dir}. Metrics: {metrics}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3e2ea",
   "metadata": {},
   "source": [
    "### Loading checkpoints\n",
    "\n",
    "After saving checkpoints, the next step is being able to resume training or evaluation from a saved state.\n",
    "This ensures that progress isn’t lost due to interruptions and allows long-running jobs to continue seamlessly across sessions.\n",
    "When restarting, Ray Train provides each worker with the latest checkpoint so that DeepSpeed can rebuild the model, optimizer, and training progress from where it left off.\n",
    "\n",
    "Restore a previously saved checkpoint into the DeepSpeed engine using `load_checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66d1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ds_engine: deepspeed.runtime.engine.DeepSpeedEngine, ckpt: ray.train.Checkpoint) -> int:\n",
    "    \"\"\"Restore DeepSpeed state and determine next epoch.\"\"\"\n",
    "    next_epoch = 0\n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            print(f\"Loading checkpoint from {checkpoint_dir}\")\n",
    "            epoch_dir = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            if not os.path.isdir(epoch_dir):\n",
    "                epoch_dir = checkpoint_dir\n",
    "\n",
    "            ds_engine.load_checkpoint(epoch_dir)\n",
    "\n",
    "            epoch_file = os.path.join(epoch_dir, \"epoch.txt\")\n",
    "            if os.path.isfile(epoch_file):\n",
    "                with open(epoch_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    last_epoch = int(f.read().strip())\n",
    "                next_epoch = last_epoch + 1\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e\n",
    "    return next_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d099c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Configure DeepSpeed\n",
    "\n",
    "Before launching distributed training, you need to define a DeepSpeed configuration dictionary (`ds_config`) that controls data type settings, batch sizes, optimizations including ZeRO (model state partitioning strategies), etc. This configuration determines how DeepSpeed manages memory, communication, and performance across GPUs.\n",
    "\n",
    "The example below shows a minimal setup that enables bfloat16 precision, gradient clipping, and ZeRO optimization. You can further customize this configuration based on your model size, hardware, and performance goals. See [Advanced Configurations](#advanced-configurations) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa896f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeed configuration\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": BATCH_SIZE,\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"grad_accum_dtype\": \"bf16\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": ZERO_STAGE,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e740251",
   "metadata": {},
   "source": [
    "## 6. Launch distributed training\n",
    "\n",
    "The final step is to configure parameters and launch the distributed training job.\n",
    "Ray Train’s `TorchTrainer` automatically starts multiple workers—one per GPU—and executes the `train_loop` on each instance. The **scaling configuration** determines how many workers to launch and what resources they use, while the **run configuration** manages storage and experiment tracking.\n",
    "\n",
    "This function does the following:\n",
    "1. Parses command-line arguments for training and model settings.\n",
    "1. Defines the Ray Train `ScalingConfig`—for example, the number of workers and GPU usage.\n",
    "1. Prepares the training loop configuration with hyperparameters and model details.\n",
    "1. Sets up the Ray Train `RunConfig` to manage storage and experiment metadata. This example sets a random experiment name, but you can specify the name of a previous experiment to load the checkpoint.\n",
    "1. Creates a `TorchTrainer` that launches the training function on multiple GPU workers.\n",
    "1. Starts training with `trainer.fit()` and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c132f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 13:33:19,281\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.128.4.230:6379...\n",
      "2026-02-08 13:33:19,288\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-k8x7fi7c1x3ppm23k4pydhfwtk.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-08 13:33:19,290\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_ab21a23f5fd120e82c962724b94a2448041a3bf7.zip' (0.11MiB) to Ray cluster...\n",
      "2026-02-08 13:33:19,290\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_ab21a23f5fd120e82c962724b94a2448041a3bf7.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 1/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 2/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m df: /home/ray/.triton/autotune: No such file or directory\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m PyTorch version 2.10.0 available.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Polars version 1.32.3 available.\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m - (ip=10.128.6.1, pid=556) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m - (ip=10.128.6.1, pid=731) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/models/openai-community/gpt2/xet-read-token/607a30d783dfa663caf39e06633721c8d4cfcd7e \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 2648.70it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m \u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Key                  | Status     |  | \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m ---------------------+------------+--+-\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m \u001b[3mNotes:\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m - UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m \u001b[3mNotes:\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Stage 3 initialize beginning\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.25 GB         Max_MA 0.25 GB         CA 0.26 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.28 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m DeepSpeedZeRoOffload initialize [begin]\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.25 GB         Max_MA 0.25 GB         CA 0.26 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.31 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m PyTorch version 2.10.0 available.\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Polars version 1.32.3 available.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Parameter Offload - Persistent parameters statistics: param_count = 98, numel = 121344\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m DeepSpeedZeRoOffload initialize [end]\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.13 GB         Max_MA 0.29 GB         CA 0.31 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.34 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Before creating fp16 partitions\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.13 GB         Max_MA 0.13 GB         CA 0.31 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.34 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m /tmp/ray/session_2026-02-08_12-58-52_585988_181/runtime_resources/pip/6e130f4c8fcdce751cf658a03d5ab932a6e336c0/virtualenv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m   return func(*args, **kwargs)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m After creating fp16 partitions: 1\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.13 GB         Max_MA 0.13 GB         CA 0.13 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Before creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.13 GB         Max_MA 0.13 GB         CA 0.13 GB         Max_CA 0 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m After creating fp32 partitions\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.36 GB         Max_MA 0.59 GB         CA 0.6 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Before initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.36 GB         Max_MA 0.36 GB         CA 0.6 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m After initializing optimizer states\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 0.36 GB         Max_MA 0.59 GB         CA 0.6 GB         Max_CA 1 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 2769.75it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m \u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Key                  | Status     |  | \n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m ---------------------+------------+--+-\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m - UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m After initializing ZeRO optimizer\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m MA 1.41 GB         Max_MA 1.48 GB         CA 1.53 GB         Max_CA 2 GB \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m CPU Virtual Memory:  used = 18.35 GB, percent = 1.0%\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/gpt2/resolve/main/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/models/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/models/openai-community/gpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/fancyzhx/ag_news/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/ag_news/resolve/eb185aade064a813bc0b7f42de02595523103ca4/ag_news.py \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/ag_news/ag_news.py \"HTTP/1.1 200 OK\"\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=ag_news \"HTTP/1.1 404 Not Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/fancyzhx/ag_news/resolve/eb185aade064a813bc0b7f42de02595523103ca4/data/train-00000-of-00001.parquet \"HTTP/1.1 302 Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/datasets/fancyzhx/ag_news/xet-read-token/eb185aade064a813bc0b7f42de02595523103ca4 \"HTTP/1.1 200 OK\"\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/fancyzhx/ag_news/resolve/eb185aade064a813bc0b7f42de02595523103ca4/data/test-00000-of-00001.parquet \"HTTP/1.1 302 Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/datasets/ag_news/revision/eb185aade064a813bc0b7f42de02595523103ca4 \"HTTP/1.1 307 Temporary Redirect\"\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1280592.72 examples/s]\n",
      "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m               0 COPY_FREE_VARS           2\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m  24           2 RESUME                   0\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m  25           4 PUSH_NULL\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m               6 LOAD_DEREF               2 (tokenizer)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m               8 LOAD_FAST                0 (examples)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              10 LOAD_CONST               1 ('text')\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              12 BINARY_SUBSCR\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              16 LOAD_CONST               2 ('max_length')\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              18 LOAD_DEREF               1 (seq_length)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              20 LOAD_CONST               3 (True)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              22 KW_NAMES                 4 (('padding', 'max_length', 'truncation'))\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              24 CALL                     4\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m              32 RETURN_VALUE\n",
      "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1001750.74 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m \n",
      "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]\n",
      "Map:  83%|████████▎ | 1000/1200 [00:00<00:00, 3403.42 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/fancyzhx/ag_news/resolve/eb185aade064a813bc0b7f42de02595523103ca4/dataset_infos.json \"HTTP/1.1 404 Not Found\"\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 3573.59 examples/s]\n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 3416.70 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Epoch: 0 Step: 1/600 Loss: 8.70724105834961\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/fancyzhx/ag_news/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: HEAD https://huggingface.co/datasets/ag_news/resolve/eb185aade064a813bc0b7f42de02595523103ca4/data/test-00000-of-00001.parquet \"HTTP/1.1 307 Temporary Redirect\"\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=ag_news \"HTTP/1.1 404 Not Found\"\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m               0 COPY_FREE_VARS           2\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m  24           2 RESUME                   0\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m  25           4 PUSH_NULL\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m               6 LOAD_DEREF               2 (tokenizer)\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m               8 LOAD_FAST                0 (examples)\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              10 LOAD_CONST               1 ('text')\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              12 BINARY_SUBSCR\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              16 LOAD_CONST               2 ('max_length')\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              18 LOAD_DEREF               1 (seq_length)\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              20 LOAD_CONST               3 (True)\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              22 KW_NAMES                 4 (('padding', 'max_length', 'truncation'))\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              24 CALL                     4\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m              32 RETURN_VALUE\n",
      "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Epoch: 0 Step: 21/600 Loss: 9.762721061706543\u001b[32m [repeated 40x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +3m41s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Stopping early at 30 steps for the tutorial\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_sample_ee5ca83a/checkpoint_2026-02-08_13-34-55.259945)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_sample_ee5ca83a/checkpoint_2026-02-08_13-34-55.259945), metrics={'loss': 9.179322147369385, 'epoch': 0}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=556, ip=10.128.6.1)\u001b[0m Checkpoint saved successfully for experiment deepspeed_sample_ee5ca83a at /tmp/tmp4butu5ze/checkpoint. Metrics: {'loss': 9.179322147369385, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Epoch: 0 Step: 30/600 Loss: 8.867111206054688\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(TrainController pid=20538)\u001b[0m [State Transition] RUNNING -> FINISHED.\n",
      "\u001b[36m(RayTrainWorker pid=731, ip=10.128.6.1)\u001b[0m Stopping early at 30 steps for the tutorial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Result: Result(metrics={'loss': 9.179322147369385, 'epoch': 0}, checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_sample_ee5ca83a/checkpoint_2026-02-08_13-34-55.259945), error=None, path='/mnt/cluster_storage/deepspeed_sample_ee5ca83a', metrics_dataframe=       loss  epoch\n",
      "0  9.179322      0, best_checkpoints=[(Checkpoint(filesystem=local, path=/mnt/cluster_storage/deepspeed_sample_ee5ca83a/checkpoint_2026-02-08_13-34-55.259945), {'loss': 9.179322147369385, 'epoch': 0})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7eff6446b7f0>)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "\n",
    "# Ray Train scaling configuration\n",
    "scaling_config = ScalingConfig(num_workers=NUM_WORKERS, use_gpu=USE_GPU)\n",
    "\n",
    "# Training loop configuration\n",
    "train_loop_config = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"ds_config\": ds_config,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"tutorial_steps\": TUTORIAL_STEPS,\n",
    "}\n",
    "\n",
    "# Ray Train run configuration\n",
    "run_config = RunConfig(\n",
    "    storage_path=STORAGE_PATH,\n",
    "    # Set the name of the previous experiment when resuming from a checkpoint\n",
    "    name=f\"{EXPERIMENT_PREFIX}_{uuid.uuid4().hex[:8]}\",\n",
    ")\n",
    "\n",
    "# Create and launch the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "# To actually run training, execute the following:\n",
    "result = trainer.fit()\n",
    "print(f\"Training finished. Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running as a standalone script\n",
    "\n",
    "While this tutorial is designed to run interactively in a Jupyter notebook, you can also launch the same training workflow as a standalone Python script.\n",
    "This is useful for running longer experiments, automating jobs, or deploying training on a cluster.\n",
    "\n",
    "The [full code](https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/deepspeed_finetune/train.py) is also available.\n",
    "To start training from the command line, run:\n",
    "\n",
    "```bash\n",
    "python train.py\n",
    "```\n",
    "\n",
    "## Advanced configurations\n",
    "\n",
    "DeepSpeed has many other configuration options to tune performance and memory usage.\n",
    "This section introduces some of the most commonly used options.\n",
    "See the [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/) for more details.\n",
    "\n",
    "### DeepSpeed ZeRO stages\n",
    "- **Stage 1**: Partitions optimizer states (always on when using ZeRO).  \n",
    "- **Stage 2**: Additionally partitions gradients.  \n",
    "- **Stage 3**: Additionally partitions model parameters or weights.\n",
    "\n",
    "The higher the stage, the more memory savings you get, but it may also introduce more communication overhead and complexity in training.\n",
    "You can select the stage through `ds_config[\"zero_optimization\"][\"stage\"]`. See the DeepSpeed docs for more details.\n",
    "\n",
    "```python\n",
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,  # or 1 or 3\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "### Mixed precision\n",
    "Enable BF16 or FP16:\n",
    "```python\n",
    "ds_config = {\n",
    "    \"bf16\": {\"enabled\": True},  # or \"fp16\": {\"enabled\": True}\n",
    "}\n",
    "```\n",
    "\n",
    "### CPU offloading\n",
    "Reduce GPU memory pressure by offloading to CPU at the cost of PCIe traffic:\n",
    "```python\n",
    "ds_config = {\n",
    "    \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "    # or\n",
    "    \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you did the following:\n",
    "\n",
    "- Fine-tuned an LLM using Ray Train and DeepSpeed ZeRO\n",
    "- Set up distributed data loading with Ray Train's `prepare_data_loader`\n",
    "- Saved and managed checkpoints with Ray Train's storage configuration\n",
    "- Configured and launched multi-GPU training with `TorchTrainer` and scaling configurations\n",
    "- Explored advanced DeepSpeed configurations (ZeRO stages, mixed precision, and CPU offloading)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
